From 04aefb2728ea2b00956f52c13dbdb477650bbf22 Mon Sep 17 00:00:00 2001
From: suijingfeng <suijingfeng@loongson.cn>
Date: Fri, 23 Sep 2022 17:28:33 +0800
Subject: [PATCH] add gsgpu API for libdrm-2.4.99

 1. add compiler chains for  gsgpu by configure.ac
 2. add compiler chains for  gsgpu by meson
 3. add basic gsgpu testsuit for testing modules
 4. add pci device id(0x7a25) for loongson gpu (LoongGPU)

gsgpu/dma: init code framework for test suits of dma

gsgpu/dma: porting some function for test suits of dma

code framework to meet specification of 'Cunit', by these steps:
1. collate DMA-related test cases
2. register functions into the suite
3. remove useless code from basic_test.c

gsgpu/dma: add test of msaa'resolve in dma-suite

gsgpu/dma: add test of mipmap'generate in dma-suite

gsgpu/dma: add test of tiled'copy in dma-suite

gsgpu/dma: refactor style of user message for xdma copy

drm/gsgpu: add hw sema basic ops

- Get / Put Hardware semaphoner

gsgpu/dma: add semaphore mode for all tests

- add semaphore mode in dma test and deadlock test
- fix bug of 'thread block' in deadlock test
- struct dma command package by fill object
- add cases of different configuration into test

gsgpu/deadlock: reduce poll command submit times

 due to loop in 'gsgpu_deadlock_helper' to submit command,
 makes the function execution take a lot of time, remove loop to fix this.

drm/gsgpu: remove author in gsgpu_drm.h
drm/gsgpu: rename gsgpu to LoongGPU LG110

Signed-off-by: liyi <liyi@loongson.cn>
Signed-off-by: wangjianfeng <wangjianfeng@loongson.cn>
Signed-off-by: suijingfeng <suijingfeng@loongson.cn>
Signed-off-by: Sui Jingfeng <suijingfeng@loongson.cn>
Signed-off-by: Mingcong Bai <jeffbai@aosc.io>
---
 data/gsgpu.ids               |    7 +
 data/meson.build             |    8 +
 gsgpu/gsgpu-symbol-check     |   79 ++
 gsgpu/gsgpu.h                | 1600 ++++++++++++++++++++++++++++++++++
 gsgpu/gsgpu_asic_id.c        |  165 ++++
 gsgpu/gsgpu_bo.c             |  727 +++++++++++++++
 gsgpu/gsgpu_cs.c             |  775 ++++++++++++++++
 gsgpu/gsgpu_device.c         |  334 +++++++
 gsgpu/gsgpu_gpu_info.c       |  281 ++++++
 gsgpu/gsgpu_hw_sema.c        |   64 ++
 gsgpu/gsgpu_internal.h       |  178 ++++
 gsgpu/gsgpu_vamgr.c          |  267 ++++++
 gsgpu/gsgpu_vm.c             |   53 ++
 gsgpu/libdrm_gsgpu.pc.in     |   11 +
 gsgpu/meson.build            |   65 ++
 gsgpu/util_hash.c            |  387 ++++++++
 gsgpu/util_hash.h            |  107 +++
 gsgpu/util_hash_table.c      |  262 ++++++
 gsgpu/util_hash_table.h      |   73 ++
 include/drm/gsgpu_drm.h      |  935 ++++++++++++++++++++
 meson.build                  |   10 +
 meson_options.txt            |    5 +
 tests/gsgpu/Makefile.am      |   28 +
 tests/gsgpu/basic_tests.c    | 1179 +++++++++++++++++++++++++
 tests/gsgpu/bo_tests.c       |  269 ++++++
 tests/gsgpu/deadlock_tests.c |  219 +++++
 tests/gsgpu/dma_tests.c      | 1231 ++++++++++++++++++++++++++
 tests/gsgpu/gsgpu_test.c     |  571 ++++++++++++
 tests/gsgpu/gsgpu_test.h     |  367 ++++++++
 tests/gsgpu/meson.build      |   33 +
 tests/gsgpu/vm_tests.c       |  173 ++++
 tests/meson.build            |    3 +
 32 files changed, 10466 insertions(+)
 create mode 100644 data/gsgpu.ids
 create mode 100755 gsgpu/gsgpu-symbol-check
 create mode 100644 gsgpu/gsgpu.h
 create mode 100644 gsgpu/gsgpu_asic_id.c
 create mode 100644 gsgpu/gsgpu_bo.c
 create mode 100644 gsgpu/gsgpu_cs.c
 create mode 100644 gsgpu/gsgpu_device.c
 create mode 100644 gsgpu/gsgpu_gpu_info.c
 create mode 100644 gsgpu/gsgpu_hw_sema.c
 create mode 100644 gsgpu/gsgpu_internal.h
 create mode 100644 gsgpu/gsgpu_vamgr.c
 create mode 100644 gsgpu/gsgpu_vm.c
 create mode 100644 gsgpu/libdrm_gsgpu.pc.in
 create mode 100644 gsgpu/meson.build
 create mode 100644 gsgpu/util_hash.c
 create mode 100644 gsgpu/util_hash.h
 create mode 100644 gsgpu/util_hash_table.c
 create mode 100644 gsgpu/util_hash_table.h
 create mode 100644 include/drm/gsgpu_drm.h
 create mode 100644 tests/gsgpu/Makefile.am
 create mode 100755 tests/gsgpu/basic_tests.c
 create mode 100644 tests/gsgpu/bo_tests.c
 create mode 100755 tests/gsgpu/deadlock_tests.c
 create mode 100644 tests/gsgpu/dma_tests.c
 create mode 100644 tests/gsgpu/gsgpu_test.c
 create mode 100755 tests/gsgpu/gsgpu_test.h
 create mode 100644 tests/gsgpu/meson.build
 create mode 100644 tests/gsgpu/vm_tests.c

diff --git a/data/gsgpu.ids b/data/gsgpu.ids
new file mode 100644
index 00000000..7a952e3d
--- /dev/null
+++ b/data/gsgpu.ids
@@ -0,0 +1,7 @@
+# List of GSGPU IDs
+#
+# Syntax:
+# device_id,	revision_id,	product_name        <-- single tab after comma
+
+1.0.0
+7A25,	1,	LOONGSON LoongGPU (TM) LG110
diff --git a/data/meson.build b/data/meson.build
index 9c26b66e..eb0d8fe2 100644
--- a/data/meson.build
+++ b/data/meson.build
@@ -25,3 +25,11 @@ if with_amdgpu
     install_dir : datadir_amdgpu,
   )
 endif
+
+if with_gsgpu
+  install_data(
+    'gsgpu.ids',
+    install_mode : 'rw-r--r--',
+    install_dir : datadir_gsgpu,
+  )
+endif
diff --git a/gsgpu/gsgpu-symbol-check b/gsgpu/gsgpu-symbol-check
new file mode 100755
index 00000000..84639318
--- /dev/null
+++ b/gsgpu/gsgpu-symbol-check
@@ -0,0 +1,79 @@
+#!/bin/bash
+
+# The following symbols (past the first five) are taken from the public headers.
+# A list of the latter should be available Makefile.am/libdrm_gsgpuinclude_HEADERS
+
+FUNCS=$($NM -D --format=bsd --defined-only ${1-.libs/libdrm_gsgpu.so} | awk '{print $3}' | while read func; do
+( grep -q "^$func$" || echo $func )  <<EOF
+__bss_start
+_edata
+_end
+_fini
+_init
+gsgpu_bo_alloc
+gsgpu_bo_cpu_map
+gsgpu_bo_cpu_unmap
+gsgpu_bo_export
+gsgpu_bo_free
+gsgpu_bo_import
+gsgpu_bo_list_create
+gsgpu_bo_list_destroy
+gsgpu_bo_list_update
+gsgpu_bo_query_info
+gsgpu_bo_set_metadata
+gsgpu_bo_va_op
+gsgpu_bo_va_op_raw
+gsgpu_bo_wait_for_idle
+gsgpu_create_bo_from_user_mem
+gsgpu_cs_chunk_fence_info_to_data
+gsgpu_cs_chunk_fence_to_dep
+gsgpu_cs_create_semaphore
+gsgpu_cs_create_syncobj
+gsgpu_cs_create_syncobj2
+gsgpu_cs_ctx_create
+gsgpu_cs_ctx_create2
+gsgpu_cs_ctx_free
+gsgpu_cs_destroy_semaphore
+gsgpu_cs_destroy_syncobj
+gsgpu_cs_export_syncobj
+gsgpu_cs_fence_to_handle
+gsgpu_cs_import_syncobj
+gsgpu_cs_query_fence_status
+gsgpu_cs_query_reset_state
+gsgpu_query_sw_info
+gsgpu_cs_signal_semaphore
+gsgpu_cs_submit
+gsgpu_cs_submit_raw
+gsgpu_cs_syncobj_export_sync_file
+gsgpu_cs_syncobj_import_sync_file
+gsgpu_cs_syncobj_reset
+gsgpu_cs_syncobj_signal
+gsgpu_cs_syncobj_wait
+gsgpu_cs_wait_fences
+gsgpu_cs_wait_semaphore
+gsgpu_device_deinitialize
+gsgpu_device_initialize
+gsgpu_get_marketing_name
+gsgpu_hw_sema_get
+gsgpu_hw_sema_put
+gsgpu_query_buffer_size_alignment
+gsgpu_query_crtc_from_id
+gsgpu_query_firmware_version
+gsgpu_query_gds_info
+gsgpu_query_gpu_info
+gsgpu_query_heap_info
+gsgpu_query_hw_ip_count
+gsgpu_query_hw_ip_info
+gsgpu_query_info
+gsgpu_query_sensor_info
+gsgpu_read_mm_registers
+gsgpu_va_range_alloc
+gsgpu_va_range_free
+gsgpu_va_range_query
+gsgpu_vm_reserve_vmid
+gsgpu_vm_unreserve_vmid
+EOF
+done)
+
+test ! -n "$FUNCS" || echo $FUNCS
+test ! -n "$FUNCS"
diff --git a/gsgpu/gsgpu.h b/gsgpu/gsgpu.h
new file mode 100644
index 00000000..2fc0bb71
--- /dev/null
+++ b/gsgpu/gsgpu.h
@@ -0,0 +1,1600 @@
+/*
+ * Copyright (C) 2021 Loongson Technology Co., Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+/**
+ * \file gsgpu.h
+ *
+ * Declare public libdrm_gsgpu API
+ *
+ * This file define API exposed by libdrm_gsgpu library.
+ * User wanted to use libdrm_gsgpu functionality must include
+ * this file.
+ *
+ */
+#ifndef _GSGPU_H_
+#define _GSGPU_H_
+
+#include <stdint.h>
+#include <stdbool.h>
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+struct drm_gsgpu_info_hw_ip;
+
+/*--------------------------------------------------------------------------*/
+/* --------------------------- Defines ------------------------------------ */
+/*--------------------------------------------------------------------------*/
+
+/**
+ * Define max. number of Command Buffers (IB) which could be sent to the single
+ * hardware IP to accommodate CE/DE requirements
+ *
+ * \sa gsgpu_cs_ib_info
+*/
+#define GSGPU_CS_MAX_IBS_PER_SUBMIT		4
+
+/**
+ * Special timeout value meaning that the timeout is infinite.
+ */
+#define GSGPU_TIMEOUT_INFINITE			0xffffffffffffffffull
+
+/**
+ * Used in gsgpu_cs_query_fence_status(), meaning that the given timeout
+ * is absolute.
+ */
+#define GSGPU_QUERY_FENCE_TIMEOUT_IS_ABSOLUTE     (1 << 0)
+
+/*--------------------------------------------------------------------------*/
+/* ----------------------------- Enums ------------------------------------ */
+/*--------------------------------------------------------------------------*/
+
+/**
+ * Enum describing possible handle types
+ *
+ * \sa gsgpu_bo_import, gsgpu_bo_export
+ *
+*/
+enum gsgpu_bo_handle_type {
+	/** GEM flink name (needs DRM authentication, used by DRI2) */
+	gsgpu_bo_handle_type_gem_flink_name = 0,
+
+	/** KMS handle which is used by all driver ioctls */
+	gsgpu_bo_handle_type_kms = 1,
+
+	/** DMA-buf fd handle */
+	gsgpu_bo_handle_type_dma_buf_fd = 2
+};
+
+/** Define known types of GPU VM VA ranges */
+enum gsgpu_gpu_va_range
+{
+	/** Allocate from "normal"/general range */
+	gsgpu_gpu_va_range_general = 0
+};
+
+enum gsgpu_sw_info {
+	gsgpu_sw_info_address32_hi = 0,
+};
+
+/*--------------------------------------------------------------------------*/
+/* -------------------------- Datatypes ----------------------------------- */
+/*--------------------------------------------------------------------------*/
+
+/**
+ * Define opaque pointer to context associated with fd.
+ * This context will be returned as the result of
+ * "initialize" function and should be pass as the first
+ * parameter to any API call
+ */
+typedef struct gsgpu_device *gsgpu_device_handle;
+
+/**
+ * Define GPU Context type as pointer to opaque structure
+ * Example of GPU Context is the "rendering" context associated
+ * with OpenGL context (glCreateContext)
+ */
+typedef struct gsgpu_context *gsgpu_context_handle;
+
+/**
+ * Define handle for gsgpu resources: buffer, GDS, etc.
+ */
+typedef struct gsgpu_bo *gsgpu_bo_handle;
+
+/**
+ * Define handle for list of BOs
+ */
+typedef struct gsgpu_bo_list *gsgpu_bo_list_handle;
+
+/**
+ * Define handle to be used to work with VA(virtual address) allocated ranges
+ */
+typedef struct gsgpu_va *gsgpu_va_handle;
+
+/**
+ * Define handle for semaphore
+ */
+typedef struct gsgpu_semaphore *gsgpu_semaphore_handle;
+
+/*--------------------------------------------------------------------------*/
+/* -------------------------- Structures ---------------------------------- */
+/*--------------------------------------------------------------------------*/
+
+/**
+ * Structure describing memory allocation request
+ *
+ * \sa gsgpu_bo_alloc()
+ *
+*/
+struct gsgpu_bo_alloc_request {
+	/** Allocation request. It must be aligned correctly. */
+	uint64_t alloc_size;
+
+	/**
+	 * It may be required to have some specific alignment requirements
+	 * for physical back-up storage (e.g. for displayable surface).
+	 * If 0 there is no special alignment requirement
+	 */
+	uint64_t phys_alignment;
+
+	/**
+	 * UMD should specify where to allocate memory and how it
+	 * will be accessed by the CPU.
+	 */
+	uint32_t preferred_heap;
+
+	/** Additional flags passed on allocation */
+	uint64_t flags;
+};
+
+/**
+ * Special UMD specific information associated with buffer.
+ *
+ * It may be need to pass some buffer charactersitic as part
+ * of buffer sharing. Such information are defined UMD and
+ * opaque for libdrm_gsgpu as well for kernel driver.
+ *
+ * \sa gsgpu_bo_set_metadata(), gsgpu_bo_query_info,
+ *     gsgpu_bo_import(), gsgpu_bo_export
+ *
+*/
+struct gsgpu_bo_metadata {
+	/** Special flag associated with surface */
+	uint64_t flags;
+
+	/**
+	 * ASIC-specific tiling information (also used by DCE).
+	 * The encoding is defined by the GSGPU_TILING_* definitions.
+	 */
+	uint64_t tiling_info;
+
+	/** Size of metadata associated with the buffer, in bytes. */
+	uint32_t size_metadata;
+
+	/** UMD specific metadata. Opaque for kernel */
+	uint32_t umd_metadata[64];
+};
+
+/**
+ * Structure describing allocated buffer. Client may need
+ * to query such information as part of 'sharing' buffers mechanism
+ *
+ * \sa gsgpu_bo_set_metadata(), gsgpu_bo_query_info(),
+ *     gsgpu_bo_import(), gsgpu_bo_export()
+*/
+struct gsgpu_bo_info {
+	/** Allocated memory size */
+	uint64_t alloc_size;
+
+	/**
+	 * It may be required to have some specific alignment requirements
+	 * for physical back-up storage.
+	 */
+	uint64_t phys_alignment;
+
+	/** Heap where to allocate memory. */
+	uint32_t preferred_heap;
+
+	/** Additional allocation flags. */
+	uint64_t alloc_flags;
+
+	/** Metadata associated with buffer if any. */
+	struct gsgpu_bo_metadata metadata;
+};
+
+/**
+ * Structure with information about "imported" buffer
+ *
+ * \sa gsgpu_bo_import()
+ *
+ */
+struct gsgpu_bo_import_result {
+	/** Handle of memory/buffer to use */
+	gsgpu_bo_handle buf_handle;
+
+	 /** Buffer size */
+	uint64_t alloc_size;
+};
+
+/**
+ *
+ * Structure to describe GDS partitioning information.
+ * \note OA and GWS resources are asscoiated with GDS partition
+ *
+ * \sa gsgpu_gpu_resource_query_gds_info
+ *
+*/
+struct gsgpu_gds_resource_info {
+	uint32_t gds_gfx_partition_size;
+	uint32_t compute_partition_size;
+	uint32_t gds_total_size;
+	uint32_t gws_per_gfx_partition;
+	uint32_t gws_per_compute_partition;
+	uint32_t oa_per_gfx_partition;
+	uint32_t oa_per_compute_partition;
+};
+
+/**
+ * Structure describing CS fence
+ *
+ * \sa gsgpu_cs_query_fence_status(), gsgpu_cs_request, gsgpu_cs_submit()
+ *
+*/
+struct gsgpu_cs_fence {
+
+	/** In which context IB was sent to execution */
+	gsgpu_context_handle context;
+
+	/** To which HW IP type the fence belongs */
+	uint32_t ip_type;
+
+	/** IP instance index if there are several IPs of the same type.   !!!Only One */
+	uint32_t ip_instance;
+
+	/** Ring index of the HW IP */
+	uint32_t ring;
+
+	/** Specify fence for which we need to check submission status.*/
+	uint64_t fence;
+};
+
+/**
+ * Structure describing IB
+ *
+ * \sa gsgpu_cs_request, gsgpu_cs_submit()
+ *
+*/
+struct gsgpu_cs_ib_info {
+	/** Special flags */
+	uint64_t flags;
+
+	/** Virtual MC address of the command buffer */
+	uint64_t ib_mc_address;
+
+	/**
+	 * Size of Command Buffer to be submitted.
+	 *   - The size is in units of dwords (4 bytes).
+	 *   - Could be 0
+	 */
+	uint32_t size;
+};
+
+/**
+ * Structure describing fence information
+ *
+ * \sa gsgpu_cs_request, gsgpu_cs_query_fence,
+ *     gsgpu_cs_submit(), gsgpu_cs_query_fence_status()
+*/
+struct gsgpu_cs_fence_info {
+	/** buffer object for the fence */
+	gsgpu_bo_handle handle;
+
+	/** fence offset in the unit of sizeof(uint64_t) */
+	uint64_t offset;
+};
+
+/**
+ * Structure describing submission request
+ *
+ * \note We could have several IBs as packet. e.g. CE, CE, DE case for gfx
+ *
+ * \sa gsgpu_cs_submit()
+*/
+struct gsgpu_cs_request {
+	/** Specify flags with additional information */
+	uint64_t flags;
+
+	/** Specify HW IP block type to which to send the IB. */
+	unsigned ip_type;
+
+	/** IP instance index if there are several IPs of the same type.  !!!!Only One*/
+	unsigned ip_instance;
+
+	/**
+	 * Specify ring index of the IP. We could have several rings
+	 * in the same IP. E.g. 0 for SDMA0 and 1 for SDMA1.
+	 */
+	uint32_t ring;
+
+	/**
+	 * List handle with resources used by this request.
+	 */
+	gsgpu_bo_list_handle resources;
+
+	/**
+	 * Number of dependencies this Command submission needs to
+	 * wait for before starting execution.
+	 */
+	uint32_t number_of_dependencies;
+
+	/**
+	 * Array of dependencies which need to be met before
+	 * execution can start.
+	 */
+	struct gsgpu_cs_fence *dependencies;
+
+	/** Number of IBs to submit in the field ibs. */
+	uint32_t number_of_ibs;
+
+	/**
+	 * IBs to submit. Those IBs will be submit together as single entity
+	 */
+	struct gsgpu_cs_ib_info *ibs;
+
+	/**
+	 * The returned sequence number for the command submission
+	 */
+	uint64_t seq_no;
+
+	/**
+	 * The fence information
+	 */
+	struct gsgpu_cs_fence_info fence_info;
+};
+
+/**
+ * Structure which provide information about GPU VM MC Address space
+ * alignments requirements
+ *
+ * \sa gsgpu_query_buffer_size_alignment
+ */
+struct gsgpu_buffer_size_alignments {
+	/** Size alignment requirement for allocation in
+	 * local memory */
+	uint64_t size_local;
+
+	/**
+	 * Size alignment requirement for allocation in remote memory
+	 */
+	uint64_t size_remote;
+};
+
+/**
+ * Structure which provide information about heap
+ *
+ * \sa gsgpu_query_heap_info()
+ *
+ */
+struct gsgpu_heap_info {
+	/** Theoretical max. available memory in the given heap */
+	uint64_t heap_size;
+
+	/**
+	 * Number of bytes allocated in the heap. This includes all processes
+	 * and private allocations in the kernel. It changes when new buffers
+	 * are allocated, freed, and moved. It cannot be larger than
+	 * heap_size.
+	 */
+	uint64_t heap_usage;
+
+	/**
+	 * Theoretical possible max. size of buffer which
+	 * could be allocated in the given heap
+	 */
+	uint64_t max_allocation;
+};
+
+/**
+ * Describe GPU h/w info needed for UMD correct initialization
+ *
+ * \sa gsgpu_query_gpu_info()
+*/
+struct gsgpu_gpu_info {
+	/** Asic id */
+	uint32_t asic_id;
+	/** Chip revision */
+	uint32_t chip_rev;
+	/** Chip external revision */
+	uint32_t chip_external_rev;
+	/** Family ID */
+	uint32_t family_id;
+	/** Special flags */
+	uint64_t ids_flags;
+	/** max engine clock*/
+	uint64_t max_engine_clk;
+	/** max memory clock */
+	uint64_t max_memory_clk;
+	/** number of shader engines */
+	uint32_t num_shader_engines;
+	/** number of shader arrays per engine */
+	uint32_t num_shader_arrays_per_engine;
+	/**  Number of available good shader pipes */
+	uint32_t avail_quad_shader_pipes;
+	/**  Max. number of shader pipes.(including good and bad pipes  */
+	uint32_t max_quad_shader_pipes;
+	/** Number of parameter cache entries per shader quad pipe */
+	uint32_t cache_entries_per_quad_pipe;
+	/**  Number of available graphics context */
+	uint32_t num_hw_gfx_contexts;
+	/** Number of render backend pipes */
+	uint32_t rb_pipes;
+	/**  Enabled render backend pipe mask */
+	uint32_t enabled_rb_pipes_mask;
+	/** Frequency of GPU Counter */
+	uint32_t gpu_counter_freq;
+	/** CC_RB_BACKEND_DISABLE.BACKEND_DISABLE per SE */
+	uint32_t backend_disable[4];
+	/** Value of MC_ARB_RAMCFG register*/
+	uint32_t mc_arb_ramcfg;
+	/** Value of GB_ADDR_CONFIG */
+	uint32_t gb_addr_cfg;
+	/** Values of the GB_TILE_MODE0..31 registers */
+	uint32_t gb_tile_mode[32];
+	/** Values of GB_MACROTILE_MODE0..15 registers */
+	uint32_t gb_macro_tile_mode[16];
+	/** Value of PA_SC_RASTER_CONFIG register per SE */
+	uint32_t pa_sc_raster_cfg[4];
+	/** Value of PA_SC_RASTER_CONFIG_1 register per SE */
+	uint32_t pa_sc_raster_cfg1[4];
+	/* CU info */
+	uint32_t cu_active_number;
+	uint32_t cu_ao_mask;
+	uint32_t cu_bitmap[4][4];
+	/* video memory type info*/
+	uint32_t vram_type;
+	/* video memory bit width*/
+	uint32_t vram_bit_width;
+	/** constant engine ram size*/
+	uint32_t ce_ram_size;
+	/* vce harvesting instance */
+	uint32_t vce_harvest_config;
+	/* PCI revision ID */
+	uint32_t pci_rev_id;
+};
+
+
+/*--------------------------------------------------------------------------*/
+/*------------------------- Functions --------------------------------------*/
+/*--------------------------------------------------------------------------*/
+
+/*
+ * Initialization / Cleanup
+ *
+*/
+
+/**
+ *
+ * \param   fd            - \c [in]  File descriptor for GS GPU device
+ *                                   received previously as the result of
+ *                                   e.g. drmOpen() call.
+ *                                   For legacy fd type, the DRI2/DRI3
+ *                                   authentication should be done before
+ *                                   calling this function.
+ * \param   major_version - \c [out] Major version of library. It is assumed
+ *                                   that adding new functionality will cause
+ *                                   increase in major version
+ * \param   minor_version - \c [out] Minor version of library
+ * \param   device_handle - \c [out] Pointer to opaque context which should
+ *                                   be passed as the first parameter on each
+ *                                   API call
+ *
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+ *
+ * \sa gsgpu_device_deinitialize()
+*/
+int gsgpu_device_initialize(int fd,
+			     uint32_t *major_version,
+			     uint32_t *minor_version,
+			     gsgpu_device_handle *device_handle);
+
+/**
+ *
+ * When access to such library does not needed any more the special
+ * function must be call giving opportunity to clean up any
+ * resources if needed.
+ *
+ * \param   device_handle - \c [in]  Context associated with file
+ *                                   descriptor for GS GPU device
+ *                                   received previously as the
+ *                                   result e.g. of drmOpen() call.
+ *
+ * \return  0 on success\n
+ *         <0 - Negative POSIX Error code
+ *
+ * \sa gsgpu_device_initialize()
+ *
+*/
+int gsgpu_device_deinitialize(gsgpu_device_handle device_handle);
+
+/*
+ * Memory Management
+ *
+*/
+
+/**
+ * Allocate memory to be used by UMD for GPU related operations
+ *
+ * \param   dev		 - \c [in] Device handle.
+ *				   See #gsgpu_device_initialize()
+ * \param   alloc_buffer - \c [in] Pointer to the structure describing an
+ *				   allocation request
+ * \param   buf_handle	- \c [out] Allocated buffer handle
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+ * \sa gsgpu_bo_free()
+*/
+int gsgpu_bo_alloc(gsgpu_device_handle dev,
+		    struct gsgpu_bo_alloc_request *alloc_buffer,
+		    gsgpu_bo_handle *buf_handle);
+
+/**
+ * Associate opaque data with buffer to be queried by another UMD
+ *
+ * \param   dev	       - \c [in] Device handle. See #gsgpu_device_initialize()
+ * \param   buf_handle - \c [in] Buffer handle
+ * \param   info       - \c [in] Metadata to associated with buffer
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+*/
+int gsgpu_bo_set_metadata(gsgpu_bo_handle buf_handle,
+			   struct gsgpu_bo_metadata *info);
+
+/**
+ * Query buffer information including metadata previusly associated with
+ * buffer.
+ *
+ * \param   dev	       - \c [in] Device handle.
+ *				 See #gsgpu_device_initialize()
+ * \param   buf_handle - \c [in]   Buffer handle
+ * \param   info       - \c [out]  Structure describing buffer
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+ * \sa gsgpu_bo_set_metadata(), gsgpu_bo_alloc()
+*/
+int gsgpu_bo_query_info(gsgpu_bo_handle buf_handle,
+			 struct gsgpu_bo_info *info);
+
+/**
+ * Allow others to get access to buffer
+ *
+ * \param   dev		  - \c [in] Device handle.
+ *				    See #gsgpu_device_initialize()
+ * \param   buf_handle    - \c [in] Buffer handle
+ * \param   type          - \c [in] Type of handle requested
+ * \param   shared_handle - \c [out] Special "shared" handle
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+ * \sa gsgpu_bo_import()
+ *
+*/
+int gsgpu_bo_export(gsgpu_bo_handle buf_handle,
+		     enum gsgpu_bo_handle_type type,
+		     uint32_t *shared_handle);
+
+/**
+ * Request access to "shared" buffer
+ *
+ * \param   dev		  - \c [in] Device handle.
+ *				    See #gsgpu_device_initialize()
+ * \param   type	  - \c [in] Type of handle requested
+ * \param   shared_handle - \c [in] Shared handle received as result "import"
+ *				     operation
+ * \param   output        - \c [out] Pointer to structure with information
+ *				     about imported buffer
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+ * \note  Buffer must be "imported" only using new "fd" (different from
+ *	  one used by "exporter").
+ *
+ * \sa gsgpu_bo_export()
+ *
+*/
+int gsgpu_bo_import(gsgpu_device_handle dev,
+		     enum gsgpu_bo_handle_type type,
+		     uint32_t shared_handle,
+		     struct gsgpu_bo_import_result *output);
+
+/**
+ * Request GPU access to user allocated memory e.g. via "malloc"
+ *
+ * \param dev - [in] Device handle. See #gsgpu_device_initialize()
+ * \param cpu - [in] CPU address of user allocated memory which we
+ * want to map to GPU address space (make GPU accessible)
+ * (This address must be correctly aligned).
+ * \param size - [in] Size of allocation (must be correctly aligned)
+ * \param buf_handle - [out] Buffer handle for the userptr memory
+ * resource on submission and be used in other operations.
+ *
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+ * \note
+ * This call doesn't guarantee that such memory will be persistently
+ * "locked" / make non-pageable. The purpose of this call is to provide
+ * opportunity for GPU get access to this resource during submission.
+ *
+ * The maximum amount of memory which could be mapped in this call depends
+ * if overcommit is disabled or not. If overcommit is disabled than the max.
+ * amount of memory to be pinned will be limited by left "free" size in total
+ * amount of memory which could be locked simultaneously ("GART" size).
+ *
+ * Supported (theoretical) max. size of mapping is restricted only by
+ * "GART" size.
+ *
+ * It is responsibility of caller to correctly specify access rights
+ * on VA assignment.
+*/
+int gsgpu_create_bo_from_user_mem(gsgpu_device_handle dev,
+				    void *cpu, uint64_t size,
+				    gsgpu_bo_handle *buf_handle);
+
+/**
+ * Free previosuly allocated memory
+ *
+ * \param   dev	       - \c [in] Device handle. See #gsgpu_device_initialize()
+ * \param   buf_handle - \c [in]  Buffer handle to free
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+ * \note In the case of memory shared between different applications all
+ *	 resources will be “physically” freed only all such applications
+ *	 will be terminated
+ * \note If is UMD responsibility to ‘free’ buffer only when there is no
+ *	 more GPU access
+ *
+ * \sa gsgpu_bo_set_metadata(), gsgpu_bo_alloc()
+ *
+*/
+int gsgpu_bo_free(gsgpu_bo_handle buf_handle);
+
+/**
+ * Request CPU access to GPU accessible memory
+ *
+ * \param   buf_handle - \c [in] Buffer handle
+ * \param   cpu        - \c [out] CPU address to be used for access
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+ * \sa gsgpu_bo_cpu_unmap()
+ *
+*/
+int gsgpu_bo_cpu_map(gsgpu_bo_handle buf_handle, void **cpu);
+
+/**
+ * Release CPU access to GPU memory
+ *
+ * \param   buf_handle  - \c [in] Buffer handle
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+ * \sa gsgpu_bo_cpu_map()
+ *
+*/
+int gsgpu_bo_cpu_unmap(gsgpu_bo_handle buf_handle);
+
+/**
+ * Wait until a buffer is not used by the device.
+ *
+ * \param   dev           - \c [in] Device handle. See #gsgpu_device_initialize()
+ * \param   buf_handle    - \c [in] Buffer handle.
+ * \param   timeout_ns    - Timeout in nanoseconds.
+ * \param   buffer_busy   - 0 if buffer is idle, all GPU access was completed
+ *                            and no GPU access is scheduled.
+ *                          1 GPU access is in fly or scheduled
+ *
+ * \return   0 - on success
+ *          <0 - Negative POSIX Error code
+ */
+int gsgpu_bo_wait_for_idle(gsgpu_bo_handle buf_handle,
+			    uint64_t timeout_ns,
+			    bool *buffer_busy);
+
+/**
+ * Creates a BO list handle for command submission.
+ *
+ * \param   dev			- \c [in] Device handle.
+ *				   See #gsgpu_device_initialize()
+ * \param   number_of_resources	- \c [in] Number of BOs in the list
+ * \param   resources		- \c [in] List of BO handles
+ * \param   resource_prios	- \c [in] Optional priority for each handle
+ * \param   result		- \c [out] Created BO list handle
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+ * \sa gsgpu_bo_list_destroy()
+*/
+int gsgpu_bo_list_create(gsgpu_device_handle dev,
+			  uint32_t number_of_resources,
+			  gsgpu_bo_handle *resources,
+			  uint8_t *resource_prios,
+			  gsgpu_bo_list_handle *result);
+
+/**
+ * Destroys a BO list handle.
+ *
+ * \param   handle	- \c [in] BO list handle.
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+ * \sa gsgpu_bo_list_create()
+*/
+int gsgpu_bo_list_destroy(gsgpu_bo_list_handle handle);
+
+/**
+ * Update resources for existing BO list
+ *
+ * \param   handle              - \c [in] BO list handle
+ * \param   number_of_resources - \c [in] Number of BOs in the list
+ * \param   resources           - \c [in] List of BO handles
+ * \param   resource_prios      - \c [in] Optional priority for each handle
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+ * \sa gsgpu_bo_list_update()
+*/
+int gsgpu_bo_list_update(gsgpu_bo_list_handle handle,
+			  uint32_t number_of_resources,
+			  gsgpu_bo_handle *resources,
+			  uint8_t *resource_prios);
+
+/*
+ * GPU Execution context
+ *
+*/
+
+/**
+ * Create GPU execution Context
+ *
+ * For the purpose of GPU Scheduler and GPU Robustness extensions it is
+ * necessary to have information/identify rendering/compute contexts.
+ * It also may be needed to associate some specific requirements with such
+ * contexts.  Kernel driver will guarantee that submission from the same
+ * context will always be executed in order (first come, first serve).
+ *
+ *
+ * \param   dev      - \c [in] Device handle. See #gsgpu_device_initialize()
+ * \param   priority - \c [in] Context creation flags. See GSGPU_CTX_PRIORITY_*
+ * \param   context  - \c [out] GPU Context handle
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+ * \sa gsgpu_cs_ctx_free()
+ *
+*/
+int gsgpu_cs_ctx_create2(gsgpu_device_handle dev,
+			 uint32_t priority,
+			 gsgpu_context_handle *context);
+/**
+ * Create GPU execution Context
+ *
+ * Refer to gsgpu_cs_ctx_create2 for full documentation. This call
+ * is missing the priority parameter.
+ *
+ * \sa gsgpu_cs_ctx_create2()
+ *
+*/
+int gsgpu_cs_ctx_create(gsgpu_device_handle dev,
+			 gsgpu_context_handle *context);
+
+/**
+ *
+ * Destroy GPU execution context when not needed any more
+ *
+ * \param   context - \c [in] GPU Context handle
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+ * \sa gsgpu_cs_ctx_create()
+ *
+*/
+int gsgpu_cs_ctx_free(gsgpu_context_handle context);
+
+/**
+ * Query reset state for the specific GPU Context
+ *
+ * \param   context - \c [in]  GPU Context handle
+ * \param   state   - \c [out] One of GSGPU_CTX_*_RESET
+ * \param   hangs   - \c [out] Number of hangs caused by the context.
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+ * \sa gsgpu_cs_ctx_create()
+ *
+*/
+int gsgpu_cs_query_reset_state(gsgpu_context_handle context,
+				uint32_t *state, uint32_t *hangs);
+
+/*
+ * Command Buffers Management
+ *
+*/
+
+/**
+ * Send request to submit command buffers to hardware.
+ *
+ * Kernel driver could use GPU Scheduler to make decision when physically
+ * sent this request to the hardware. Accordingly this request could be put
+ * in queue and sent for execution later. The only guarantee is that request
+ * from the same GPU context to the same ip:ip_instance:ring will be executed in
+ * order.
+ *
+ * The caller can specify the user fence buffer/location with the fence_info in the
+ * cs_request.The sequence number is returned via the 'seq_no' parameter
+ * in ibs_request structure.
+ *
+ *
+ * \param   dev		       - \c [in]  Device handle.
+ *					  See #gsgpu_device_initialize()
+ * \param   context            - \c [in]  GPU Context
+ * \param   flags              - \c [in]  Global submission flags
+ * \param   ibs_request        - \c [in/out] Pointer to submission requests.
+ *					  We could submit to the several
+ *					  engines/rings simulteniously as
+ *					  'atomic' operation
+ * \param   number_of_requests - \c [in]  Number of submission requests
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+ * \note It is required to pass correct resource list with buffer handles
+ *	 which will be accessible by command buffers from submission
+ *	 This will allow kernel driver to correctly implement "paging".
+ *	 Failure to do so will have unpredictable results.
+ *
+ * \sa gsgpu_command_buffer_alloc(), gsgpu_command_buffer_free(),
+ *     gsgpu_cs_query_fence_status()
+ *
+*/
+int gsgpu_cs_submit(gsgpu_context_handle context,
+		     uint64_t flags,
+		     struct gsgpu_cs_request *ibs_request,
+		     uint32_t number_of_requests);
+
+/**
+ *  Query status of Command Buffer Submission
+ *
+ * \param   fence   - \c [in] Structure describing fence to query
+ * \param   timeout_ns - \c [in] Timeout value to wait
+ * \param   flags   - \c [in] Flags for the query
+ * \param   expired - \c [out] If fence expired or not.\n
+ *				0  – if fence is not expired\n
+ *				!0 - otherwise
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+ * \note If UMD wants only to check operation status and returned immediately
+ *	 then timeout value as 0 must be passed. In this case success will be
+ *	 returned in the case if submission was completed or timeout error
+ *	 code.
+ *
+ * \sa gsgpu_cs_submit()
+*/
+int gsgpu_cs_query_fence_status(struct gsgpu_cs_fence *fence,
+				 uint64_t timeout_ns,
+				 uint64_t flags,
+				 uint32_t *expired);
+
+/**
+ *  Wait for multiple fences
+ *
+ * \param   fences      - \c [in] The fence array to wait
+ * \param   fence_count - \c [in] The fence count
+ * \param   wait_all    - \c [in] If true, wait all fences to be signaled,
+ *                                otherwise, wait at least one fence
+ * \param   timeout_ns  - \c [in] The timeout to wait, in nanoseconds
+ * \param   status      - \c [out] '1' for signaled, '0' for timeout
+ * \param   first       - \c [out] the index of the first signaled fence from @fences
+ *
+ * \return  0 on success
+ *          <0 - Negative POSIX Error code
+ *
+ * \note    Currently it supports only one gsgpu_device. All fences come from
+ *          the same gsgpu_device with the same fd.
+*/
+int gsgpu_cs_wait_fences(struct gsgpu_cs_fence *fences,
+			  uint32_t fence_count,
+			  bool wait_all,
+			  uint64_t timeout_ns,
+			  uint32_t *status, uint32_t *first);
+
+/*
+ * Query / Info API
+ *
+*/
+
+/**
+ * Query allocation size alignments
+ *
+ * UMD should query information about GPU VM MC size alignments requirements
+ * to be able correctly choose required allocation size and implement
+ * internal optimization if needed.
+ *
+ * \param   dev  - \c [in] Device handle. See #gsgpu_device_initialize()
+ * \param   info - \c [out] Pointer to structure to get size alignment
+ *			  requirements
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+*/
+int gsgpu_query_buffer_size_alignment(gsgpu_device_handle dev,
+				       struct gsgpu_buffer_size_alignments
+						*info);
+
+/**
+ * Query firmware versions
+ *
+ * \param   dev	        - \c [in] Device handle. See #gsgpu_device_initialize()
+ * \param   fw_type     - \c [in] GSGPU_INFO_FW_*
+ * \param   ip_instance - \c [in] Index of the IP block of the same type.
+ * \param   index       - \c [in] Index of the engine. (for SDMA and MEC)
+ * \param   version     - \c [out] Pointer to to the "version" return value
+ * \param   feature     - \c [out] Pointer to to the "feature" return value
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+*/
+int gsgpu_query_firmware_version(gsgpu_device_handle dev, unsigned fw_type,
+				  unsigned ip_instance, unsigned index,
+				  uint32_t *version, uint32_t *feature);
+
+/**
+ * Query the number of HW IP instances of a certain type.
+ *
+ * \param   dev      - \c [in] Device handle. See #gsgpu_device_initialize()
+ * \param   type     - \c [in] Hardware IP block type = GSGPU_HW_IP_*
+ * \param   count    - \c [out] Pointer to structure to get information
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+*/
+int gsgpu_query_hw_ip_count(gsgpu_device_handle dev, unsigned type,
+			     uint32_t *count);
+
+/**
+ * Query engine information
+ *
+ * This query allows UMD to query information different engines and their
+ * capabilities.
+ *
+ * \param   dev         - \c [in] Device handle. See #gsgpu_device_initialize()
+ * \param   type        - \c [in] Hardware IP block type = GSGPU_HW_IP_*
+ * \param   ip_instance - \c [in] Index of the IP block of the same type.
+ * \param   info        - \c [out] Pointer to structure to get information
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+*/
+int gsgpu_query_hw_ip_info(gsgpu_device_handle dev, unsigned type,
+			    unsigned ip_instance,
+			    struct drm_gsgpu_info_hw_ip *info);
+
+/**
+ * Query heap information
+ *
+ * This query allows UMD to query potentially available memory resources and
+ * adjust their logic if necessary.
+ *
+ * \param   dev  - \c [in] Device handle. See #gsgpu_device_initialize()
+ * \param   heap - \c [in] Heap type
+ * \param   info - \c [in] Pointer to structure to get needed information
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+*/
+int gsgpu_query_heap_info(gsgpu_device_handle dev, uint32_t heap,
+			   uint32_t flags, struct gsgpu_heap_info *info);
+
+/**
+ * Get the CRTC ID from the mode object ID
+ *
+ * \param   dev    - \c [in] Device handle. See #gsgpu_device_initialize()
+ * \param   id     - \c [in] Mode object ID
+ * \param   result - \c [in] Pointer to the CRTC ID
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+*/
+int gsgpu_query_crtc_from_id(gsgpu_device_handle dev, unsigned id,
+			      int32_t *result);
+
+/**
+ * Query GPU H/w Info
+ *
+ * Query hardware specific information
+ *
+ * \param   dev  - \c [in] Device handle. See #gsgpu_device_initialize()
+ * \param   heap - \c [in] Heap type
+ * \param   info - \c [in] Pointer to structure to get needed information
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+*/
+int gsgpu_query_gpu_info(gsgpu_device_handle dev,
+			   struct gsgpu_gpu_info *info);
+
+/**
+ * Query hardware or driver information.
+ *
+ * The return size is query-specific and depends on the "info_id" parameter.
+ * No more than "size" bytes is returned.
+ *
+ * \param   dev     - \c [in] Device handle. See #gsgpu_device_initialize()
+ * \param   info_id - \c [in] GSGPU_INFO_*
+ * \param   size    - \c [in] Size of the returned value.
+ * \param   value   - \c [out] Pointer to the return value.
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX error code
+ *
+*/
+int gsgpu_query_info(gsgpu_device_handle dev, unsigned info_id,
+		      unsigned size, void *value);
+
+/**
+ * Query hardware or driver information.
+ *
+ * The return size is query-specific and depends on the "info_id" parameter.
+ * No more than "size" bytes is returned.
+ *
+ * \param   dev     - \c [in] Device handle. See #gsgpu_device_initialize()
+ * \param   info    - \c [in] gsgpu_sw_info_*
+ * \param   value   - \c [out] Pointer to the return value.
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX error code
+ *
+*/
+int gsgpu_query_sw_info(gsgpu_device_handle dev, enum gsgpu_sw_info info,
+			 void *value);
+
+/**
+ * Query information about GDS
+ *
+ * \param   dev	     - \c [in] Device handle. See #gsgpu_device_initialize()
+ * \param   gds_info - \c [out] Pointer to structure to get GDS information
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+*/
+int gsgpu_query_gds_info(gsgpu_device_handle dev,
+			struct gsgpu_gds_resource_info *gds_info);
+
+/**
+ * Query information about sensor.
+ *
+ * The return size is query-specific and depends on the "sensor_type"
+ * parameter. No more than "size" bytes is returned.
+ *
+ * \param   dev         - \c [in] Device handle. See #gsgpu_device_initialize()
+ * \param   sensor_type - \c [in] GSGPU_INFO_SENSOR_*
+ * \param   size        - \c [in] Size of the returned value.
+ * \param   value       - \c [out] Pointer to the return value.
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+*/
+int gsgpu_query_sensor_info(gsgpu_device_handle dev, unsigned sensor_type,
+			     unsigned size, void *value);
+
+/**
+ * Read a set of consecutive memory-mapped registers.
+ * Not all registers are allowed to be read by userspace.
+ *
+ * \param   dev          - \c [in] Device handle. See #gsgpu_device_initialize(
+ * \param   dword_offset - \c [in] Register offset in dwords
+ * \param   count        - \c [in] The number of registers to read starting
+ *                                 from the offset
+ * \param   instance     - \c [in] GRBM_GFX_INDEX selector. It may have other
+ *                                 uses. Set it to 0xffffffff if unsure.
+ * \param   flags        - \c [in] Flags with additional information.
+ * \param   values       - \c [out] The pointer to return values.
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX error code
+ *
+*/
+int gsgpu_read_mm_registers(gsgpu_device_handle dev, unsigned dword_offset,
+			     unsigned count, uint32_t instance, uint32_t flags,
+			     uint32_t *values);
+
+/**
+ * Flag to request VA address range in the 32bit address space
+*/
+#define GSGPU_VA_RANGE_32_BIT		0x1
+#define GSGPU_VA_RANGE_HIGH		0x2
+
+/**
+ * Allocate virtual address range
+ *
+ * \param dev - [in] Device handle. See #gsgpu_device_initialize()
+ * \param va_range_type - \c [in] Type of MC va range from which to allocate
+ * \param size - \c [in] Size of range. Size must be correctly* aligned.
+ * It is client responsibility to correctly aligned size based on the future
+ * usage of allocated range.
+ * \param va_base_alignment - \c [in] Overwrite base address alignment
+ * requirement for GPU VM MC virtual
+ * address assignment. Must be multiple of size alignments received as
+ * 'gsgpu_buffer_size_alignments'.
+ * If 0 use the default one.
+ * \param va_base_required - \c [in] Specified required va base address.
+ * If 0 then library choose available one.
+ * If !0 value will be passed and those value already "in use" then
+ * corresponding error status will be returned.
+ * \param va_base_allocated - \c [out] On return: Allocated VA base to be used
+ * by client.
+ * \param va_range_handle - \c [out] On return: Handle assigned to allocation
+ * \param flags - \c [in] flags for special VA range
+ *
+ * \return 0 on success\n
+ * >0 - GS specific error code\n
+ * <0 - Negative POSIX Error code
+ *
+ * \notes \n
+ * It is client responsibility to correctly handle VA assignments and usage.
+ * Neither kernel driver nor libdrm_amdpgu are able to prevent and
+ * detect wrong va assignemnt.
+ *
+ * It is client responsibility to correctly handle multi-GPU cases and to pass
+ * the corresponding arrays of all devices handles where corresponding VA will
+ * be used.
+ *
+*/
+int gsgpu_va_range_alloc(gsgpu_device_handle dev,
+			   enum gsgpu_gpu_va_range va_range_type,
+			   uint64_t size,
+			   uint64_t va_base_alignment,
+			   uint64_t va_base_required,
+			   uint64_t *va_base_allocated,
+			   gsgpu_va_handle *va_range_handle,
+			   uint64_t flags);
+
+/**
+ * Free previously allocated virtual address range
+ *
+ *
+ * \param va_range_handle - \c [in] Handle assigned to VA allocation
+ *
+ * \return 0 on success\n
+ * >0 - GS specific error code\n
+ * <0 - Negative POSIX Error code
+ *
+*/
+int gsgpu_va_range_free(gsgpu_va_handle va_range_handle);
+
+/**
+* Query virtual address range
+*
+* UMD can query GPU VM range supported by each device
+* to initialize its own VAM accordingly.
+*
+* \param   dev    - [in] Device handle. See #gsgpu_device_initialize()
+* \param   type   - \c [in] Type of virtual address range
+* \param   offset - \c [out] Start offset of virtual address range
+* \param   size   - \c [out] Size of virtual address range
+*
+* \return   0 on success\n
+*          <0 - Negative POSIX Error code
+*
+*/
+
+int gsgpu_va_range_query(gsgpu_device_handle dev,
+			  enum gsgpu_gpu_va_range type,
+			  uint64_t *start,
+			  uint64_t *end);
+
+/**
+ *  VA mapping/unmapping for the buffer object
+ *
+ * \param  bo		- \c [in] BO handle
+ * \param  offset	- \c [in] Start offset to map
+ * \param  size		- \c [in] Size to map
+ * \param  addr		- \c [in] Start virtual address.
+ * \param  flags	- \c [in] Supported flags for mapping/unmapping
+ * \param  ops		- \c [in] GSGPU_VA_OP_MAP or GSGPU_VA_OP_UNMAP
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+*/
+
+int gsgpu_bo_va_op(gsgpu_bo_handle bo,
+		    uint64_t offset,
+		    uint64_t size,
+		    uint64_t addr,
+		    uint64_t flags,
+		    uint32_t ops);
+
+/**
+ *  VA mapping/unmapping for a buffer object or PRT region.
+ *
+ * This is not a simple drop-in extension for gsgpu_bo_va_op; instead, all
+ * parameters are treated "raw", i.e. size is not automatically aligned, and
+ * all flags must be specified explicitly.
+ *
+ * \param  dev		- \c [in] device handle
+ * \param  bo		- \c [in] BO handle (may be NULL)
+ * \param  offset	- \c [in] Start offset to map
+ * \param  size		- \c [in] Size to map
+ * \param  addr		- \c [in] Start virtual address.
+ * \param  flags	- \c [in] Supported flags for mapping/unmapping
+ * \param  ops		- \c [in] GSGPU_VA_OP_MAP or GSGPU_VA_OP_UNMAP
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+*/
+
+int gsgpu_bo_va_op_raw(gsgpu_device_handle dev,
+			gsgpu_bo_handle bo,
+			uint64_t offset,
+			uint64_t size,
+			uint64_t addr,
+			uint64_t flags,
+			uint32_t ops);
+
+/**
+ *  create semaphore
+ *
+ * \param   sem	   - \c [out] semaphore handle
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+*/
+int gsgpu_cs_create_semaphore(gsgpu_semaphore_handle *sem);
+
+/**
+ *  signal semaphore
+ *
+ * \param   context        - \c [in] GPU Context
+ * \param   ip_type        - \c [in] Hardware IP block type = GSGPU_HW_IP_*
+ * \param   ip_instance    - \c [in] Index of the IP block of the same type
+ * \param   ring           - \c [in] Specify ring index of the IP
+ * \param   sem	           - \c [in] semaphore handle
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+*/
+int gsgpu_cs_signal_semaphore(gsgpu_context_handle ctx,
+			       uint32_t ip_type,
+			       uint32_t ip_instance,
+			       uint32_t ring,
+			       gsgpu_semaphore_handle sem);
+
+/**
+ *  wait semaphore
+ *
+ * \param   context        - \c [in] GPU Context
+ * \param   ip_type        - \c [in] Hardware IP block type = GSGPU_HW_IP_*
+ * \param   ip_instance    - \c [in] Index of the IP block of the same type
+ * \param   ring           - \c [in] Specify ring index of the IP
+ * \param   sem	           - \c [in] semaphore handle
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+*/
+int gsgpu_cs_wait_semaphore(gsgpu_context_handle ctx,
+			     uint32_t ip_type,
+			     uint32_t ip_instance,
+			     uint32_t ring,
+			     gsgpu_semaphore_handle sem);
+
+/**
+ *  destroy semaphore
+ *
+ * \param   sem	    - \c [in] semaphore handle
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+*/
+int gsgpu_cs_destroy_semaphore(gsgpu_semaphore_handle sem);
+
+/**
+ *  Get the ASIC marketing name
+ *
+ * \param   dev         - \c [in] Device handle. See #gsgpu_device_initialize()
+ *
+ * \return  the constant string of the marketing name
+ *          "NULL" means the ASIC is not found
+*/
+const char *gsgpu_get_marketing_name(gsgpu_device_handle dev);
+
+/**
+ *  Create kernel sync object
+ *
+ * \param   dev         - \c [in]  device handle
+ * \param   flags       - \c [in]  flags that affect creation
+ * \param   syncobj     - \c [out] sync object handle
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+*/
+int gsgpu_cs_create_syncobj2(gsgpu_device_handle dev,
+			      uint32_t  flags,
+			      uint32_t *syncobj);
+
+/**
+ *  Create kernel sync object
+ *
+ * \param   dev	      - \c [in]  device handle
+ * \param   syncobj   - \c [out] sync object handle
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+*/
+int gsgpu_cs_create_syncobj(gsgpu_device_handle dev,
+			     uint32_t *syncobj);
+/**
+ *  Destroy kernel sync object
+ *
+ * \param   dev	    - \c [in] device handle
+ * \param   syncobj - \c [in] sync object handle
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+*/
+int gsgpu_cs_destroy_syncobj(gsgpu_device_handle dev,
+			      uint32_t syncobj);
+
+/**
+ * Reset kernel sync objects to unsignalled state.
+ *
+ * \param dev           - \c [in] device handle
+ * \param syncobjs      - \c [in] array of sync object handles
+ * \param syncobj_count - \c [in] number of handles in syncobjs
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+*/
+int gsgpu_cs_syncobj_reset(gsgpu_device_handle dev,
+			    const uint32_t *syncobjs, uint32_t syncobj_count);
+
+/**
+ * Signal kernel sync objects.
+ *
+ * \param dev           - \c [in] device handle
+ * \param syncobjs      - \c [in] array of sync object handles
+ * \param syncobj_count - \c [in] number of handles in syncobjs
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+*/
+int gsgpu_cs_syncobj_signal(gsgpu_device_handle dev,
+			     const uint32_t *syncobjs, uint32_t syncobj_count);
+
+/**
+ *  Wait for one or all sync objects to signal.
+ *
+ * \param   dev	    - \c [in] self-explanatory
+ * \param   handles - \c [in] array of sync object handles
+ * \param   num_handles - \c [in] self-explanatory
+ * \param   timeout_nsec - \c [in] self-explanatory
+ * \param   flags   - \c [in] a bitmask of DRM_SYNCOBJ_WAIT_FLAGS_*
+ * \param   first_signaled - \c [in] self-explanatory
+ *
+ * \return   0 on success\n
+ *          -ETIME - Timeout
+ *          <0 - Negative POSIX Error code
+ *
+ */
+int gsgpu_cs_syncobj_wait(gsgpu_device_handle dev,
+			   uint32_t *handles, unsigned num_handles,
+			   int64_t timeout_nsec, unsigned flags,
+			   uint32_t *first_signaled);
+
+/**
+ *  Export kernel sync object to shareable fd.
+ *
+ * \param   dev	       - \c [in] device handle
+ * \param   syncobj    - \c [in] sync object handle
+ * \param   shared_fd  - \c [out] shared file descriptor.
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+*/
+int gsgpu_cs_export_syncobj(gsgpu_device_handle dev,
+			     uint32_t syncobj,
+			     int *shared_fd);
+/**
+ *  Import kernel sync object from shareable fd.
+ *
+ * \param   dev	       - \c [in] device handle
+ * \param   shared_fd  - \c [in] shared file descriptor.
+ * \param   syncobj    - \c [out] sync object handle
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+*/
+int gsgpu_cs_import_syncobj(gsgpu_device_handle dev,
+			     int shared_fd,
+			     uint32_t *syncobj);
+
+/**
+ *  Export kernel sync object to a sync_file.
+ *
+ * \param   dev	       - \c [in] device handle
+ * \param   syncobj    - \c [in] sync object handle
+ * \param   sync_file_fd - \c [out] sync_file file descriptor.
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+ */
+int gsgpu_cs_syncobj_export_sync_file(gsgpu_device_handle dev,
+				       uint32_t syncobj,
+				       int *sync_file_fd);
+
+/**
+ *  Import kernel sync object from a sync_file.
+ *
+ * \param   dev	       - \c [in] device handle
+ * \param   syncobj    - \c [in] sync object handle
+ * \param   sync_file_fd - \c [in] sync_file file descriptor.
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+ */
+int gsgpu_cs_syncobj_import_sync_file(gsgpu_device_handle dev,
+				       uint32_t syncobj,
+				       int sync_file_fd);
+
+/**
+ * Export an gsgpu fence as a handle (syncobj or fd).
+ *
+ * \param what		GSGPU_FENCE_TO_HANDLE_GET_{SYNCOBJ, FD}
+ * \param out_handle	returned handle
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ */
+int gsgpu_cs_fence_to_handle(gsgpu_device_handle dev,
+			      struct gsgpu_cs_fence *fence,
+			      uint32_t what,
+			      uint32_t *out_handle);
+
+/**
+ *  Submit raw command submission to kernel
+ *
+ * \param   dev	       - \c [in] device handle
+ * \param   context    - \c [in] context handle for context id
+ * \param   bo_list_handle - \c [in] request bo list handle (0 for none)
+ * \param   num_chunks - \c [in] number of CS chunks to submit
+ * \param   chunks     - \c [in] array of CS chunks
+ * \param   seq_no     - \c [out] output sequence number for submission.
+ *
+ * \return   0 on success\n
+ *          <0 - Negative POSIX Error code
+ *
+ */
+struct drm_gsgpu_cs_chunk;
+struct drm_gsgpu_cs_chunk_dep;
+struct drm_gsgpu_cs_chunk_data;
+
+int gsgpu_cs_submit_raw(gsgpu_device_handle dev,
+			 gsgpu_context_handle context,
+			 gsgpu_bo_list_handle bo_list_handle,
+			 int num_chunks,
+			 struct drm_gsgpu_cs_chunk *chunks,
+			 uint64_t *seq_no);
+
+void gsgpu_cs_chunk_fence_to_dep(struct gsgpu_cs_fence *fence,
+				  struct drm_gsgpu_cs_chunk_dep *dep);
+void gsgpu_cs_chunk_fence_info_to_data(struct gsgpu_cs_fence_info *fence_info,
+					struct drm_gsgpu_cs_chunk_data *data);
+
+/**
+ * Reserve VMID
+ * \param   context - \c [in]  GPU Context
+ * \param   flags - \c [in]  TBD
+ *
+ * \return  0 on success otherwise POSIX Error code
+*/
+int gsgpu_vm_reserve_vmid(gsgpu_device_handle dev, uint32_t flags);
+
+/**
+ * Free reserved VMID
+ * \param   context - \c [in]  GPU Context
+ * \param   flags - \c [in]  TBD
+ *
+ * \return  0 on success otherwise POSIX Error code
+*/
+int gsgpu_vm_unreserve_vmid(gsgpu_device_handle dev, uint32_t flags);
+
+/**
+ * Get avalible Hardware semaphore from device
+ *
+ * \ param handle device pionter
+ * \ param ctx_id cs contex
+ * \param sema get value
+ *
+ * \return: 0 on success otherwise POSIX Error Code
+ */
+int gsgpu_hw_sema_get(gsgpu_device_handle dev, gsgpu_context_handle ctx, uint64_t *sema);
+
+/**
+ * Put using Hardware semaphore to device
+ *
+ * \ param handle device pionter
+ * \ param ctx_id cs contex
+ * \param sema id value
+ *
+ * \return: 0 on success otherwise POSIX Error Code
+ */
+int gsgpu_hw_sema_put(gsgpu_device_handle dev, gsgpu_context_handle ctx, uint64_t sema);
+
+#ifdef __cplusplus
+}
+#endif
+#endif /* #ifdef _GSGPU_H_ */
diff --git a/gsgpu/gsgpu_asic_id.c b/gsgpu/gsgpu_asic_id.c
new file mode 100644
index 00000000..9c567fe6
--- /dev/null
+++ b/gsgpu/gsgpu_asic_id.c
@@ -0,0 +1,165 @@
+/*
+ * Copyright (C) 2020 Loongson Technology Co., Ltd.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#ifdef HAVE_CONFIG_H
+#include "config.h"
+#endif
+
+#include <ctype.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <stdint.h>
+#include <string.h>
+#include <unistd.h>
+#include <errno.h>
+
+#include "xf86drm.h"
+#include "gsgpu_drm.h"
+#include "gsgpu_internal.h"
+
+static int parse_one_line(struct gsgpu_device *dev, const char *line)
+{
+	char *buf, *saveptr;
+	char *s_did;
+	uint32_t did;
+	char *s_rid;
+	uint32_t rid;
+	char *s_name;
+	char *endptr;
+	int r = -EINVAL;
+
+	/* ignore empty line and commented line */
+	if (strlen(line) == 0 || line[0] == '#')
+		return -EAGAIN;
+
+	buf = strdup(line);
+	if (!buf)
+		return -ENOMEM;
+
+	/* device id */
+	s_did = strtok_r(buf, ",", &saveptr);
+	if (!s_did)
+		goto out;
+
+	did = strtol(s_did, &endptr, 16);
+	if (*endptr)
+		goto out;
+
+	if (did != dev->info.asic_id) {
+		r = -EAGAIN;
+		goto out;
+	}
+
+	/* revision id */
+	s_rid = strtok_r(NULL, ",", &saveptr);
+	if (!s_rid)
+		goto out;
+
+	rid = strtol(s_rid, &endptr, 16);
+	if (*endptr)
+		goto out;
+
+	if (rid != dev->info.pci_rev_id) {
+		r = -EAGAIN;
+		goto out;
+	}
+
+	/* marketing name */
+	s_name = strtok_r(NULL, ",", &saveptr);
+	if (!s_name)
+		goto out;
+
+	/* trim leading whitespaces or tabs */
+	while (isblank(*s_name))
+		s_name++;
+	if (strlen(s_name) == 0)
+		goto out;
+
+	dev->marketing_name = strdup(s_name);
+	if (dev->marketing_name)
+		r = 0;
+	else
+		r = -ENOMEM;
+
+out:
+	free(buf);
+
+	return r;
+}
+
+void gsgpu_parse_asic_ids(struct gsgpu_device *dev)
+{
+	FILE *fp;
+	char *line = NULL;
+	size_t len = 0;
+	ssize_t n;
+	int line_num = 1;
+	int r = 0;
+
+	fp = fopen(GSGPU_ASIC_ID_TABLE, "r");
+	if (!fp) {
+		fprintf(stderr, "%s: %s\n", GSGPU_ASIC_ID_TABLE,
+			strerror(errno));
+		return;
+	}
+
+	/* 1st valid line is file version */
+	while ((n = getline(&line, &len, fp)) != -1) {
+		/* trim trailing newline */
+		if (line[n - 1] == '\n')
+			line[n - 1] = '\0';
+
+		/* ignore empty line and commented line */
+		if (strlen(line) == 0 || line[0] == '#') {
+			line_num++;
+			continue;
+		}
+
+		drmMsg("%s version: %s\n", GSGPU_ASIC_ID_TABLE, line);
+		break;
+	}
+
+	while ((n = getline(&line, &len, fp)) != -1) {
+		/* trim trailing newline */
+		if (line[n - 1] == '\n')
+			line[n - 1] = '\0';
+
+		r = parse_one_line(dev, line);
+		if (r != -EAGAIN)
+			break;
+
+		line_num++;
+	}
+
+	if (r == -EINVAL) {
+		fprintf(stderr, "Invalid format: %s: line %d: %s\n",
+			GSGPU_ASIC_ID_TABLE, line_num, line);
+	} else if (r && r != -EAGAIN) {
+		fprintf(stderr, "%s: Cannot parse ASIC IDs: %s\n",
+			__func__, strerror(-r));
+	}
+
+	free(line);
+	fclose(fp);
+}
diff --git a/gsgpu/gsgpu_bo.c b/gsgpu/gsgpu_bo.c
new file mode 100644
index 00000000..6d9ab44a
--- /dev/null
+++ b/gsgpu/gsgpu_bo.c
@@ -0,0 +1,727 @@
+/*
+ * Copyright (C) 2020 Loongson Technology Co., Ltd.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#ifdef HAVE_CONFIG_H
+#include "config.h"
+#endif
+
+#include <stdlib.h>
+#include <stdio.h>
+#include <stdint.h>
+#include <string.h>
+#include <errno.h>
+#include <fcntl.h>
+#include <unistd.h>
+#include <sys/ioctl.h>
+#include <sys/mman.h>
+#include <sys/time.h>
+
+#include "libdrm_macros.h"
+#include "xf86drm.h"
+#include "gsgpu_drm.h"
+#include "gsgpu_internal.h"
+#include "util_hash_table.h"
+#include "util_math.h"
+
+#define BO_ALIGN_SIZE (16 * 1024)
+
+static void gsgpu_close_kms_handle(gsgpu_device_handle dev,
+				     uint32_t handle)
+{
+	struct drm_gem_close args = {};
+
+	args.handle = handle;
+	drmIoctl(dev->fd, DRM_IOCTL_GEM_CLOSE, &args);
+}
+
+int gsgpu_bo_alloc(gsgpu_device_handle dev,
+		    struct gsgpu_bo_alloc_request *alloc_buffer,
+		    gsgpu_bo_handle *buf_handle)
+{
+	struct gsgpu_bo *bo;
+	union drm_gsgpu_gem_create args;
+	unsigned heap = alloc_buffer->preferred_heap;
+	int r = 0;
+
+	/* It's an error if the heap is not specified */
+	if (!(heap & (GSGPU_GEM_DOMAIN_GTT | GSGPU_GEM_DOMAIN_VRAM)))
+		return -EINVAL;
+
+	bo = calloc(1, sizeof(struct gsgpu_bo));
+	if (!bo)
+		return -ENOMEM;
+
+	atomic_set(&bo->refcount, 1);
+	bo->dev = dev;
+	bo->alloc_size = alloc_buffer->alloc_size;
+
+	memset(&args, 0, sizeof(args));
+	args.in.bo_size = alloc_buffer->alloc_size;
+	args.in.alignment = alloc_buffer->phys_alignment;
+
+	/* Set the placement. */
+	args.in.domains = heap;
+	args.in.domain_flags = alloc_buffer->flags;
+
+	/* Allocate the buffer with the preferred heap. */
+	r = drmCommandWriteRead(dev->fd, DRM_GSGPU_GEM_CREATE,
+				&args, sizeof(args));
+	if (r) {
+		free(bo);
+		return r;
+	}
+
+	bo->handle = args.out.handle;
+
+	pthread_mutex_init(&bo->cpu_access_mutex, NULL);
+
+	*buf_handle = bo;
+	return 0;
+}
+
+int gsgpu_bo_set_metadata(gsgpu_bo_handle bo,
+			   struct gsgpu_bo_metadata *info)
+{
+	struct drm_gsgpu_gem_metadata args = {};
+
+	args.handle = bo->handle;
+	args.op = GSGPU_GEM_METADATA_OP_SET_METADATA;
+	args.data.flags = info->flags;
+	args.data.tiling_info = info->tiling_info;
+
+	if (info->size_metadata > sizeof(args.data.data))
+		return -EINVAL;
+
+	if (info->size_metadata) {
+		args.data.data_size_bytes = info->size_metadata;
+		memcpy(args.data.data, info->umd_metadata, info->size_metadata);
+	}
+
+	return drmCommandWriteRead(bo->dev->fd,
+				   DRM_GSGPU_GEM_METADATA,
+				   &args, sizeof(args));
+}
+
+int gsgpu_bo_query_info(gsgpu_bo_handle bo,
+			 struct gsgpu_bo_info *info)
+{
+	struct drm_gsgpu_gem_metadata metadata = {};
+	struct drm_gsgpu_gem_create_in bo_info = {};
+	struct drm_gsgpu_gem_op gem_op = {};
+	int r;
+
+	/* Validate the BO passed in */
+	if (!bo->handle)
+		return -EINVAL;
+
+	/* Query metadata. */
+	metadata.handle = bo->handle;
+	metadata.op = GSGPU_GEM_METADATA_OP_GET_METADATA;
+
+	r = drmCommandWriteRead(bo->dev->fd, DRM_GSGPU_GEM_METADATA,
+				&metadata, sizeof(metadata));
+	if (r)
+		return r;
+
+	if (metadata.data.data_size_bytes >
+	    sizeof(info->metadata.umd_metadata))
+		return -EINVAL;
+
+	/* Query buffer info. */
+	gem_op.handle = bo->handle;
+	gem_op.op = GSGPU_GEM_OP_GET_GEM_CREATE_INFO;
+	gem_op.value = (uintptr_t)&bo_info;
+
+	r = drmCommandWriteRead(bo->dev->fd, DRM_GSGPU_GEM_OP,
+				&gem_op, sizeof(gem_op));
+	if (r)
+		return r;
+
+	memset(info, 0, sizeof(*info));
+	info->alloc_size = bo_info.bo_size;
+	info->phys_alignment = bo_info.alignment;
+	info->preferred_heap = bo_info.domains;
+	info->alloc_flags = bo_info.domain_flags;
+	info->metadata.flags = metadata.data.flags;
+	info->metadata.tiling_info = metadata.data.tiling_info;
+
+	info->metadata.size_metadata = metadata.data.data_size_bytes;
+	if (metadata.data.data_size_bytes > 0)
+		memcpy(info->metadata.umd_metadata, metadata.data.data,
+		       metadata.data.data_size_bytes);
+
+	return 0;
+}
+
+static void gsgpu_add_handle_to_table(gsgpu_bo_handle bo)
+{
+	pthread_mutex_lock(&bo->dev->bo_table_mutex);
+	util_hash_table_set(bo->dev->bo_handles,
+			    (void*)(uintptr_t)bo->handle, bo);
+	pthread_mutex_unlock(&bo->dev->bo_table_mutex);
+}
+
+static int gsgpu_bo_export_flink(gsgpu_bo_handle bo)
+{
+	struct drm_gem_flink flink;
+	int fd, dma_fd;
+	uint32_t handle;
+	int r;
+
+	fd = bo->dev->fd;
+	handle = bo->handle;
+	if (bo->flink_name)
+		return 0;
+
+
+	if (bo->dev->flink_fd != bo->dev->fd) {
+		r = drmPrimeHandleToFD(bo->dev->fd, bo->handle, DRM_CLOEXEC,
+				       &dma_fd);
+		if (!r) {
+			r = drmPrimeFDToHandle(bo->dev->flink_fd, dma_fd, &handle);
+			close(dma_fd);
+		}
+		if (r)
+			return r;
+		fd = bo->dev->flink_fd;
+	}
+	memset(&flink, 0, sizeof(flink));
+	flink.handle = handle;
+
+	r = drmIoctl(fd, DRM_IOCTL_GEM_FLINK, &flink);
+	if (r)
+		return r;
+
+	bo->flink_name = flink.name;
+
+	if (bo->dev->flink_fd != bo->dev->fd) {
+		struct drm_gem_close args = {};
+		args.handle = handle;
+		drmIoctl(bo->dev->flink_fd, DRM_IOCTL_GEM_CLOSE, &args);
+	}
+
+	pthread_mutex_lock(&bo->dev->bo_table_mutex);
+	util_hash_table_set(bo->dev->bo_flink_names,
+			    (void*)(uintptr_t)bo->flink_name,
+			    bo);
+	pthread_mutex_unlock(&bo->dev->bo_table_mutex);
+
+	return 0;
+}
+
+int gsgpu_bo_export(gsgpu_bo_handle bo,
+		     enum gsgpu_bo_handle_type type,
+		     uint32_t *shared_handle)
+{
+	int r;
+
+	switch (type) {
+	case gsgpu_bo_handle_type_gem_flink_name:
+		r = gsgpu_bo_export_flink(bo);
+		if (r)
+			return r;
+
+		*shared_handle = bo->flink_name;
+		return 0;
+
+	case gsgpu_bo_handle_type_kms:
+		gsgpu_add_handle_to_table(bo);
+		*shared_handle = bo->handle;
+		return 0;
+
+	case gsgpu_bo_handle_type_dma_buf_fd:
+		gsgpu_add_handle_to_table(bo);
+		return drmPrimeHandleToFD(bo->dev->fd, bo->handle, DRM_CLOEXEC,
+				       (int*)shared_handle);
+	}
+	return -EINVAL;
+}
+
+int gsgpu_bo_import(gsgpu_device_handle dev,
+		     enum gsgpu_bo_handle_type type,
+		     uint32_t shared_handle,
+		     struct gsgpu_bo_import_result *output)
+{
+	struct drm_gem_open open_arg = {};
+	struct gsgpu_bo *bo = NULL;
+	int r;
+	int dma_fd;
+	uint64_t dma_buf_size = 0;
+
+	/* We must maintain a list of pairs <handle, bo>, so that we always
+	 * return the same gsgpu_bo instance for the same handle. */
+	pthread_mutex_lock(&dev->bo_table_mutex);
+
+	/* Convert a DMA buf handle to a KMS handle now. */
+	if (type == gsgpu_bo_handle_type_dma_buf_fd) {
+		uint32_t handle;
+		off_t size;
+
+		/* Get a KMS handle. */
+		r = drmPrimeFDToHandle(dev->fd, shared_handle, &handle);
+		if (r) {
+			pthread_mutex_unlock(&dev->bo_table_mutex);
+			return r;
+		}
+
+		/* Query the buffer size. */
+		size = lseek(shared_handle, 0, SEEK_END);
+		if (size == (off_t)-1) {
+			pthread_mutex_unlock(&dev->bo_table_mutex);
+			gsgpu_close_kms_handle(dev, handle);
+			return -errno;
+		}
+		lseek(shared_handle, 0, SEEK_SET);
+
+		dma_buf_size = size;
+		shared_handle = handle;
+	}
+
+	/* If we have already created a buffer with this handle, find it. */
+	switch (type) {
+	case gsgpu_bo_handle_type_gem_flink_name:
+		bo = util_hash_table_get(dev->bo_flink_names,
+					 (void*)(uintptr_t)shared_handle);
+		break;
+
+	case gsgpu_bo_handle_type_dma_buf_fd:
+		bo = util_hash_table_get(dev->bo_handles,
+					 (void*)(uintptr_t)shared_handle);
+		break;
+
+	case gsgpu_bo_handle_type_kms:
+		/* Importing a KMS handle in not allowed. */
+		pthread_mutex_unlock(&dev->bo_table_mutex);
+		return -EPERM;
+
+	default:
+		pthread_mutex_unlock(&dev->bo_table_mutex);
+		return -EINVAL;
+	}
+
+	if (bo) {
+		/* The buffer already exists, just bump the refcount. */
+		atomic_inc(&bo->refcount);
+		pthread_mutex_unlock(&dev->bo_table_mutex);
+
+		output->buf_handle = bo;
+		output->alloc_size = bo->alloc_size;
+		return 0;
+	}
+
+	bo = calloc(1, sizeof(struct gsgpu_bo));
+	if (!bo) {
+		pthread_mutex_unlock(&dev->bo_table_mutex);
+		if (type == gsgpu_bo_handle_type_dma_buf_fd) {
+			gsgpu_close_kms_handle(dev, shared_handle);
+		}
+		return -ENOMEM;
+	}
+
+	/* Open the handle. */
+	switch (type) {
+	case gsgpu_bo_handle_type_gem_flink_name:
+		open_arg.name = shared_handle;
+		r = drmIoctl(dev->flink_fd, DRM_IOCTL_GEM_OPEN, &open_arg);
+		if (r) {
+			free(bo);
+			pthread_mutex_unlock(&dev->bo_table_mutex);
+			return r;
+		}
+
+		bo->handle = open_arg.handle;
+		if (dev->flink_fd != dev->fd) {
+			r = drmPrimeHandleToFD(dev->flink_fd, bo->handle, DRM_CLOEXEC, &dma_fd);
+			if (r) {
+				free(bo);
+				pthread_mutex_unlock(&dev->bo_table_mutex);
+				return r;
+			}
+			r = drmPrimeFDToHandle(dev->fd, dma_fd, &bo->handle );
+
+			close(dma_fd);
+
+			if (r) {
+				free(bo);
+				pthread_mutex_unlock(&dev->bo_table_mutex);
+				return r;
+			}
+		}
+		bo->flink_name = shared_handle;
+		bo->alloc_size = open_arg.size;
+		util_hash_table_set(dev->bo_flink_names,
+				    (void*)(uintptr_t)bo->flink_name, bo);
+		break;
+
+	case gsgpu_bo_handle_type_dma_buf_fd:
+		bo->handle = shared_handle;
+		bo->alloc_size = dma_buf_size;
+		break;
+
+	case gsgpu_bo_handle_type_kms:
+		assert(0); /* unreachable */
+	}
+
+	/* Initialize it. */
+	atomic_set(&bo->refcount, 1);
+	bo->dev = dev;
+	pthread_mutex_init(&bo->cpu_access_mutex, NULL);
+
+	util_hash_table_set(dev->bo_handles, (void*)(uintptr_t)bo->handle, bo);
+	pthread_mutex_unlock(&dev->bo_table_mutex);
+
+	output->buf_handle = bo;
+	output->alloc_size = bo->alloc_size;
+	return 0;
+}
+
+int gsgpu_bo_free(gsgpu_bo_handle buf_handle)
+{
+	struct gsgpu_device *dev;
+	struct gsgpu_bo *bo = buf_handle;
+
+	assert(bo != NULL);
+	dev = bo->dev;
+	pthread_mutex_lock(&dev->bo_table_mutex);
+
+	if (update_references(&bo->refcount, NULL)) {
+		/* Remove the buffer from the hash tables. */
+		util_hash_table_remove(dev->bo_handles,
+					(void*)(uintptr_t)bo->handle);
+
+		if (bo->flink_name) {
+			util_hash_table_remove(dev->bo_flink_names,
+						(void*)(uintptr_t)bo->flink_name);
+		}
+
+		/* Release CPU access. */
+		if (bo->cpu_map_count > 0) {
+			bo->cpu_map_count = 1;
+			gsgpu_bo_cpu_unmap(bo);
+		}
+
+		gsgpu_close_kms_handle(dev, bo->handle);
+		pthread_mutex_destroy(&bo->cpu_access_mutex);
+		free(bo);
+	}
+
+	pthread_mutex_unlock(&dev->bo_table_mutex);
+	return 0;
+}
+
+int gsgpu_bo_cpu_map(gsgpu_bo_handle bo, void **cpu)
+{
+	union drm_gsgpu_gem_mmap args;
+	void *ptr;
+	int r;
+
+	pthread_mutex_lock(&bo->cpu_access_mutex);
+
+	if (bo->cpu_ptr) {
+		/* already mapped */
+		assert(bo->cpu_map_count > 0);
+		bo->cpu_map_count++;
+		*cpu = bo->cpu_ptr;
+		pthread_mutex_unlock(&bo->cpu_access_mutex);
+		return 0;
+	}
+
+	assert(bo->cpu_map_count == 0);
+
+	memset(&args, 0, sizeof(args));
+
+	/* Query the buffer address (args.addr_ptr).
+	 * The kernel driver ignores the offset and size parameters. */
+	args.in.handle = bo->handle;
+
+	r = drmCommandWriteRead(bo->dev->fd, DRM_GSGPU_GEM_MMAP, &args,
+				sizeof(args));
+	if (r) {
+		pthread_mutex_unlock(&bo->cpu_access_mutex);
+		return r;
+	}
+
+	/* Map the buffer. */
+	ptr = drm_mmap(NULL, bo->alloc_size, PROT_READ | PROT_WRITE, MAP_SHARED,
+		       bo->dev->fd, args.out.addr_ptr);
+	if (ptr == MAP_FAILED) {
+		pthread_mutex_unlock(&bo->cpu_access_mutex);
+		return -errno;
+	}
+
+	bo->cpu_ptr = ptr;
+	bo->cpu_map_count = 1;
+	pthread_mutex_unlock(&bo->cpu_access_mutex);
+
+	*cpu = ptr;
+	return 0;
+}
+
+int gsgpu_bo_cpu_unmap(gsgpu_bo_handle bo)
+{
+	int r;
+
+	pthread_mutex_lock(&bo->cpu_access_mutex);
+	assert(bo->cpu_map_count >= 0);
+
+	if (bo->cpu_map_count == 0) {
+		/* not mapped */
+		pthread_mutex_unlock(&bo->cpu_access_mutex);
+		return -EINVAL;
+	}
+
+	bo->cpu_map_count--;
+	if (bo->cpu_map_count > 0) {
+		/* mapped multiple times */
+		pthread_mutex_unlock(&bo->cpu_access_mutex);
+		return 0;
+	}
+
+	r = drm_munmap(bo->cpu_ptr, bo->alloc_size) == 0 ? 0 : -errno;
+	bo->cpu_ptr = NULL;
+	pthread_mutex_unlock(&bo->cpu_access_mutex);
+	return r;
+}
+
+int gsgpu_query_buffer_size_alignment(gsgpu_device_handle dev,
+				struct gsgpu_buffer_size_alignments *info)
+{
+	info->size_local = dev->dev_info.pte_fragment_size;
+	info->size_remote = dev->dev_info.gart_page_size;
+	return 0;
+}
+
+int gsgpu_bo_wait_for_idle(gsgpu_bo_handle bo,
+			    uint64_t timeout_ns,
+			    bool *busy)
+{
+	union drm_gsgpu_gem_wait_idle args;
+	int r;
+
+	memset(&args, 0, sizeof(args));
+	args.in.handle = bo->handle;
+	args.in.timeout = gsgpu_cs_calculate_timeout(timeout_ns);
+
+	r = drmCommandWriteRead(bo->dev->fd, DRM_GSGPU_GEM_WAIT_IDLE,
+				&args, sizeof(args));
+
+	if (r == 0) {
+		*busy = args.out.status;
+		return 0;
+	} else {
+		fprintf(stderr, "gsgpu: GEM_WAIT_IDLE failed with %i\n", r);
+		return r;
+	}
+}
+
+int gsgpu_create_bo_from_user_mem(gsgpu_device_handle dev,
+				    void *cpu,
+				    uint64_t size,
+				    gsgpu_bo_handle *buf_handle)
+{
+	int r;
+	struct gsgpu_bo *bo;
+	struct drm_gsgpu_gem_userptr args;
+
+	args.addr = (uintptr_t)cpu;
+	args.flags = GSGPU_GEM_USERPTR_ANONONLY | GSGPU_GEM_USERPTR_REGISTER |
+		GSGPU_GEM_USERPTR_VALIDATE;
+	args.size = size;
+	r = drmCommandWriteRead(dev->fd, DRM_GSGPU_GEM_USERPTR,
+				&args, sizeof(args));
+	if (r)
+		return r;
+
+	bo = calloc(1, sizeof(struct gsgpu_bo));
+	if (!bo)
+		return -ENOMEM;
+
+	atomic_set(&bo->refcount, 1);
+	bo->dev = dev;
+	bo->alloc_size = size;
+	bo->handle = args.handle;
+
+	*buf_handle = bo;
+
+	return r;
+}
+
+int gsgpu_bo_list_create(gsgpu_device_handle dev,
+			  uint32_t number_of_resources,
+			  gsgpu_bo_handle *resources,
+			  uint8_t *resource_prios,
+			  gsgpu_bo_list_handle *result)
+{
+	struct drm_gsgpu_bo_list_entry *list;
+	union drm_gsgpu_bo_list args;
+	unsigned i;
+	int r;
+
+	if (!number_of_resources)
+		return -EINVAL;
+
+	/* overflow check for multiplication */
+	if (number_of_resources > UINT32_MAX / sizeof(struct drm_gsgpu_bo_list_entry))
+		return -EINVAL;
+
+	list = malloc(number_of_resources * sizeof(struct drm_gsgpu_bo_list_entry));
+	if (!list)
+		return -ENOMEM;
+
+	*result = malloc(sizeof(struct gsgpu_bo_list));
+	if (!*result) {
+		free(list);
+		return -ENOMEM;
+	}
+
+	memset(&args, 0, sizeof(args));
+	args.in.operation = GSGPU_BO_LIST_OP_CREATE;
+	args.in.bo_number = number_of_resources;
+	args.in.bo_info_size = sizeof(struct drm_gsgpu_bo_list_entry);
+	args.in.bo_info_ptr = (uint64_t)(uintptr_t)list;
+
+	for (i = 0; i < number_of_resources; i++) {
+		list[i].bo_handle = resources[i]->handle;
+		if (resource_prios)
+			list[i].bo_priority = resource_prios[i];
+		else
+			list[i].bo_priority = 0;
+	}
+
+	r = drmCommandWriteRead(dev->fd, DRM_GSGPU_BO_LIST,
+				&args, sizeof(args));
+	free(list);
+	if (r) {
+		free(*result);
+		return r;
+	}
+
+	(*result)->dev = dev;
+	(*result)->handle = args.out.list_handle;
+	return 0;
+}
+
+int gsgpu_bo_list_destroy(gsgpu_bo_list_handle list)
+{
+	union drm_gsgpu_bo_list args;
+	int r;
+
+	memset(&args, 0, sizeof(args));
+	args.in.operation = GSGPU_BO_LIST_OP_DESTROY;
+	args.in.list_handle = list->handle;
+
+	r = drmCommandWriteRead(list->dev->fd, DRM_GSGPU_BO_LIST,
+				&args, sizeof(args));
+
+	if (!r)
+		free(list);
+
+	return r;
+}
+
+int gsgpu_bo_list_update(gsgpu_bo_list_handle handle,
+			  uint32_t number_of_resources,
+			  gsgpu_bo_handle *resources,
+			  uint8_t *resource_prios)
+{
+	struct drm_gsgpu_bo_list_entry *list;
+	union drm_gsgpu_bo_list args;
+	unsigned i;
+	int r;
+
+	if (!number_of_resources)
+		return -EINVAL;
+
+	/* overflow check for multiplication */
+	if (number_of_resources > UINT32_MAX / sizeof(struct drm_gsgpu_bo_list_entry))
+		return -EINVAL;
+
+	list = malloc(number_of_resources * sizeof(struct drm_gsgpu_bo_list_entry));
+	if (!list)
+		return -ENOMEM;
+
+	args.in.operation = GSGPU_BO_LIST_OP_UPDATE;
+	args.in.list_handle = handle->handle;
+	args.in.bo_number = number_of_resources;
+	args.in.bo_info_size = sizeof(struct drm_gsgpu_bo_list_entry);
+	args.in.bo_info_ptr = (uintptr_t)list;
+
+	for (i = 0; i < number_of_resources; i++) {
+		list[i].bo_handle = resources[i]->handle;
+		if (resource_prios)
+			list[i].bo_priority = resource_prios[i];
+		else
+			list[i].bo_priority = 0;
+	}
+
+	r = drmCommandWriteRead(handle->dev->fd, DRM_GSGPU_BO_LIST,
+				&args, sizeof(args));
+	free(list);
+	return r;
+}
+
+int gsgpu_bo_va_op(gsgpu_bo_handle bo,
+		     uint64_t offset,
+		     uint64_t size,
+		     uint64_t addr,
+		     uint64_t flags,
+		     uint32_t ops)
+{
+	gsgpu_device_handle dev = bo->dev;
+
+	size = ALIGN(size, BO_ALIGN_SIZE);
+
+	return gsgpu_bo_va_op_raw(dev, bo, offset, size, addr,
+				   GSGPU_VM_PAGE_READABLE |
+				   GSGPU_VM_PAGE_WRITEABLE |
+				   GSGPU_VM_PAGE_EXECUTABLE, ops);
+}
+
+int gsgpu_bo_va_op_raw(gsgpu_device_handle dev,
+			gsgpu_bo_handle bo,
+			uint64_t offset,
+			uint64_t size,
+			uint64_t addr,
+			uint64_t flags,
+			uint32_t ops)
+{
+	struct drm_gsgpu_gem_va va;
+	int r;
+
+	if (ops != GSGPU_VA_OP_MAP && ops != GSGPU_VA_OP_UNMAP &&
+	    ops != GSGPU_VA_OP_REPLACE && ops != GSGPU_VA_OP_CLEAR)
+		return -EINVAL;
+
+	memset(&va, 0, sizeof(va));
+	va.handle = bo ? bo->handle : 0;
+	va.operation = ops;
+	va.flags = flags;
+	va.va_address = addr;
+	va.offset_in_bo = offset;
+	va.map_size = size;
+
+	r = drmCommandWriteRead(dev->fd, DRM_GSGPU_GEM_VA, &va, sizeof(va));
+
+	return r;
+}
diff --git a/gsgpu/gsgpu_cs.c b/gsgpu/gsgpu_cs.c
new file mode 100644
index 00000000..0bd39213
--- /dev/null
+++ b/gsgpu/gsgpu_cs.c
@@ -0,0 +1,775 @@
+/*
+ * Copyright (C) 2020 Loongson Technology Co., Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#ifdef HAVE_CONFIG_H
+#include "config.h"
+#endif
+
+#include <stdlib.h>
+#include <stdio.h>
+#include <string.h>
+#include <errno.h>
+#include <pthread.h>
+#include <sched.h>
+#include <sys/ioctl.h>
+#ifdef HAVE_ALLOCA_H
+# include <alloca.h>
+#endif
+
+#include "xf86drm.h"
+#include "gsgpu_drm.h"
+#include "gsgpu_internal.h"
+
+static int gsgpu_cs_unreference_sem(gsgpu_semaphore_handle sem);
+static int gsgpu_cs_reset_sem(gsgpu_semaphore_handle sem);
+
+/**
+ * Create command submission context
+ *
+ * \param   dev      - \c [in] Device handle. See #gsgpu_device_initialize()
+ * \param   priority - \c [in] Context creation flags. See GSGPU_CTX_PRIORITY_*
+ * \param   context  - \c [out] GPU Context handle
+ *
+ * \return  0 on success otherwise POSIX Error code
+*/
+int gsgpu_cs_ctx_create2(gsgpu_device_handle dev, uint32_t priority,
+							gsgpu_context_handle *context)
+{
+	struct gsgpu_context *gpu_context;
+	union drm_gsgpu_ctx args;
+	int i, j, k;
+	int r;
+
+	if (!dev || !context)
+		return -EINVAL;
+
+	gpu_context = calloc(1, sizeof(struct gsgpu_context));
+	if (!gpu_context)
+		return -ENOMEM;
+
+	gpu_context->dev = dev;
+
+	r = pthread_mutex_init(&gpu_context->sequence_mutex, NULL);
+	if (r)
+		goto error;
+
+	/* Create the context */
+	memset(&args, 0, sizeof(args));
+	args.in.op = GSGPU_CTX_OP_ALLOC_CTX;
+	args.in.priority = priority;
+
+	r = drmCommandWriteRead(dev->fd, DRM_GSGPU_CTX, &args, sizeof(args));
+	if (r)
+		goto error;
+
+	gpu_context->id = args.out.alloc.ctx_id;
+	for (i = 0; i < GSGPU_HW_IP_NUM; i++)
+		for (j = 0; j < GSGPU_HW_IP_INSTANCE_MAX_COUNT; j++)
+			for (k = 0; k < GSGPU_CS_MAX_RINGS; k++)
+				list_inithead(&gpu_context->sem_list[i][j][k]);
+	*context = (gsgpu_context_handle)gpu_context;
+
+	return 0;
+
+error:
+	pthread_mutex_destroy(&gpu_context->sequence_mutex);
+	free(gpu_context);
+	return r;
+}
+
+int gsgpu_cs_ctx_create(gsgpu_device_handle dev,
+			 gsgpu_context_handle *context)
+{
+	return gsgpu_cs_ctx_create2(dev, GSGPU_CTX_PRIORITY_NORMAL, context);
+}
+
+/**
+ * Release command submission context
+ *
+ * \param   dev - \c [in] gsgpu device handle
+ * \param   context - \c [in] gsgpu context handle
+ *
+ * \return  0 on success otherwise POSIX Error code
+*/
+int gsgpu_cs_ctx_free(gsgpu_context_handle context)
+{
+	union drm_gsgpu_ctx args;
+	int i, j, k;
+	int r;
+
+	if (!context)
+		return -EINVAL;
+
+	pthread_mutex_destroy(&context->sequence_mutex);
+
+	/* now deal with kernel side */
+	memset(&args, 0, sizeof(args));
+	args.in.op = GSGPU_CTX_OP_FREE_CTX;
+	args.in.ctx_id = context->id;
+	r = drmCommandWriteRead(context->dev->fd, DRM_GSGPU_CTX,
+				&args, sizeof(args));
+	for (i = 0; i < GSGPU_HW_IP_NUM; i++) {
+		for (j = 0; j < GSGPU_HW_IP_INSTANCE_MAX_COUNT; j++) {
+			for (k = 0; k < GSGPU_CS_MAX_RINGS; k++) {
+				gsgpu_semaphore_handle sem;
+				LIST_FOR_EACH_ENTRY(sem, &context->sem_list[i][j][k], list) {
+					list_del(&sem->list);
+					gsgpu_cs_reset_sem(sem);
+					gsgpu_cs_unreference_sem(sem);
+				}
+			}
+		}
+	}
+	free(context);
+
+	return r;
+}
+
+int gsgpu_cs_query_reset_state(gsgpu_context_handle context,
+				uint32_t *state, uint32_t *hangs)
+{
+	union drm_gsgpu_ctx args;
+	int r;
+
+	if (!context)
+		return -EINVAL;
+
+	memset(&args, 0, sizeof(args));
+	args.in.op = GSGPU_CTX_OP_QUERY_STATE;
+	args.in.ctx_id = context->id;
+	r = drmCommandWriteRead(context->dev->fd, DRM_GSGPU_CTX,
+				&args, sizeof(args));
+	if (!r) {
+		*state = args.out.state.reset_status;
+		*hangs = args.out.state.hangs;
+	}
+	return r;
+}
+
+/**
+ * Submit command to kernel DRM
+ * \param   dev - \c [in]  Device handle
+ * \param   context - \c [in]  GPU Context
+ * \param   ibs_request - \c [in]  Pointer to submission requests
+ * \param   fence - \c [out] return fence for this submission
+ *
+ * \return  0 on success otherwise POSIX Error code
+ * \sa gsgpu_cs_submit()
+*/
+static int gsgpu_cs_submit_one(gsgpu_context_handle context,
+				struct gsgpu_cs_request *ibs_request)
+{
+	union drm_gsgpu_cs cs;
+	uint64_t *chunk_array;
+	struct drm_gsgpu_cs_chunk *chunks;
+	struct drm_gsgpu_cs_chunk_data *chunk_data;
+	struct drm_gsgpu_cs_chunk_dep *dependencies = NULL;
+	struct drm_gsgpu_cs_chunk_dep *sem_dependencies = NULL;
+	struct list_head *sem_list;
+	gsgpu_semaphore_handle sem, tmp;
+	uint32_t i, size, sem_count = 0;
+	bool user_fence;
+	int r = 0;
+
+	if (ibs_request->ip_type >= GSGPU_HW_IP_NUM)
+		return -EINVAL;
+	if (ibs_request->ring >= GSGPU_CS_MAX_RINGS)
+		return -EINVAL;
+	if (ibs_request->number_of_ibs > GSGPU_CS_MAX_IBS_PER_SUBMIT)
+		return -EINVAL;
+	if (ibs_request->number_of_ibs == 0) {
+		ibs_request->seq_no = GSGPU_NULL_SUBMIT_SEQ;
+		return 0;
+	}
+	user_fence = (ibs_request->fence_info.handle != NULL);
+
+	size = ibs_request->number_of_ibs + (user_fence ? 2 : 1) + 1;
+
+	chunk_array = alloca(sizeof(uint64_t) * size);
+	chunks = alloca(sizeof(struct drm_gsgpu_cs_chunk) * size);
+
+	size = ibs_request->number_of_ibs + (user_fence ? 1 : 0);
+
+	chunk_data = alloca(sizeof(struct drm_gsgpu_cs_chunk_data) * size);
+
+	memset(&cs, 0, sizeof(cs));
+	cs.in.chunks = (uint64_t)(uintptr_t)chunk_array;
+	cs.in.ctx_id = context->id;
+	if (ibs_request->resources)
+		cs.in.bo_list_handle = ibs_request->resources->handle;
+	cs.in.num_chunks = ibs_request->number_of_ibs;
+	/* IB chunks */
+	for (i = 0; i < ibs_request->number_of_ibs; i++) {
+		struct gsgpu_cs_ib_info *ib;
+		chunk_array[i] = (uint64_t)(uintptr_t)&chunks[i];
+		chunks[i].chunk_id = GSGPU_CHUNK_ID_IB;
+		chunks[i].length_dw = sizeof(struct drm_gsgpu_cs_chunk_ib) / 4;
+		chunks[i].chunk_data = (uint64_t)(uintptr_t)&chunk_data[i];
+
+		ib = &ibs_request->ibs[i];
+
+		chunk_data[i].ib_data._pad = 0;
+		chunk_data[i].ib_data.va_start = ib->ib_mc_address;
+		chunk_data[i].ib_data.ib_bytes = ib->size * 4;
+		chunk_data[i].ib_data.ip_type = ibs_request->ip_type;
+		chunk_data[i].ib_data.ip_instance = ibs_request->ip_instance;
+		chunk_data[i].ib_data.ring = ibs_request->ring;
+		chunk_data[i].ib_data.flags = ib->flags;
+	}
+
+	pthread_mutex_lock(&context->sequence_mutex);
+
+	if (user_fence) {
+		i = cs.in.num_chunks++;
+
+		/* fence chunk */
+		chunk_array[i] = (uint64_t)(uintptr_t)&chunks[i];
+		chunks[i].chunk_id = GSGPU_CHUNK_ID_FENCE;
+		chunks[i].length_dw = sizeof(struct drm_gsgpu_cs_chunk_fence) / 4;
+		chunks[i].chunk_data = (uint64_t)(uintptr_t)&chunk_data[i];
+
+		/* fence bo handle */
+		chunk_data[i].fence_data.handle = ibs_request->fence_info.handle->handle;
+		/* offset */
+		chunk_data[i].fence_data.offset =
+			ibs_request->fence_info.offset * sizeof(uint64_t);
+	}
+
+	if (ibs_request->number_of_dependencies) {
+		dependencies = malloc(sizeof(struct drm_gsgpu_cs_chunk_dep) *
+			ibs_request->number_of_dependencies);
+		if (!dependencies) {
+			r = -ENOMEM;
+			goto error_unlock;
+		}
+
+		for (i = 0; i < ibs_request->number_of_dependencies; ++i) {
+			struct gsgpu_cs_fence *info = &ibs_request->dependencies[i];
+			struct drm_gsgpu_cs_chunk_dep *dep = &dependencies[i];
+			dep->ip_type = info->ip_type;
+			dep->ip_instance = info->ip_instance;
+			dep->ring = info->ring;
+			dep->ctx_id = info->context->id;
+			dep->handle = info->fence;
+		}
+
+		i = cs.in.num_chunks++;
+
+		/* dependencies chunk */
+		chunk_array[i] = (uint64_t)(uintptr_t)&chunks[i];
+		chunks[i].chunk_id = GSGPU_CHUNK_ID_DEPENDENCIES;
+		chunks[i].length_dw = sizeof(struct drm_gsgpu_cs_chunk_dep) / 4
+			* ibs_request->number_of_dependencies;
+		chunks[i].chunk_data = (uint64_t)(uintptr_t)dependencies;
+	}
+
+	sem_list = &context->sem_list[ibs_request->ip_type][ibs_request->ip_instance][ibs_request->ring];
+	LIST_FOR_EACH_ENTRY(sem, sem_list, list)
+		sem_count++;
+	if (sem_count) {
+		sem_dependencies = malloc(sizeof(struct drm_gsgpu_cs_chunk_dep) * sem_count);
+		if (!sem_dependencies) {
+			r = -ENOMEM;
+			goto error_unlock;
+		}
+		sem_count = 0;
+		LIST_FOR_EACH_ENTRY_SAFE(sem, tmp, sem_list, list) {
+			struct gsgpu_cs_fence *info = &sem->signal_fence;
+			struct drm_gsgpu_cs_chunk_dep *dep = &sem_dependencies[sem_count++];
+			dep->ip_type = info->ip_type;
+			dep->ip_instance = info->ip_instance;
+			dep->ring = info->ring;
+			dep->ctx_id = info->context->id;
+			dep->handle = info->fence;
+
+			list_del(&sem->list);
+			gsgpu_cs_reset_sem(sem);
+			gsgpu_cs_unreference_sem(sem);
+		}
+		i = cs.in.num_chunks++;
+
+		/* dependencies chunk */
+		chunk_array[i] = (uint64_t)(uintptr_t)&chunks[i];
+		chunks[i].chunk_id = GSGPU_CHUNK_ID_DEPENDENCIES;
+		chunks[i].length_dw = sizeof(struct drm_gsgpu_cs_chunk_dep) / 4 * sem_count;
+		chunks[i].chunk_data = (uint64_t)(uintptr_t)sem_dependencies;
+	}
+
+	r = drmCommandWriteRead(context->dev->fd, DRM_GSGPU_CS,
+				&cs, sizeof(cs));
+	if (r)
+		goto error_unlock;
+
+	ibs_request->seq_no = cs.out.handle;
+	context->last_seq[ibs_request->ip_type][ibs_request->ip_instance][ibs_request->ring] = ibs_request->seq_no;
+error_unlock:
+	pthread_mutex_unlock(&context->sequence_mutex);
+	free(dependencies);
+	free(sem_dependencies);
+	return r;
+}
+
+int gsgpu_cs_submit(gsgpu_context_handle context,
+		     uint64_t flags,
+		     struct gsgpu_cs_request *ibs_request,
+		     uint32_t number_of_requests)
+{
+	uint32_t i;
+	int r;
+
+	if (!context || !ibs_request)
+		return -EINVAL;
+
+	r = 0;
+	for (i = 0; i < number_of_requests; i++) {
+		r = gsgpu_cs_submit_one(context, ibs_request);
+		if (r)
+			break;
+		ibs_request++;
+	}
+
+	return r;
+}
+
+/**
+ * Calculate absolute timeout.
+ *
+ * \param   timeout - \c [in] timeout in nanoseconds.
+ *
+ * \return  absolute timeout in nanoseconds
+*/
+drm_private uint64_t gsgpu_cs_calculate_timeout(uint64_t timeout)
+{
+	int r;
+
+	if (timeout != GSGPU_TIMEOUT_INFINITE) {
+		struct timespec current;
+		uint64_t current_ns;
+		r = clock_gettime(CLOCK_MONOTONIC, &current);
+		if (r) {
+			fprintf(stderr, "clock_gettime() returned error (%d)!", errno);
+			return GSGPU_TIMEOUT_INFINITE;
+		}
+
+		current_ns = ((uint64_t)current.tv_sec) * 1000000000ull;
+		current_ns += current.tv_nsec;
+		timeout += current_ns;
+		if (timeout < current_ns)
+			timeout = GSGPU_TIMEOUT_INFINITE;
+	}
+	return timeout;
+}
+
+static int gsgpu_ioctl_wait_cs(gsgpu_context_handle context,
+				unsigned ip,
+				unsigned ip_instance,
+				uint32_t ring,
+				uint64_t handle,
+				uint64_t timeout_ns,
+				uint64_t flags,
+				bool *busy)
+{
+	gsgpu_device_handle dev = context->dev;
+	union drm_gsgpu_wait_cs args;
+	int r;
+
+	memset(&args, 0, sizeof(args));
+	args.in.handle = handle;
+	args.in.ip_type = ip;
+	args.in.ip_instance = ip_instance;
+	args.in.ring = ring;
+	args.in.ctx_id = context->id;
+
+	if (flags & GSGPU_QUERY_FENCE_TIMEOUT_IS_ABSOLUTE)
+		args.in.timeout = timeout_ns;
+	else
+		args.in.timeout = gsgpu_cs_calculate_timeout(timeout_ns);
+
+	r = drmIoctl(dev->fd, DRM_IOCTL_GSGPU_WAIT_CS, &args);
+	if (r)
+		return -errno;
+
+	*busy = args.out.status;
+	return 0;
+}
+
+int gsgpu_cs_query_fence_status(struct gsgpu_cs_fence *fence,
+				 uint64_t timeout_ns,
+				 uint64_t flags,
+				 uint32_t *expired)
+{
+	bool busy = true;
+	int r;
+
+	if (!fence || !expired || !fence->context)
+		return -EINVAL;
+	if (fence->ip_type >= GSGPU_HW_IP_NUM)
+		return -EINVAL;
+	if (fence->ring >= GSGPU_CS_MAX_RINGS)
+		return -EINVAL;
+	if (fence->fence == GSGPU_NULL_SUBMIT_SEQ) {
+		*expired = true;
+		return 0;
+	}
+
+	*expired = false;
+
+	r = gsgpu_ioctl_wait_cs(fence->context,
+				fence->ip_type,fence->ip_instance,
+				fence->ring,fence->fence,
+				timeout_ns, flags, &busy);
+
+	if (!r && !busy)
+		*expired = true;
+
+	return r;
+}
+
+static int gsgpu_ioctl_wait_fences(struct gsgpu_cs_fence *fences,
+				    uint32_t fence_count,
+				    bool wait_all,
+				    uint64_t timeout_ns,
+				    uint32_t *status,
+				    uint32_t *first)
+{
+	struct drm_gsgpu_fence *drm_fences;
+	gsgpu_device_handle dev = fences[0].context->dev;
+	union drm_gsgpu_wait_fences args;
+	int r;
+	uint32_t i;
+
+	drm_fences = alloca(sizeof(struct drm_gsgpu_fence) * fence_count);
+	for (i = 0; i < fence_count; i++) {
+		drm_fences[i].ctx_id = fences[i].context->id;
+		drm_fences[i].ip_type = fences[i].ip_type;
+		drm_fences[i].ip_instance = fences[i].ip_instance;
+		drm_fences[i].ring = fences[i].ring;
+		drm_fences[i].seq_no = fences[i].fence;
+	}
+
+	memset(&args, 0, sizeof(args));
+	args.in.fences = (uint64_t)(uintptr_t)drm_fences;
+	args.in.fence_count = fence_count;
+	args.in.wait_all = wait_all;
+	args.in.timeout_ns = gsgpu_cs_calculate_timeout(timeout_ns);
+
+	r = drmIoctl(dev->fd, DRM_IOCTL_GSGPU_WAIT_FENCES, &args);
+	if (r)
+		return -errno;
+
+	*status = args.out.status;
+
+	if (first)
+		*first = args.out.first_signaled;
+
+	return 0;
+}
+
+int gsgpu_cs_wait_fences(struct gsgpu_cs_fence *fences,
+			  uint32_t fence_count,
+			  bool wait_all,
+			  uint64_t timeout_ns,
+			  uint32_t *status,
+			  uint32_t *first)
+{
+	uint32_t i;
+
+	/* Sanity check */
+	if (!fences || !status || !fence_count)
+		return -EINVAL;
+
+	for (i = 0; i < fence_count; i++) {
+		if (NULL == fences[i].context)
+			return -EINVAL;
+		if (fences[i].ip_type >= GSGPU_HW_IP_NUM)
+			return -EINVAL;
+		if (fences[i].ring >= GSGPU_CS_MAX_RINGS)
+			return -EINVAL;
+	}
+
+	*status = 0;
+
+	return gsgpu_ioctl_wait_fences(fences, fence_count, wait_all,
+					timeout_ns, status, first);
+}
+
+int gsgpu_cs_create_semaphore(gsgpu_semaphore_handle *sem)
+{
+	struct gsgpu_semaphore *gpu_semaphore;
+
+	if (!sem)
+		return -EINVAL;
+
+	gpu_semaphore = calloc(1, sizeof(struct gsgpu_semaphore));
+	if (!gpu_semaphore)
+		return -ENOMEM;
+
+	atomic_set(&gpu_semaphore->refcount, 1);
+	*sem = gpu_semaphore;
+
+	return 0;
+}
+
+int gsgpu_cs_signal_semaphore(gsgpu_context_handle ctx,
+			       uint32_t ip_type,
+			       uint32_t ip_instance,
+			       uint32_t ring,
+			       gsgpu_semaphore_handle sem)
+{
+	if (!ctx || !sem)
+		return -EINVAL;
+	if (ip_type >= GSGPU_HW_IP_NUM)
+		return -EINVAL;
+	if (ring >= GSGPU_CS_MAX_RINGS)
+		return -EINVAL;
+	/* sem has been signaled */
+	if (sem->signal_fence.context)
+		return -EINVAL;
+	pthread_mutex_lock(&ctx->sequence_mutex);
+	sem->signal_fence.context = ctx;
+	sem->signal_fence.ip_type = ip_type;
+	sem->signal_fence.ip_instance = ip_instance;
+	sem->signal_fence.ring = ring;
+	sem->signal_fence.fence = ctx->last_seq[ip_type][ip_instance][ring];
+	update_references(NULL, &sem->refcount);
+	pthread_mutex_unlock(&ctx->sequence_mutex);
+	return 0;
+}
+
+int gsgpu_cs_wait_semaphore(gsgpu_context_handle ctx,
+			     uint32_t ip_type,
+			     uint32_t ip_instance,
+			     uint32_t ring,
+			     gsgpu_semaphore_handle sem)
+{
+	if (!ctx || !sem)
+		return -EINVAL;
+	if (ip_type >= GSGPU_HW_IP_NUM)
+		return -EINVAL;
+	if (ring >= GSGPU_CS_MAX_RINGS)
+		return -EINVAL;
+	/* must signal first */
+	if (!sem->signal_fence.context)
+		return -EINVAL;
+
+	pthread_mutex_lock(&ctx->sequence_mutex);
+	list_add(&sem->list, &ctx->sem_list[ip_type][ip_instance][ring]);
+	pthread_mutex_unlock(&ctx->sequence_mutex);
+	return 0;
+}
+
+static int gsgpu_cs_reset_sem(gsgpu_semaphore_handle sem)
+{
+	if (!sem || !sem->signal_fence.context)
+		return -EINVAL;
+
+	sem->signal_fence.context = NULL;
+	sem->signal_fence.ip_type = 0;
+	sem->signal_fence.ip_instance = 0;
+	sem->signal_fence.ring = 0;
+	sem->signal_fence.fence = 0;
+
+	return 0;
+}
+
+static int gsgpu_cs_unreference_sem(gsgpu_semaphore_handle sem)
+{
+	if (!sem)
+		return -EINVAL;
+
+	if (update_references(&sem->refcount, NULL))
+		free(sem);
+	return 0;
+}
+
+int gsgpu_cs_destroy_semaphore(gsgpu_semaphore_handle sem)
+{
+	return gsgpu_cs_unreference_sem(sem);
+}
+
+int gsgpu_cs_create_syncobj2(gsgpu_device_handle dev,
+			      uint32_t  flags,
+			      uint32_t *handle)
+{
+	if (NULL == dev)
+		return -EINVAL;
+
+	return drmSyncobjCreate(dev->fd, flags, handle);
+}
+
+int gsgpu_cs_create_syncobj(gsgpu_device_handle dev,
+			     uint32_t *handle)
+{
+	if (NULL == dev)
+		return -EINVAL;
+
+	return drmSyncobjCreate(dev->fd, 0, handle);
+}
+
+int gsgpu_cs_destroy_syncobj(gsgpu_device_handle dev,
+			      uint32_t handle)
+{
+	if (NULL == dev)
+		return -EINVAL;
+
+	return drmSyncobjDestroy(dev->fd, handle);
+}
+
+int gsgpu_cs_syncobj_reset(gsgpu_device_handle dev,
+			    const uint32_t *syncobjs, uint32_t syncobj_count)
+{
+	if (NULL == dev)
+		return -EINVAL;
+
+	return drmSyncobjReset(dev->fd, syncobjs, syncobj_count);
+}
+
+int gsgpu_cs_syncobj_signal(gsgpu_device_handle dev,
+			     const uint32_t *syncobjs, uint32_t syncobj_count)
+{
+	if (NULL == dev)
+		return -EINVAL;
+
+	return drmSyncobjSignal(dev->fd, syncobjs, syncobj_count);
+}
+
+int gsgpu_cs_syncobj_wait(gsgpu_device_handle dev,
+			   uint32_t *handles, unsigned num_handles,
+			   int64_t timeout_nsec, unsigned flags,
+			   uint32_t *first_signaled)
+{
+	if (NULL == dev)
+		return -EINVAL;
+
+	return drmSyncobjWait(dev->fd, handles, num_handles, timeout_nsec,
+			      flags, first_signaled);
+}
+
+int gsgpu_cs_export_syncobj(gsgpu_device_handle dev,
+			     uint32_t handle,
+			     int *shared_fd)
+{
+	if (NULL == dev)
+		return -EINVAL;
+
+	return drmSyncobjHandleToFD(dev->fd, handle, shared_fd);
+}
+
+int gsgpu_cs_import_syncobj(gsgpu_device_handle dev,
+			     int shared_fd,
+			     uint32_t *handle)
+{
+	if (NULL == dev)
+		return -EINVAL;
+
+	return drmSyncobjFDToHandle(dev->fd, shared_fd, handle);
+}
+
+int gsgpu_cs_syncobj_export_sync_file(gsgpu_device_handle dev,
+				       uint32_t syncobj,
+				       int *sync_file_fd)
+{
+	if (NULL == dev)
+		return -EINVAL;
+
+	return drmSyncobjExportSyncFile(dev->fd, syncobj, sync_file_fd);
+}
+
+int gsgpu_cs_syncobj_import_sync_file(gsgpu_device_handle dev,
+				       uint32_t syncobj,
+				       int sync_file_fd)
+{
+	if (NULL == dev)
+		return -EINVAL;
+
+	return drmSyncobjImportSyncFile(dev->fd, syncobj, sync_file_fd);
+}
+
+int gsgpu_cs_submit_raw(gsgpu_device_handle dev,
+			 gsgpu_context_handle context,
+			 gsgpu_bo_list_handle bo_list_handle,
+			 int num_chunks,
+			 struct drm_gsgpu_cs_chunk *chunks,
+			 uint64_t *seq_no)
+{
+	union drm_gsgpu_cs cs = {0};
+	uint64_t *chunk_array;
+	int i, r;
+	if (num_chunks == 0)
+		return -EINVAL;
+
+	chunk_array = alloca(sizeof(uint64_t) * num_chunks);
+	for (i = 0; i < num_chunks; i++)
+		chunk_array[i] = (uint64_t)(uintptr_t)&chunks[i];
+	cs.in.chunks = (uint64_t)(uintptr_t)chunk_array;
+	cs.in.ctx_id = context->id;
+	cs.in.bo_list_handle = bo_list_handle ? bo_list_handle->handle : 0;
+	cs.in.num_chunks = num_chunks;
+	r = drmCommandWriteRead(dev->fd, DRM_GSGPU_CS,
+				&cs, sizeof(cs));
+	if (r)
+		return r;
+
+	if (seq_no)
+		*seq_no = cs.out.handle;
+	return 0;
+}
+
+void gsgpu_cs_chunk_fence_info_to_data(struct gsgpu_cs_fence_info *fence_info,
+					struct drm_gsgpu_cs_chunk_data *data)
+{
+	data->fence_data.handle = fence_info->handle->handle;
+	data->fence_data.offset = fence_info->offset * sizeof(uint64_t);
+}
+
+void gsgpu_cs_chunk_fence_to_dep(struct gsgpu_cs_fence *fence,
+				  struct drm_gsgpu_cs_chunk_dep *dep)
+{
+	dep->ip_type = fence->ip_type;
+	dep->ip_instance = fence->ip_instance;
+	dep->ring = fence->ring;
+	dep->ctx_id = fence->context->id;
+	dep->handle = fence->fence;
+}
+
+int gsgpu_cs_fence_to_handle(gsgpu_device_handle dev,
+			      struct gsgpu_cs_fence *fence,
+			      uint32_t what,
+			      uint32_t *out_handle)
+{
+	union drm_gsgpu_fence_to_handle fth = {0};
+	int r;
+
+	fth.in.fence.ctx_id = fence->context->id;
+	fth.in.fence.ip_type = fence->ip_type;
+	fth.in.fence.ip_instance = fence->ip_instance;
+	fth.in.fence.ring = fence->ring;
+	fth.in.fence.seq_no = fence->fence;
+	fth.in.what = what;
+
+	r = drmCommandWriteRead(dev->fd, DRM_GSGPU_FENCE_TO_HANDLE,
+				&fth, sizeof(fth));
+	if (r == 0)
+		*out_handle = fth.out.handle;
+	return r;
+}
diff --git a/gsgpu/gsgpu_device.c b/gsgpu/gsgpu_device.c
new file mode 100644
index 00000000..3058e60f
--- /dev/null
+++ b/gsgpu/gsgpu_device.c
@@ -0,0 +1,334 @@
+/*
+ * Copyright (C) 2020 Loongson Technology Co., Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+/**
+ * \file gsgpu_device.c
+ *
+ *  Implementation of functions for GS GPU device
+ *
+ */
+
+#ifdef HAVE_CONFIG_H
+#include "config.h"
+#endif
+
+#include <sys/stat.h>
+#include <errno.h>
+#include <string.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <unistd.h>
+
+#include "xf86drm.h"
+#include "gsgpu_drm.h"
+#include "gsgpu_internal.h"
+#include "util_hash_table.h"
+#include "util_math.h"
+
+#define PTR_TO_UINT(x) ((unsigned)((intptr_t)(x)))
+#define UINT_TO_PTR(x) ((void *)((intptr_t)(x)))
+
+static pthread_mutex_t fd_mutex = PTHREAD_MUTEX_INITIALIZER;
+static struct util_hash_table *fd_tab;
+
+static unsigned handle_hash(void *key)
+{
+	return PTR_TO_UINT(key);
+}
+
+static int handle_compare(void *key1, void *key2)
+{
+	return PTR_TO_UINT(key1) != PTR_TO_UINT(key2);
+}
+
+static unsigned fd_hash(void *key)
+{
+	int fd = PTR_TO_UINT(key);
+	char *name = drmGetPrimaryDeviceNameFromFd(fd);
+	unsigned result = 0;
+	char *c;
+
+	if (name == NULL)
+		return 0;
+
+	for (c = name; *c; ++c)
+		result += *c;
+
+	free(name);
+
+	return result;
+}
+
+static int fd_compare(void *key1, void *key2)
+{
+	int fd1 = PTR_TO_UINT(key1);
+	int fd2 = PTR_TO_UINT(key2);
+	char *name1 = drmGetPrimaryDeviceNameFromFd(fd1);
+	char *name2 = drmGetPrimaryDeviceNameFromFd(fd2);
+	int result;
+
+	if (name1 == NULL || name2 == NULL) {
+		free(name1);
+		free(name2);
+		return 0;
+	}
+
+	result = strcmp(name1, name2);
+	free(name1);
+	free(name2);
+
+	return result;
+}
+
+/**
+* Get the authenticated form fd,
+*
+* \param   fd   - \c [in]  File descriptor for GS GPU device
+* \param   auth - \c [out] Pointer to output the fd is authenticated or not
+*                          A render node fd, output auth = 0
+*                          A legacy fd, get the authenticated for compatibility root
+*
+* \return   0 on success\n
+*          >0 - GS specific error code\n
+*          <0 - Negative POSIX Error code
+*/
+static int gsgpu_get_auth(int fd, int *auth)
+{
+	int r = 0;
+	drm_client_t client = {};
+
+	if (drmGetNodeTypeFromFd(fd) == DRM_NODE_RENDER)
+		*auth = 0;
+	else {
+		client.idx = 0;
+		r = drmIoctl(fd, DRM_IOCTL_GET_CLIENT, &client);
+		if (!r)
+			*auth = client.auth;
+	}
+	return r;
+}
+
+static void gsgpu_device_free_internal(gsgpu_device_handle dev)
+{
+	gsgpu_vamgr_deinit(&dev->vamgr_32);
+	gsgpu_vamgr_deinit(&dev->vamgr);
+	gsgpu_vamgr_deinit(&dev->vamgr_high_32);
+	gsgpu_vamgr_deinit(&dev->vamgr_high);
+	util_hash_table_destroy(dev->bo_flink_names);
+	util_hash_table_destroy(dev->bo_handles);
+	pthread_mutex_destroy(&dev->bo_table_mutex);
+	util_hash_table_remove(fd_tab, UINT_TO_PTR(dev->fd));
+	close(dev->fd);
+	if ((dev->flink_fd >= 0) && (dev->fd != dev->flink_fd))
+		close(dev->flink_fd);
+	free(dev->marketing_name);
+	free(dev);
+}
+
+/**
+ * Assignment between two gsgpu_device pointers with reference counting.
+ *
+ * Usage:
+ *    struct gsgpu_device *dst = ... , *src = ...;
+ *
+ *    dst = src;
+ *    // No reference counting. Only use this when you need to move
+ *    // a reference from one pointer to another.
+ *
+ *    gsgpu_device_reference(&dst, src);
+ *    // Reference counters are updated. dst is decremented and src is
+ *    // incremented. dst is freed if its reference counter is 0.
+ */
+static void gsgpu_device_reference(struct gsgpu_device **dst,
+			     struct gsgpu_device *src)
+{
+	if (update_references(&(*dst)->refcount, &src->refcount))
+		gsgpu_device_free_internal(*dst);
+	*dst = src;
+}
+
+int gsgpu_device_initialize(int fd,
+			     uint32_t *major_version,
+			     uint32_t *minor_version,
+			     gsgpu_device_handle *device_handle)
+{
+	struct gsgpu_device *dev;
+	drmVersionPtr version;
+	int r;
+	int flag_auth = 0;
+	int flag_authexist=0;
+	uint32_t accel_working = 0;
+	uint64_t start, max;
+
+	*device_handle = NULL;
+
+	pthread_mutex_lock(&fd_mutex);
+	if (!fd_tab)
+		fd_tab = util_hash_table_create(fd_hash, fd_compare);
+	r = gsgpu_get_auth(fd, &flag_auth);
+	if (r) {
+		fprintf(stderr, "%s: gsgpu_get_auth (1) failed (%i)\n",
+			__func__, r);
+		pthread_mutex_unlock(&fd_mutex);
+		return r;
+	}
+	dev = util_hash_table_get(fd_tab, UINT_TO_PTR(fd));
+	if (dev) {
+		r = gsgpu_get_auth(dev->fd, &flag_authexist);
+		if (r) {
+			fprintf(stderr, "%s: gsgpu_get_auth (2) failed (%i)\n",
+				__func__, r);
+			pthread_mutex_unlock(&fd_mutex);
+			return r;
+		}
+		if ((flag_auth) && (!flag_authexist)) {
+			dev->flink_fd = dup(fd);
+		}
+		*major_version = dev->major_version;
+		*minor_version = dev->minor_version;
+		gsgpu_device_reference(device_handle, dev);
+		pthread_mutex_unlock(&fd_mutex);
+		return 0;
+	}
+
+	dev = calloc(1, sizeof(struct gsgpu_device));
+	if (!dev) {
+		fprintf(stderr, "%s: calloc failed\n", __func__);
+		pthread_mutex_unlock(&fd_mutex);
+		return -ENOMEM;
+	}
+
+	dev->fd = -1;
+	dev->flink_fd = -1;
+
+	atomic_set(&dev->refcount, 1);
+
+	version = drmGetVersion(fd);
+	if (version->version_major != 0) {
+		fprintf(stderr, "%s: DRM version is %d.%d.%d but this driver is "
+			"only compatible with 0.x.x.\n",
+			__func__,
+			version->version_major,
+			version->version_minor,
+			version->version_patchlevel);
+		drmFreeVersion(version);
+		r = -EBADF;
+		goto cleanup;
+	}
+
+	dev->fd = dup(fd);
+	dev->flink_fd = dev->fd;
+	dev->major_version = version->version_major;
+	dev->minor_version = version->version_minor;
+	drmFreeVersion(version);
+
+	dev->bo_flink_names = util_hash_table_create(handle_hash,
+						     handle_compare);
+	dev->bo_handles = util_hash_table_create(handle_hash, handle_compare);
+	pthread_mutex_init(&dev->bo_table_mutex, NULL);
+
+	/* Check if acceleration is working. */
+	r = gsgpu_query_info(dev, GSGPU_INFO_ACCEL_WORKING, 4, &accel_working);
+	if (r) {
+		fprintf(stderr, "%s: gsgpu_query_info(ACCEL_WORKING) failed (%i)\n",
+			__func__, r);
+		goto cleanup;
+	}
+	if (!accel_working) {
+		fprintf(stderr, "%s: GSGPU_INFO_ACCEL_WORKING = 0\n", __func__);
+		r = -EBADF;
+		goto cleanup;
+	}
+
+	r = gsgpu_query_gpu_info_init(dev);
+	if (r) {
+		fprintf(stderr, "%s: gsgpu_query_gpu_info_init failed\n", __func__);
+		goto cleanup;
+	}
+
+	start = dev->dev_info.virtual_address_offset;
+	max = MIN2(dev->dev_info.virtual_address_max, 0x100000000ULL);
+	gsgpu_vamgr_init(&dev->vamgr_32, start, max,
+			  dev->dev_info.virtual_address_alignment);
+
+	start = max;
+	max = MAX2(dev->dev_info.virtual_address_max, 0x100000000ULL);
+	gsgpu_vamgr_init(&dev->vamgr, start, max,
+			  dev->dev_info.virtual_address_alignment);
+
+	start = dev->dev_info.high_va_offset;
+	max = MIN2(dev->dev_info.high_va_max, (start & ~0xffffffffULL) +
+		   0x100000000ULL);
+	gsgpu_vamgr_init(&dev->vamgr_high_32, start, max,
+			  dev->dev_info.virtual_address_alignment);
+
+	start = max;
+	max = MAX2(dev->dev_info.high_va_max, (start & ~0xffffffffULL) +
+		   0x100000000ULL);
+	gsgpu_vamgr_init(&dev->vamgr_high, start, max,
+			  dev->dev_info.virtual_address_alignment);
+
+	gsgpu_parse_asic_ids(dev);
+
+	*major_version = dev->major_version;
+	*minor_version = dev->minor_version;
+	*device_handle = dev;
+	util_hash_table_set(fd_tab, UINT_TO_PTR(dev->fd), dev);
+	pthread_mutex_unlock(&fd_mutex);
+
+	return 0;
+
+cleanup:
+	if (dev->fd >= 0)
+		close(dev->fd);
+	free(dev);
+	pthread_mutex_unlock(&fd_mutex);
+	return r;
+}
+
+int gsgpu_device_deinitialize(gsgpu_device_handle dev)
+{
+	gsgpu_device_reference(&dev, NULL);
+	return 0;
+}
+
+const char *gsgpu_get_marketing_name(gsgpu_device_handle dev)
+{
+	return dev->marketing_name;
+}
+
+int gsgpu_query_sw_info(gsgpu_device_handle dev, enum gsgpu_sw_info info,
+			 void *value)
+{
+	uint32_t *val32 = (uint32_t*)value;
+
+	switch (info) {
+	case gsgpu_sw_info_address32_hi:
+		if (dev->vamgr_high_32.va_max)
+			*val32 = (dev->vamgr_high_32.va_max - 1) >> 32;
+		else
+			*val32 = (dev->vamgr_32.va_max - 1) >> 32;
+		return 0;
+	}
+	return -EINVAL;
+}
diff --git a/gsgpu/gsgpu_gpu_info.c b/gsgpu/gsgpu_gpu_info.c
new file mode 100644
index 00000000..7174fee1
--- /dev/null
+++ b/gsgpu/gsgpu_gpu_info.c
@@ -0,0 +1,281 @@
+/*
+ * Copyright (C) 2020 Loongson Technology Co., Ltd.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#ifdef HAVE_CONFIG_H
+#include "config.h"
+#endif
+
+#include <errno.h>
+#include <string.h>
+
+#include "gsgpu.h"
+#include "gsgpu_drm.h"
+#include "gsgpu_internal.h"
+#include "xf86drm.h"
+
+int gsgpu_query_info(gsgpu_device_handle dev, unsigned info_id,
+		      unsigned size, void *value)
+{
+	struct drm_gsgpu_info request;
+
+	memset(&request, 0, sizeof(request));
+	request.return_pointer = (uintptr_t)value;
+	request.return_size = size;
+	request.query = info_id;
+
+	return drmCommandWrite(dev->fd, DRM_GSGPU_INFO, &request,
+			       sizeof(struct drm_gsgpu_info));
+}
+
+int gsgpu_query_crtc_from_id(gsgpu_device_handle dev, unsigned id,
+			      int32_t *result)
+{
+	struct drm_gsgpu_info request;
+
+	memset(&request, 0, sizeof(request));
+	request.return_pointer = (uintptr_t)result;
+	request.return_size = sizeof(*result);
+	request.query = GSGPU_INFO_CRTC_FROM_ID;
+	request.mode_crtc.id = id;
+
+	return drmCommandWrite(dev->fd, DRM_GSGPU_INFO, &request,
+			       sizeof(struct drm_gsgpu_info));
+}
+
+int gsgpu_read_mm_registers(gsgpu_device_handle dev, unsigned dword_offset,
+			     unsigned count, uint32_t instance, uint32_t flags,
+			     uint32_t *values)
+{
+	struct drm_gsgpu_info request;
+
+	memset(&request, 0, sizeof(request));
+	request.return_pointer = (uintptr_t)values;
+	request.return_size = count * sizeof(uint32_t);
+	request.query = GSGPU_INFO_READ_MMR_REG;
+	request.read_mmr_reg.dword_offset = dword_offset;
+	request.read_mmr_reg.count = count;
+	request.read_mmr_reg.instance = instance;
+	request.read_mmr_reg.flags = flags;
+
+	return drmCommandWrite(dev->fd, DRM_GSGPU_INFO, &request,
+			       sizeof(struct drm_gsgpu_info));
+}
+
+int gsgpu_query_hw_ip_count(gsgpu_device_handle dev, unsigned type,
+			     uint32_t *count)
+{
+	struct drm_gsgpu_info request;
+
+	memset(&request, 0, sizeof(request));
+	request.return_pointer = (uintptr_t)count;
+	request.return_size = sizeof(*count);
+	request.query = GSGPU_INFO_HW_IP_COUNT;
+	request.query_hw_ip.type = type;
+
+	return drmCommandWrite(dev->fd, DRM_GSGPU_INFO, &request,
+			       sizeof(struct drm_gsgpu_info));
+}
+
+int gsgpu_query_hw_ip_info(gsgpu_device_handle dev, unsigned type,
+			    unsigned ip_instance,
+			    struct drm_gsgpu_info_hw_ip *info)
+{
+	struct drm_gsgpu_info request;
+
+	memset(&request, 0, sizeof(request));
+	request.return_pointer = (uintptr_t)info;
+	request.return_size = sizeof(*info);
+	request.query = GSGPU_INFO_HW_IP_INFO;
+	request.query_hw_ip.type = type;
+	request.query_hw_ip.ip_instance = ip_instance;
+
+	return drmCommandWrite(dev->fd, DRM_GSGPU_INFO, &request,
+			       sizeof(struct drm_gsgpu_info));
+}
+
+int gsgpu_query_firmware_version(gsgpu_device_handle dev, unsigned fw_type,
+				  unsigned ip_instance, unsigned index,
+				  uint32_t *version, uint32_t *feature)
+{
+	struct drm_gsgpu_info request;
+	struct drm_gsgpu_info_firmware firmware = {};
+	int r;
+
+	memset(&request, 0, sizeof(request));
+	request.return_pointer = (uintptr_t)&firmware;
+	request.return_size = sizeof(firmware);
+	request.query = GSGPU_INFO_FW_VERSION;
+	request.query_fw.fw_type = fw_type;
+	request.query_fw.ip_instance = ip_instance;
+	request.query_fw.index = index;
+
+	r = drmCommandWrite(dev->fd, DRM_GSGPU_INFO, &request,
+			    sizeof(struct drm_gsgpu_info));
+	if (r)
+		return r;
+
+	*version = firmware.ver;
+	*feature = firmware.feature;
+	return 0;
+}
+
+drm_private int gsgpu_query_gpu_info_init(gsgpu_device_handle dev)
+{
+	int r;
+
+	r = gsgpu_query_info(dev, GSGPU_INFO_DEV_INFO, sizeof(dev->dev_info),
+			      &dev->dev_info);
+	if (r)
+		return r;
+
+	dev->info.asic_id = dev->dev_info.device_id;
+	dev->info.chip_rev = dev->dev_info.chip_rev;
+	dev->info.chip_external_rev = dev->dev_info.external_rev;
+	dev->info.family_id = dev->dev_info.family;
+	dev->info.max_engine_clk = dev->dev_info.max_engine_clock;
+	dev->info.max_memory_clk = dev->dev_info.max_memory_clock;
+	dev->info.gpu_counter_freq = dev->dev_info.gpu_counter_freq;
+	dev->info.enabled_rb_pipes_mask = dev->dev_info.enabled_rb_pipes_mask;
+	dev->info.rb_pipes = dev->dev_info.num_rb_pipes;
+	dev->info.ids_flags = dev->dev_info.ids_flags;
+	dev->info.num_hw_gfx_contexts = dev->dev_info.num_hw_gfx_contexts;
+	dev->info.num_shader_engines = dev->dev_info.num_shader_engines;
+	dev->info.num_shader_arrays_per_engine =
+		dev->dev_info.num_shader_arrays_per_engine;
+	dev->info.vram_type = dev->dev_info.vram_type;
+	dev->info.vram_bit_width = dev->dev_info.vram_bit_width;
+	dev->info.ce_ram_size = dev->dev_info.ce_ram_size;
+	dev->info.vce_harvest_config = dev->dev_info.vce_harvest_config;
+	dev->info.pci_rev_id = dev->dev_info.pci_rev;
+
+	dev->info.cu_active_number = dev->dev_info.cu_active_number;
+	dev->info.cu_ao_mask = dev->dev_info.cu_ao_mask;
+	memcpy(&dev->info.cu_bitmap[0][0], &dev->dev_info.cu_bitmap[0][0], sizeof(dev->info.cu_bitmap));
+
+	return 0;
+}
+
+int gsgpu_query_gpu_info(gsgpu_device_handle dev,
+			struct gsgpu_gpu_info *info)
+{
+	if (!dev || !info)
+		return -EINVAL;
+
+	/* Get ASIC info*/
+	*info = dev->info;
+
+	return 0;
+}
+
+int gsgpu_query_heap_info(gsgpu_device_handle dev,
+			uint32_t heap,
+			uint32_t flags,
+			struct gsgpu_heap_info *info)
+{
+	struct drm_gsgpu_info_vram_gtt vram_gtt_info = {};
+	int r;
+
+	r = gsgpu_query_info(dev, GSGPU_INFO_VRAM_GTT,
+			      sizeof(vram_gtt_info), &vram_gtt_info);
+	if (r)
+		return r;
+
+	/* Get heap information */
+	switch (heap) {
+	case GSGPU_GEM_DOMAIN_VRAM:
+		/* query visible only vram heap */
+		if (flags & GSGPU_GEM_CREATE_CPU_ACCESS_REQUIRED)
+			info->heap_size = vram_gtt_info.vram_cpu_accessible_size;
+		else /* query total vram heap */
+			info->heap_size = vram_gtt_info.vram_size;
+
+		info->max_allocation = vram_gtt_info.vram_cpu_accessible_size;
+
+		if (flags & GSGPU_GEM_CREATE_CPU_ACCESS_REQUIRED)
+			r = gsgpu_query_info(dev, GSGPU_INFO_VIS_VRAM_USAGE,
+					      sizeof(info->heap_usage),
+					      &info->heap_usage);
+		else
+			r = gsgpu_query_info(dev, GSGPU_INFO_VRAM_USAGE,
+					      sizeof(info->heap_usage),
+					      &info->heap_usage);
+		if (r)
+			return r;
+		break;
+	case GSGPU_GEM_DOMAIN_GTT:
+		info->heap_size = vram_gtt_info.gtt_size;
+		info->max_allocation = vram_gtt_info.vram_cpu_accessible_size;
+
+		r = gsgpu_query_info(dev, GSGPU_INFO_GTT_USAGE,
+				      sizeof(info->heap_usage),
+				      &info->heap_usage);
+		if (r)
+			return r;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+int gsgpu_query_gds_info(gsgpu_device_handle dev,
+			struct gsgpu_gds_resource_info *gds_info)
+{
+	struct drm_gsgpu_info_gds gds_config = {};
+        int r;
+
+	if (!gds_info)
+		return -EINVAL;
+
+        r = gsgpu_query_info(dev, GSGPU_INFO_GDS_CONFIG,
+                              sizeof(gds_config), &gds_config);
+        if (r)
+                return r;
+
+	gds_info->gds_gfx_partition_size = gds_config.gds_gfx_partition_size;
+	gds_info->compute_partition_size = gds_config.compute_partition_size;
+	gds_info->gds_total_size = gds_config.gds_total_size;
+	gds_info->gws_per_gfx_partition = gds_config.gws_per_gfx_partition;
+	gds_info->gws_per_compute_partition = gds_config.gws_per_compute_partition;
+	gds_info->oa_per_gfx_partition = gds_config.oa_per_gfx_partition;
+	gds_info->oa_per_compute_partition = gds_config.oa_per_compute_partition;
+
+	return 0;
+}
+
+int gsgpu_query_sensor_info(gsgpu_device_handle dev, unsigned sensor_type,
+			     unsigned size, void *value)
+{
+	struct drm_gsgpu_info request;
+
+	memset(&request, 0, sizeof(request));
+	request.return_pointer = (uintptr_t)value;
+	request.return_size = size;
+	request.query = GSGPU_INFO_SENSOR;
+	request.sensor_info.type = sensor_type;
+
+	return drmCommandWrite(dev->fd, DRM_GSGPU_INFO, &request,
+			       sizeof(struct drm_gsgpu_info));
+}
diff --git a/gsgpu/gsgpu_hw_sema.c b/gsgpu/gsgpu_hw_sema.c
new file mode 100644
index 00000000..5696ac7e
--- /dev/null
+++ b/gsgpu/gsgpu_hw_sema.c
@@ -0,0 +1,64 @@
+/*
+ * Copyright (C) 2020 Loongson Technology Co., Ltd.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#ifdef HAVE_CONFIG_H
+#include "config.h"
+#endif
+
+#include "libdrm_macros.h"
+#include "xf86drm.h"
+#include "gsgpu_drm.h"
+#include "gsgpu_internal.h"
+
+int gsgpu_hw_sema_get(gsgpu_device_handle dev, gsgpu_context_handle ctx, uint64_t *sema)
+{
+	int r = 0;
+	struct drm_gsgpu_hw_sema args = {0};
+
+	args.ctx_id = ctx->id;
+	args.ops = GSGPU_HW_SEMA_GET;
+
+	r = drmCommandWriteRead(dev->fd, DRM_GSGPU_HWSEMA_OP,
+				&args, sizeof(args));
+	if (!r)
+		*sema = args.id;
+
+	return r;
+}
+
+int gsgpu_hw_sema_put(gsgpu_device_handle dev, gsgpu_context_handle ctx, uint64_t sema)
+{
+	int r = 0;
+
+	struct drm_gsgpu_hw_sema args = {0};
+
+	args.id = sema;
+	args.ctx_id = ctx->id;
+	args.ops = GSGPU_HW_SEMA_PUT;
+
+	r = drmCommandWriteRead(dev->fd, DRM_GSGPU_HWSEMA_OP,
+				&args, sizeof(args));
+
+	return r;
+}
diff --git a/gsgpu/gsgpu_internal.h b/gsgpu/gsgpu_internal.h
new file mode 100644
index 00000000..18bbbc24
--- /dev/null
+++ b/gsgpu/gsgpu_internal.h
@@ -0,0 +1,178 @@
+/*
+ * Copyright (C) 2020 Loongson Technology Co., Ltd.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#ifndef _GSGPU_INTERNAL_H_
+#define _GSGPU_INTERNAL_H_
+
+#ifdef HAVE_CONFIG_H
+#include "config.h"
+#endif
+
+#include <assert.h>
+#include <pthread.h>
+
+#include "libdrm_macros.h"
+#include "xf86atomic.h"
+#include "gsgpu.h"
+#include "util_double_list.h"
+
+#define GSGPU_CS_MAX_RINGS 8
+/* do not use below macro if b is not power of 2 aligned value */
+#define __round_mask(x, y) ((__typeof__(x))((y)-1))
+#define ROUND_UP(x, y) ((((x)-1) | __round_mask(x, y))+1)
+#define ROUND_DOWN(x, y) ((x) & ~__round_mask(x, y))
+
+#define GSGPU_INVALID_VA_ADDRESS	0xffffffffffffffff
+#define GSGPU_NULL_SUBMIT_SEQ		0
+
+struct gsgpu_bo_va_hole {
+	struct list_head list;
+	uint64_t offset;
+	uint64_t size;
+};
+
+struct gsgpu_bo_va_mgr {
+	uint64_t va_max;
+	struct list_head va_holes;
+	pthread_mutex_t bo_va_mutex;
+	uint32_t va_alignment;
+};
+
+struct gsgpu_va {
+	gsgpu_device_handle dev;
+	uint64_t address;
+	uint64_t size;
+	enum gsgpu_gpu_va_range range;
+	struct gsgpu_bo_va_mgr *vamgr;
+};
+
+struct gsgpu_device {
+	atomic_t refcount;
+	int fd;
+	int flink_fd;
+	unsigned major_version;
+	unsigned minor_version;
+
+	char *marketing_name;
+	/** List of buffer handles. Protected by bo_table_mutex. */
+	struct util_hash_table *bo_handles;
+	/** List of buffer GEM flink names. Protected by bo_table_mutex. */
+	struct util_hash_table *bo_flink_names;
+	/** This protects all hash tables. */
+	pthread_mutex_t bo_table_mutex;
+	struct drm_gsgpu_info_device dev_info;
+	struct gsgpu_gpu_info info;
+	/** The VA manager for the lower virtual address space */
+	struct gsgpu_bo_va_mgr vamgr;
+	/** The VA manager for the 32bit address space */
+	struct gsgpu_bo_va_mgr vamgr_32;
+	/** The VA manager for the high virtual address space */
+	struct gsgpu_bo_va_mgr vamgr_high;
+	/** The VA manager for the 32bit high address space */
+	struct gsgpu_bo_va_mgr vamgr_high_32;
+};
+
+struct gsgpu_bo {
+	atomic_t refcount;
+	struct gsgpu_device *dev;
+
+	uint64_t alloc_size;
+
+	uint32_t handle;
+	uint32_t flink_name;
+
+	pthread_mutex_t cpu_access_mutex;
+	void *cpu_ptr;
+	int cpu_map_count;
+};
+
+struct gsgpu_bo_list {
+	struct gsgpu_device *dev;
+
+	uint32_t handle;
+};
+
+struct gsgpu_context {
+	struct gsgpu_device *dev;
+	/** Mutex for accessing fences and to maintain command submissions
+	    in good sequence. */
+	pthread_mutex_t sequence_mutex;
+	/* context id*/
+	uint32_t id;
+	uint64_t last_seq[GSGPU_HW_IP_NUM][GSGPU_HW_IP_INSTANCE_MAX_COUNT][GSGPU_CS_MAX_RINGS];
+	struct list_head sem_list[GSGPU_HW_IP_NUM][GSGPU_HW_IP_INSTANCE_MAX_COUNT][GSGPU_CS_MAX_RINGS];
+};
+
+/**
+ * Structure describing sw semaphore based on scheduler
+ *
+ */
+struct gsgpu_semaphore {
+	atomic_t refcount;
+	struct list_head list;
+	struct gsgpu_cs_fence signal_fence;
+};
+
+/**
+ * Functions.
+ */
+
+drm_private void gsgpu_vamgr_init(struct gsgpu_bo_va_mgr *mgr, uint64_t start,
+		       uint64_t max, uint64_t alignment);
+
+drm_private void gsgpu_vamgr_deinit(struct gsgpu_bo_va_mgr *mgr);
+
+drm_private void gsgpu_parse_asic_ids(struct gsgpu_device *dev);
+
+drm_private int gsgpu_query_gpu_info_init(gsgpu_device_handle dev);
+
+drm_private uint64_t gsgpu_cs_calculate_timeout(uint64_t timeout);
+
+/**
+ * Inline functions.
+ */
+
+/**
+ * Increment src and decrement dst as if we were updating references
+ * for an assignment between 2 pointers of some objects.
+ *
+ * \return  true if dst is 0
+ */
+static inline bool update_references(atomic_t *dst, atomic_t *src)
+{
+	if (dst != src) {
+		/* bump src first */
+		if (src) {
+			assert(atomic_read(src) > 0);
+			atomic_inc(src);
+		}
+		if (dst) {
+			assert(atomic_read(dst) > 0);
+			return atomic_dec_and_test(dst);
+		}
+	}
+	return false;
+}
+
+#endif
diff --git a/gsgpu/gsgpu_vamgr.c b/gsgpu/gsgpu_vamgr.c
new file mode 100644
index 00000000..62e85c90
--- /dev/null
+++ b/gsgpu/gsgpu_vamgr.c
@@ -0,0 +1,267 @@
+/*
+ * Copyright (C) 2020 Loongson Technology Co., Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#ifdef HAVE_CONFIG_H
+#include "config.h"
+#endif
+
+#include <stdlib.h>
+#include <string.h>
+#include <errno.h>
+#include "gsgpu.h"
+#include "gsgpu_drm.h"
+#include "gsgpu_internal.h"
+#include "util_math.h"
+
+int gsgpu_va_range_query(gsgpu_device_handle dev,
+			  enum gsgpu_gpu_va_range type,
+			  uint64_t *start, uint64_t *end)
+{
+	if (type != gsgpu_gpu_va_range_general)
+		return -EINVAL;
+
+	*start = dev->dev_info.virtual_address_offset;
+	*end = dev->dev_info.virtual_address_max;
+	return 0;
+}
+
+drm_private void gsgpu_vamgr_init(struct gsgpu_bo_va_mgr *mgr, uint64_t start,
+				   uint64_t max, uint64_t alignment)
+{
+	struct gsgpu_bo_va_hole *n;
+
+	mgr->va_max = max;
+	mgr->va_alignment = alignment;
+
+	list_inithead(&mgr->va_holes);
+	pthread_mutex_init(&mgr->bo_va_mutex, NULL);
+	pthread_mutex_lock(&mgr->bo_va_mutex);
+	n = calloc(1, sizeof(struct gsgpu_bo_va_hole));
+	n->size = mgr->va_max - start;
+	n->offset = start;
+	list_add(&n->list, &mgr->va_holes);
+	pthread_mutex_unlock(&mgr->bo_va_mutex);
+}
+
+drm_private void gsgpu_vamgr_deinit(struct gsgpu_bo_va_mgr *mgr)
+{
+	struct gsgpu_bo_va_hole *hole, *tmp;
+	LIST_FOR_EACH_ENTRY_SAFE(hole, tmp, &mgr->va_holes, list) {
+		list_del(&hole->list);
+		free(hole);
+	}
+	pthread_mutex_destroy(&mgr->bo_va_mutex);
+}
+
+static drm_private uint64_t
+gsgpu_vamgr_find_va(struct gsgpu_bo_va_mgr *mgr, uint64_t size,
+		     uint64_t alignment, uint64_t base_required)
+{
+	struct gsgpu_bo_va_hole *hole, *n;
+	uint64_t offset = 0, waste = 0;
+
+
+	alignment = MAX2(alignment, mgr->va_alignment);
+	size = ALIGN(size, mgr->va_alignment);
+
+	if (base_required % alignment)
+		return GSGPU_INVALID_VA_ADDRESS;
+
+	pthread_mutex_lock(&mgr->bo_va_mutex);
+	LIST_FOR_EACH_ENTRY_SAFE_REV(hole, n, &mgr->va_holes, list) {
+		if (base_required) {
+			if (hole->offset > base_required ||
+			    (hole->offset + hole->size) < (base_required + size))
+				continue;
+			waste = base_required - hole->offset;
+			offset = base_required;
+		} else {
+			offset = hole->offset;
+			waste = offset % alignment;
+			waste = waste ? alignment - waste : 0;
+			offset += waste;
+			if (offset >= (hole->offset + hole->size)) {
+				continue;
+			}
+		}
+		if (!waste && hole->size == size) {
+			offset = hole->offset;
+			list_del(&hole->list);
+			free(hole);
+			pthread_mutex_unlock(&mgr->bo_va_mutex);
+			return offset;
+		}
+		if ((hole->size - waste) > size) {
+			if (waste) {
+				n = calloc(1, sizeof(struct gsgpu_bo_va_hole));
+				n->size = waste;
+				n->offset = hole->offset;
+				list_add(&n->list, &hole->list);
+			}
+			hole->size -= (size + waste);
+			hole->offset += size + waste;
+			pthread_mutex_unlock(&mgr->bo_va_mutex);
+			return offset;
+		}
+		if ((hole->size - waste) == size) {
+			hole->size = waste;
+			pthread_mutex_unlock(&mgr->bo_va_mutex);
+			return offset;
+		}
+	}
+
+	pthread_mutex_unlock(&mgr->bo_va_mutex);
+	return GSGPU_INVALID_VA_ADDRESS;
+}
+
+static drm_private void
+gsgpu_vamgr_free_va(struct gsgpu_bo_va_mgr *mgr, uint64_t va, uint64_t size)
+{
+	struct gsgpu_bo_va_hole *hole, *next;
+
+	if (va == GSGPU_INVALID_VA_ADDRESS)
+		return;
+
+	size = ALIGN(size, mgr->va_alignment);
+
+	pthread_mutex_lock(&mgr->bo_va_mutex);
+	hole = container_of(&mgr->va_holes, hole, list);
+	LIST_FOR_EACH_ENTRY(next, &mgr->va_holes, list) {
+		if (next->offset < va)
+			break;
+		hole = next;
+	}
+
+	if (&hole->list != &mgr->va_holes) {
+		/* Grow upper hole if it's adjacent */
+		if (hole->offset == (va + size)) {
+			hole->offset = va;
+			hole->size += size;
+			/* Merge lower hole if it's adjacent */
+			if (next != hole &&
+			    &next->list != &mgr->va_holes &&
+			    (next->offset + next->size) == va) {
+				next->size += hole->size;
+				list_del(&hole->list);
+				free(hole);
+			}
+			goto out;
+		}
+	}
+
+	/* Grow lower hole if it's adjacent */
+	if (next != hole && &next->list != &mgr->va_holes &&
+	    (next->offset + next->size) == va) {
+		next->size += size;
+		goto out;
+	}
+
+	/* FIXME on allocation failure we just lose virtual address space
+	 * maybe print a warning
+	 */
+	next = calloc(1, sizeof(struct gsgpu_bo_va_hole));
+	if (next) {
+		next->size = size;
+		next->offset = va;
+		list_add(&next->list, &hole->list);
+	}
+
+out:
+	pthread_mutex_unlock(&mgr->bo_va_mutex);
+}
+
+int gsgpu_va_range_alloc(gsgpu_device_handle dev,
+			  enum gsgpu_gpu_va_range va_range_type,
+			  uint64_t size,
+			  uint64_t va_base_alignment,
+			  uint64_t va_base_required,
+			  uint64_t *va_base_allocated,
+			  gsgpu_va_handle *va_range_handle,
+			  uint64_t flags)
+{
+	struct gsgpu_bo_va_mgr *vamgr;
+
+	/* Clear the flag when the high VA manager is not initialized */
+	if (flags & GSGPU_VA_RANGE_HIGH && !dev->vamgr_high_32.va_max)
+		flags &= ~GSGPU_VA_RANGE_HIGH;
+
+	if (flags & GSGPU_VA_RANGE_HIGH) {
+		if (flags & GSGPU_VA_RANGE_32_BIT)
+			vamgr = &dev->vamgr_high_32;
+		else
+			vamgr = &dev->vamgr_high;
+	} else {
+		if (flags & GSGPU_VA_RANGE_32_BIT)
+			vamgr = &dev->vamgr_32;
+		else
+			vamgr = &dev->vamgr;
+	}
+
+	va_base_alignment = MAX2(va_base_alignment, vamgr->va_alignment);
+	size = ALIGN(size, vamgr->va_alignment);
+
+	*va_base_allocated = gsgpu_vamgr_find_va(vamgr, size,
+					va_base_alignment, va_base_required);
+
+	if (!(flags & GSGPU_VA_RANGE_32_BIT) &&
+	    (*va_base_allocated == GSGPU_INVALID_VA_ADDRESS)) {
+		/* fallback to 32bit address */
+		if (flags & GSGPU_VA_RANGE_HIGH)
+			vamgr = &dev->vamgr_high_32;
+		else
+			vamgr = &dev->vamgr_32;
+		*va_base_allocated = gsgpu_vamgr_find_va(vamgr, size,
+					va_base_alignment, va_base_required);
+	}
+
+	if (*va_base_allocated != GSGPU_INVALID_VA_ADDRESS) {
+		struct gsgpu_va* va;
+		va = calloc(1, sizeof(struct gsgpu_va));
+		if(!va){
+			gsgpu_vamgr_free_va(vamgr, *va_base_allocated, size);
+			return -ENOMEM;
+		}
+		va->dev = dev;
+		va->address = *va_base_allocated;
+		va->size = size;
+		va->range = va_range_type;
+		va->vamgr = vamgr;
+		*va_range_handle = va;
+	} else {
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+int gsgpu_va_range_free(gsgpu_va_handle va_range_handle)
+{
+	if(!va_range_handle || !va_range_handle->address)
+		return 0;
+
+	gsgpu_vamgr_free_va(va_range_handle->vamgr,
+			va_range_handle->address,
+			va_range_handle->size);
+	free(va_range_handle);
+	return 0;
+}
diff --git a/gsgpu/gsgpu_vm.c b/gsgpu/gsgpu_vm.c
new file mode 100644
index 00000000..d55ce12c
--- /dev/null
+++ b/gsgpu/gsgpu_vm.c
@@ -0,0 +1,53 @@
+/*
+ * Copyright (C) 2020 Loongson Technology Co., Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#ifdef HAVE_CONFIG_H
+#include "config.h"
+#endif
+
+#include "gsgpu.h"
+#include "gsgpu_drm.h"
+#include "xf86drm.h"
+#include "gsgpu_internal.h"
+
+int gsgpu_vm_reserve_vmid(gsgpu_device_handle dev, uint32_t flags)
+{
+	union drm_gsgpu_vm vm;
+
+	vm.in.op = GSGPU_VM_OP_RESERVE_VMID;
+	vm.in.flags = flags;
+
+	return drmCommandWriteRead(dev->fd, DRM_GSGPU_VM,
+				   &vm, sizeof(vm));
+}
+
+int gsgpu_vm_unreserve_vmid(gsgpu_device_handle dev, uint32_t flags)
+{
+	union drm_gsgpu_vm vm;
+
+	vm.in.op = GSGPU_VM_OP_UNRESERVE_VMID;
+	vm.in.flags = flags;
+
+	return drmCommandWriteRead(dev->fd, DRM_GSGPU_VM,
+				   &vm, sizeof(vm));
+}
diff --git a/gsgpu/libdrm_gsgpu.pc.in b/gsgpu/libdrm_gsgpu.pc.in
new file mode 100644
index 00000000..d9e47157
--- /dev/null
+++ b/gsgpu/libdrm_gsgpu.pc.in
@@ -0,0 +1,11 @@
+prefix=@prefix@
+exec_prefix=@exec_prefix@
+libdir=@libdir@
+includedir=@includedir@
+
+Name: libdrm_gsgpu
+Description: Userspace interface to kernel DRM services for gsgpu
+Version: @PACKAGE_VERSION@
+Libs: -L${libdir} -ldrm_gsgpu
+Cflags: -I${includedir} -I${includedir}/libdrm
+Requires.private: libdrm
diff --git a/gsgpu/meson.build b/gsgpu/meson.build
new file mode 100644
index 00000000..220800c3
--- /dev/null
+++ b/gsgpu/meson.build
@@ -0,0 +1,65 @@
+# Copyright © 2017-2018 Intel Corporation
+
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+
+# The above copyright notice and this permission notice shall be included in
+# all copies or substantial portions of the Software.
+
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+# SOFTWARE.
+
+
+datadir_gsgpu = join_paths(get_option('prefix'), get_option('datadir'), 'libdrm')
+
+libdrm_gsgpu = shared_library(
+  'drm_gsgpu',
+  [
+    files(
+      'gsgpu_asic_id.c', 'gsgpu_bo.c', 'gsgpu_cs.c', 'gsgpu_device.c',
+      'gsgpu_gpu_info.c', 'gsgpu_vamgr.c', 'gsgpu_vm.c', 'util_hash.c',
+      'util_hash_table.c', 'gsgpu_hw_sema.c',
+    ),
+    config_file,
+  ],
+  c_args : [
+    '-DGSGPU_ASIC_ID_TABLE="@0@"'.format(join_paths(datadir_gsgpu, 'gsgpu.ids')),
+  ],
+  include_directories : [inc_root, inc_drm],
+  link_with : libdrm,
+  dependencies : dep_pthread_stubs,
+  version : '1.0.0',
+  install : true,
+)
+
+install_headers('gsgpu.h', subdir : 'libdrm')
+
+pkg.generate(
+  name : 'libdrm_gsgpu',
+  libraries : libdrm_gsgpu,
+  subdirs : ['.', 'libdrm'],
+  version : meson.project_version(),
+  requires_private : 'libdrm',
+  description : 'Userspace interface to kernel DRM services for gsgpu',
+)
+
+ext_libdrm_gsgpu = declare_dependency(
+  link_with : [libdrm, libdrm_gsgpu],
+  include_directories : [inc_drm, include_directories('.')],
+)
+
+test(
+  'gsgpu-symbol-check',
+  prog_bash,
+  env : env_test,
+  args : [files('gsgpu-symbol-check'), libdrm_gsgpu]
+)
diff --git a/gsgpu/util_hash.c b/gsgpu/util_hash.c
new file mode 100644
index 00000000..9b5c74c8
--- /dev/null
+++ b/gsgpu/util_hash.c
@@ -0,0 +1,387 @@
+/**************************************************************************
+ *
+ * Copyright 2007 VMware, Inc.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
+ * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.
+ * IN NO EVENT SHALL VMWARE AND/OR ITS SUPPLIERS BE LIABLE FOR
+ * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ **************************************************************************/
+
+ /*
+  * Authors:
+  *   Zack Rusin <zackr@vmware.com>
+  */
+
+#ifdef HAVE_CONFIG_H
+#include "config.h"
+#endif
+
+#include "util_hash.h"
+
+#include <stdlib.h>
+#include <assert.h>
+
+#define MAX(a, b) ((a > b) ? (a) : (b))
+
+static const int MinNumBits = 4;
+
+static const unsigned char prime_deltas[] = {
+	0,  0,  1,  3,  1,  5,  3,  3,  1,  9,  7,  5,  3,  9, 25,  3,
+	1, 21,  3, 21,  7, 15,  9,  5,  3, 29, 15,  0,  0,  0,  0,  0
+};
+
+static int primeForNumBits(int numBits)
+{
+	return (1 << numBits) + prime_deltas[numBits];
+}
+
+/* Returns the smallest integer n such that
+   primeForNumBits(n) >= hint.
+*/
+static int countBits(int hint)
+{
+	int numBits = 0;
+	int bits = hint;
+
+	while (bits > 1) {
+		bits >>= 1;
+		numBits++;
+	}
+
+	if (numBits >= (int)sizeof(prime_deltas)) {
+		numBits = sizeof(prime_deltas) - 1;
+	} else if (primeForNumBits(numBits) < hint) {
+		++numBits;
+	}
+	return numBits;
+}
+
+struct util_node {
+   struct util_node *next;
+   unsigned key;
+   void *value;
+};
+
+struct util_hash_data {
+   struct util_node *fakeNext;
+   struct util_node **buckets;
+   int size;
+   int nodeSize;
+   short userNumBits;
+   short numBits;
+   int numBuckets;
+};
+
+struct util_hash {
+   union {
+      struct util_hash_data *d;
+      struct util_node      *e;
+   } data;
+};
+
+static void *util_data_allocate_node(struct util_hash_data *hash)
+{
+   return malloc(hash->nodeSize);
+}
+
+static void util_free_node(struct util_node *node)
+{
+   free(node);
+}
+
+static struct util_node *
+util_hash_create_node(struct util_hash *hash,
+                      unsigned akey, void *avalue,
+                      struct util_node **anextNode)
+{
+   struct util_node *node = util_data_allocate_node(hash->data.d);
+
+   if (!node)
+      return NULL;
+
+   node->key = akey;
+   node->value = avalue;
+
+   node->next = (struct util_node*)(*anextNode);
+   *anextNode = node;
+   ++hash->data.d->size;
+   return node;
+}
+
+static void util_data_rehash(struct util_hash_data *hash, int hint)
+{
+   if (hint < 0) {
+      hint = countBits(-hint);
+      if (hint < MinNumBits)
+         hint = MinNumBits;
+      hash->userNumBits = (short)hint;
+      while (primeForNumBits(hint) < (hash->size >> 1))
+         ++hint;
+   } else if (hint < MinNumBits) {
+      hint = MinNumBits;
+   }
+
+   if (hash->numBits != hint) {
+      struct util_node *e = (struct util_node *)(hash);
+      struct util_node **oldBuckets = hash->buckets;
+      int oldNumBuckets = hash->numBuckets;
+      int  i = 0;
+
+      hash->numBits = (short)hint;
+      hash->numBuckets = primeForNumBits(hint);
+      hash->buckets = malloc(sizeof(struct util_node*) * hash->numBuckets);
+      for (i = 0; i < hash->numBuckets; ++i)
+         hash->buckets[i] = e;
+
+      for (i = 0; i < oldNumBuckets; ++i) {
+         struct util_node *firstNode = oldBuckets[i];
+         while (firstNode != e) {
+            unsigned h = firstNode->key;
+            struct util_node *lastNode = firstNode;
+            struct util_node *afterLastNode;
+            struct util_node **beforeFirstNode;
+
+            while (lastNode->next != e && lastNode->next->key == h)
+               lastNode = lastNode->next;
+
+            afterLastNode = lastNode->next;
+            beforeFirstNode = &hash->buckets[h % hash->numBuckets];
+            while (*beforeFirstNode != e)
+               beforeFirstNode = &(*beforeFirstNode)->next;
+            lastNode->next = *beforeFirstNode;
+            *beforeFirstNode = firstNode;
+            firstNode = afterLastNode;
+         }
+      }
+      free(oldBuckets);
+   }
+}
+
+static void util_data_might_grow(struct util_hash_data *hash)
+{
+   if (hash->size >= hash->numBuckets)
+      util_data_rehash(hash, hash->numBits + 1);
+}
+
+static void util_data_has_shrunk(struct util_hash_data *hash)
+{
+   if (hash->size <= (hash->numBuckets >> 3) &&
+       hash->numBits > hash->userNumBits) {
+      int max = MAX(hash->numBits-2, hash->userNumBits);
+      util_data_rehash(hash,  max);
+   }
+}
+
+static struct util_node *util_data_first_node(struct util_hash_data *hash)
+{
+   struct util_node *e = (struct util_node *)(hash);
+   struct util_node **bucket = hash->buckets;
+   int n = hash->numBuckets;
+   while (n--) {
+      if (*bucket != e)
+         return *bucket;
+      ++bucket;
+   }
+   return e;
+}
+
+static struct util_node **util_hash_find_node(struct util_hash *hash, unsigned akey)
+{
+   struct util_node **node;
+
+   if (hash->data.d->numBuckets) {
+      node = (struct util_node **)(&hash->data.d->buckets[akey % hash->data.d->numBuckets]);
+      assert(*node == hash->data.e || (*node)->next);
+      while (*node != hash->data.e && (*node)->key != akey)
+         node = &(*node)->next;
+   } else {
+      node = (struct util_node **)((const struct util_node * const *)(&hash->data.e));
+   }
+   return node;
+}
+
+drm_private struct util_hash_iter
+util_hash_insert(struct util_hash *hash, unsigned key, void *data)
+{
+   util_data_might_grow(hash->data.d);
+
+   {
+      struct util_node **nextNode = util_hash_find_node(hash, key);
+      struct util_node *node = util_hash_create_node(hash, key, data, nextNode);
+      if (!node) {
+         struct util_hash_iter null_iter = {hash, 0};
+         return null_iter;
+      }
+
+      {
+         struct util_hash_iter iter = {hash, node};
+         return iter;
+      }
+   }
+}
+
+drm_private struct util_hash *util_hash_create(void)
+{
+   struct util_hash *hash = malloc(sizeof(struct util_hash));
+   if (!hash)
+      return NULL;
+
+   hash->data.d = malloc(sizeof(struct util_hash_data));
+   if (!hash->data.d) {
+      free(hash);
+      return NULL;
+   }
+
+   hash->data.d->fakeNext = 0;
+   hash->data.d->buckets = 0;
+   hash->data.d->size = 0;
+   hash->data.d->nodeSize = sizeof(struct util_node);
+   hash->data.d->userNumBits = (short)MinNumBits;
+   hash->data.d->numBits = 0;
+   hash->data.d->numBuckets = 0;
+
+   return hash;
+}
+
+drm_private void util_hash_delete(struct util_hash *hash)
+{
+   struct util_node *e_for_x = (struct util_node *)(hash->data.d);
+   struct util_node **bucket = (struct util_node **)(hash->data.d->buckets);
+   int n = hash->data.d->numBuckets;
+   while (n--) {
+      struct util_node *cur = *bucket++;
+      while (cur != e_for_x) {
+         struct util_node *next = cur->next;
+         util_free_node(cur);
+         cur = next;
+      }
+   }
+   free(hash->data.d->buckets);
+   free(hash->data.d);
+   free(hash);
+}
+
+drm_private struct util_hash_iter
+util_hash_find(struct util_hash *hash, unsigned key)
+{
+   struct util_node **nextNode = util_hash_find_node(hash, key);
+   struct util_hash_iter iter = {hash, *nextNode};
+   return iter;
+}
+
+drm_private unsigned util_hash_iter_key(struct util_hash_iter iter)
+{
+   if (!iter.node || iter.hash->data.e == iter.node)
+      return 0;
+   return iter.node->key;
+}
+
+drm_private void *util_hash_iter_data(struct util_hash_iter iter)
+{
+   if (!iter.node || iter.hash->data.e == iter.node)
+      return 0;
+   return iter.node->value;
+}
+
+static struct util_node *util_hash_data_next(struct util_node *node)
+{
+   union {
+      struct util_node *next;
+      struct util_node *e;
+      struct util_hash_data *d;
+   } a;
+   int start;
+   struct util_node **bucket;
+   int n;
+
+   a.next = node->next;
+   if (!a.next) {
+      /* iterating beyond the last element */
+      return 0;
+   }
+   if (a.next->next)
+      return a.next;
+
+   start = (node->key % a.d->numBuckets) + 1;
+   bucket = a.d->buckets + start;
+   n = a.d->numBuckets - start;
+   while (n--) {
+      if (*bucket != a.e)
+         return *bucket;
+      ++bucket;
+   }
+   return a.e;
+}
+
+drm_private struct util_hash_iter
+util_hash_iter_next(struct util_hash_iter iter)
+{
+   struct util_hash_iter next = {iter.hash, util_hash_data_next(iter.node)};
+   return next;
+}
+
+drm_private int util_hash_iter_is_null(struct util_hash_iter iter)
+{
+   if (!iter.node || iter.node == iter.hash->data.e)
+      return 1;
+   return 0;
+}
+
+drm_private void *util_hash_take(struct util_hash *hash, unsigned akey)
+{
+   struct util_node **node = util_hash_find_node(hash, akey);
+   if (*node != hash->data.e) {
+      void *t = (*node)->value;
+      struct util_node *next = (*node)->next;
+      util_free_node(*node);
+      *node = next;
+      --hash->data.d->size;
+      util_data_has_shrunk(hash->data.d);
+      return t;
+   }
+   return 0;
+}
+
+drm_private struct util_hash_iter util_hash_first_node(struct util_hash *hash)
+{
+   struct util_hash_iter iter = {hash, util_data_first_node(hash->data.d)};
+   return iter;
+}
+
+drm_private struct util_hash_iter
+util_hash_erase(struct util_hash *hash, struct util_hash_iter iter)
+{
+   struct util_hash_iter ret = iter;
+   struct util_node *node = iter.node;
+   struct util_node **node_ptr;
+
+   if (node == hash->data.e)
+      return iter;
+
+   ret = util_hash_iter_next(ret);
+   node_ptr = (struct util_node**)(&hash->data.d->buckets[node->key % hash->data.d->numBuckets]);
+   while (*node_ptr != node)
+      node_ptr = &(*node_ptr)->next;
+   *node_ptr = node->next;
+   util_free_node(node);
+   --hash->data.d->size;
+   return ret;
+}
diff --git a/gsgpu/util_hash.h b/gsgpu/util_hash.h
new file mode 100644
index 00000000..6c30be41
--- /dev/null
+++ b/gsgpu/util_hash.h
@@ -0,0 +1,107 @@
+/**************************************************************************
+ *
+ * Copyright 2007 VMware, Inc.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
+ * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.
+ * IN NO EVENT SHALL VMWARE AND/OR ITS SUPPLIERS BE LIABLE FOR
+ * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ **************************************************************************/
+
+/**
+ * @file
+ * Hash implementation.
+ *
+ * This file provides a hash implementation that is capable of dealing
+ * with collisions. It stores colliding entries in linked list. All
+ * functions operating on the hash return an iterator. The iterator
+ * itself points to the collision list. If there wasn't any collision
+ * the list will have just one entry, otherwise client code should
+ * iterate over the entries to find the exact entry among ones that
+ * had the same key (e.g. memcmp could be used on the data to check
+ * that)
+ *
+ * @author Zack Rusin <zackr@vmware.com>
+ */
+
+#ifndef UTIL_HASH_H
+#define UTIL_HASH_H
+
+#ifdef HAVE_CONFIG_H
+#include "config.h"
+#endif
+
+#include <stdbool.h>
+
+#include "libdrm_macros.h"
+
+struct util_hash;
+struct util_node;
+
+struct util_hash_iter {
+	struct util_hash *hash;
+	struct util_node *node;
+};
+
+
+drm_private struct util_hash *util_hash_create(void);
+drm_private void util_hash_delete(struct util_hash *hash);
+
+
+/**
+ * Adds a data with the given key to the hash. If entry with the given
+ * key is already in the hash, this current entry is instered before it
+ * in the collision list.
+ * Function returns iterator pointing to the inserted item in the hash.
+ */
+drm_private struct util_hash_iter
+util_hash_insert(struct util_hash *hash, unsigned key, void *data);
+
+/**
+ * Removes the item pointed to by the current iterator from the hash.
+ * Note that the data itself is not erased and if it was a malloc'ed pointer
+ * it will have to be freed after calling this function by the callee.
+ * Function returns iterator pointing to the item after the removed one in
+ * the hash.
+ */
+drm_private struct util_hash_iter
+util_hash_erase(struct util_hash *hash, struct util_hash_iter iter);
+
+drm_private void *util_hash_take(struct util_hash *hash, unsigned key);
+
+
+drm_private struct util_hash_iter util_hash_first_node(struct util_hash *hash);
+
+/**
+ * Return an iterator pointing to the first entry in the collision list.
+ */
+drm_private struct util_hash_iter
+util_hash_find(struct util_hash *hash, unsigned key);
+
+
+drm_private int util_hash_iter_is_null(struct util_hash_iter iter);
+drm_private unsigned util_hash_iter_key(struct util_hash_iter iter);
+drm_private void *util_hash_iter_data(struct util_hash_iter iter);
+
+
+drm_private struct util_hash_iter
+util_hash_iter_next(struct util_hash_iter iter);
+
+#endif
diff --git a/gsgpu/util_hash_table.c b/gsgpu/util_hash_table.c
new file mode 100644
index 00000000..bfb45e95
--- /dev/null
+++ b/gsgpu/util_hash_table.c
@@ -0,0 +1,262 @@
+/**************************************************************************
+ *
+ * Copyright 2008 VMware, Inc.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
+ * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.
+ * IN NO EVENT SHALL VMWARE AND/OR ITS SUPPLIERS BE LIABLE FOR
+ * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ **************************************************************************/
+
+/**
+ * @file
+ * General purpose hash table implementation.
+ *
+ * Just uses the util_hash for now, but it might be better switch to a linear
+ * probing hash table implementation at some point -- as it is said they have
+ * better lookup and cache performance and it appears to be possible to write
+ * a lock-free implementation of such hash tables .
+ *
+ * @author José Fonseca <jfonseca@vmware.com>
+ */
+
+
+#ifdef HAVE_CONFIG_H
+#include "config.h"
+#endif
+
+#include "util_hash_table.h"
+#include "util_hash.h"
+
+#include <stdlib.h>
+#include <assert.h>
+
+struct util_hash_table
+{
+	struct util_hash *head;
+
+	/** Hash function */
+	unsigned (*make_hash)(void *key);
+
+	/** Compare two keys */
+	int (*compare)(void *key1, void *key2);
+};
+
+struct util_hash_table_item
+{
+	void *key;
+	void *value;
+};
+
+
+static struct util_hash_table_item *
+util_hash_table_item(struct util_hash_iter iter)
+{
+	return (struct util_hash_table_item *)util_hash_iter_data(iter);
+}
+
+drm_private struct util_hash_table *
+util_hash_table_create(unsigned (*hash)(void *key),
+		       int (*compare)(void *key1, void *key2))
+{
+	struct util_hash_table *ht;
+
+	ht = malloc(sizeof(struct util_hash_table));
+	if(!ht)
+		return NULL;
+
+	ht->head = util_hash_create();
+	if(!ht->head) {
+		free(ht);
+		return NULL;
+	}
+
+	ht->make_hash = hash;
+	ht->compare = compare;
+
+	return ht;
+}
+
+static struct util_hash_iter
+util_hash_table_find_iter(struct util_hash_table *ht,
+			  void *key, unsigned key_hash)
+{
+	struct util_hash_iter iter;
+	struct util_hash_table_item *item;
+
+	iter = util_hash_find(ht->head, key_hash);
+	while (!util_hash_iter_is_null(iter)) {
+		item = (struct util_hash_table_item *)util_hash_iter_data(iter);
+		if (!ht->compare(item->key, key))
+			break;
+		iter = util_hash_iter_next(iter);
+	}
+
+	return iter;
+}
+
+static struct util_hash_table_item *
+util_hash_table_find_item(struct util_hash_table *ht,
+                          void *key, unsigned key_hash)
+{
+	struct util_hash_iter iter;
+	struct util_hash_table_item *item;
+
+	iter = util_hash_find(ht->head, key_hash);
+	while (!util_hash_iter_is_null(iter)) {
+		item = (struct util_hash_table_item *)util_hash_iter_data(iter);
+		if (!ht->compare(item->key, key))
+			return item;
+		iter = util_hash_iter_next(iter);
+	}
+
+	return NULL;
+}
+
+drm_private void
+util_hash_table_set(struct util_hash_table *ht, void *key, void *value)
+{
+	unsigned key_hash;
+	struct util_hash_table_item *item;
+	struct util_hash_iter iter;
+
+	assert(ht);
+	if (!ht)
+		return;
+
+	key_hash = ht->make_hash(key);
+
+	item = util_hash_table_find_item(ht, key, key_hash);
+	if(item) {
+		/* TODO: key/value destruction? */
+		item->value = value;
+		return;
+	}
+
+	item = malloc(sizeof(struct util_hash_table_item));
+	if(!item)
+		return;
+
+	item->key = key;
+	item->value = value;
+
+	iter = util_hash_insert(ht->head, key_hash, item);
+	if(util_hash_iter_is_null(iter)) {
+		free(item);
+		return;
+	}
+}
+
+drm_private void *util_hash_table_get(struct util_hash_table *ht, void *key)
+{
+	unsigned key_hash;
+	struct util_hash_table_item *item;
+
+	assert(ht);
+	if (!ht)
+		return NULL;
+
+	key_hash = ht->make_hash(key);
+
+	item = util_hash_table_find_item(ht, key, key_hash);
+	if(!item)
+		return NULL;
+
+	return item->value;
+}
+
+drm_private void util_hash_table_remove(struct util_hash_table *ht, void *key)
+{
+	unsigned key_hash;
+	struct util_hash_iter iter;
+	struct util_hash_table_item *item;
+
+	assert(ht);
+	if (!ht)
+		return;
+
+	key_hash = ht->make_hash(key);
+
+	iter = util_hash_table_find_iter(ht, key, key_hash);
+	if(util_hash_iter_is_null(iter))
+		return;
+
+	item = util_hash_table_item(iter);
+	assert(item);
+	free(item);
+
+	util_hash_erase(ht->head, iter);
+}
+
+drm_private void util_hash_table_clear(struct util_hash_table *ht)
+{
+	struct util_hash_iter iter;
+	struct util_hash_table_item *item;
+
+	assert(ht);
+	if (!ht)
+		return;
+
+	iter = util_hash_first_node(ht->head);
+	while (!util_hash_iter_is_null(iter)) {
+		item = (struct util_hash_table_item *)util_hash_take(ht->head, util_hash_iter_key(iter));
+		free(item);
+		iter = util_hash_first_node(ht->head);
+	}
+}
+
+drm_private void util_hash_table_foreach(struct util_hash_table *ht,
+			void (*callback)(void *key, void *value, void *data),
+			void *data)
+{
+	struct util_hash_iter iter;
+	struct util_hash_table_item *item;
+
+	assert(ht);
+	if (!ht)
+		return;
+
+	iter = util_hash_first_node(ht->head);
+	while (!util_hash_iter_is_null(iter)) {
+		item = (struct util_hash_table_item *)util_hash_iter_data(iter);
+		callback(item->key, item->value, data);
+		iter = util_hash_iter_next(iter);
+	}
+}
+
+drm_private void util_hash_table_destroy(struct util_hash_table *ht)
+{
+	struct util_hash_iter iter;
+	struct util_hash_table_item *item;
+
+	assert(ht);
+	if (!ht)
+		return;
+
+	iter = util_hash_first_node(ht->head);
+	while (!util_hash_iter_is_null(iter)) {
+		item = (struct util_hash_table_item *)util_hash_iter_data(iter);
+		free(item);
+		iter = util_hash_iter_next(iter);
+	}
+
+	util_hash_delete(ht->head);
+	free(ht);
+}
diff --git a/gsgpu/util_hash_table.h b/gsgpu/util_hash_table.h
new file mode 100644
index 00000000..fdc751e4
--- /dev/null
+++ b/gsgpu/util_hash_table.h
@@ -0,0 +1,73 @@
+/**************************************************************************
+ *
+ * Copyright 2008 VMware, Inc.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
+ * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.
+ * IN NO EVENT SHALL VMWARE AND/OR ITS SUPPLIERS BE LIABLE FOR
+ * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ **************************************************************************/
+
+/**
+ * General purpose hash table.
+ *
+ * @author José Fonseca <jfonseca@vmware.com>
+ */
+
+#ifndef U_HASH_TABLE_H_
+#define U_HASH_TABLE_H_
+
+#ifdef HAVE_CONFIG_H
+#include "config.h"
+#endif
+
+#include "libdrm_macros.h"
+
+/**
+ * Generic purpose hash table.
+ */
+struct util_hash_table;
+
+/**
+ * Create an hash table.
+ *
+ * @param hash hash function
+ * @param compare should return 0 for two equal keys.
+ */
+drm_private struct util_hash_table *
+util_hash_table_create(unsigned (*hash)(void *key),
+		       int (*compare)(void *key1, void *key2));
+
+drm_private void
+util_hash_table_set(struct util_hash_table *ht, void *key, void *value);
+
+drm_private void *util_hash_table_get(struct util_hash_table *ht, void *key);
+
+drm_private void util_hash_table_remove(struct util_hash_table *ht, void *key);
+
+drm_private void util_hash_table_clear(struct util_hash_table *ht);
+
+drm_private void util_hash_table_foreach(struct util_hash_table *ht,
+			void (*callback)(void *key, void *value, void *data),
+			void *data);
+
+drm_private void util_hash_table_destroy(struct util_hash_table *ht);
+
+#endif /* U_HASH_TABLE_H_ */
diff --git a/include/drm/gsgpu_drm.h b/include/drm/gsgpu_drm.h
new file mode 100644
index 00000000..a08ca6e3
--- /dev/null
+++ b/include/drm/gsgpu_drm.h
@@ -0,0 +1,935 @@
+/* gsgpu_drm.h -- Public header for the gsgpu driver -*- linux-c -*-
+ *
+ * Copyright (C) 2021, Loongson Technology Corporation Limited, Inc
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#ifndef __GSGPU_DRM_H__
+#define __GSGPU_DRM_H__
+
+#include "drm.h"
+
+#if defined(__cplusplus)
+extern "C" {
+#endif
+
+#define DRM_GSGPU_GEM_CREATE		0x00
+#define DRM_GSGPU_GEM_MMAP		0x01
+#define DRM_GSGPU_CTX			0x02
+#define DRM_GSGPU_BO_LIST		0x03
+#define DRM_GSGPU_CS			0x04
+#define DRM_GSGPU_INFO			0x05
+#define DRM_GSGPU_GEM_METADATA		0x06
+#define DRM_GSGPU_GEM_WAIT_IDLE		0x07
+#define DRM_GSGPU_GEM_VA		0x08
+#define DRM_GSGPU_WAIT_CS		0x09
+#define DRM_GSGPU_GEM_OP		0x10
+#define DRM_GSGPU_GEM_USERPTR		0x11
+#define DRM_GSGPU_WAIT_FENCES		0x12
+#define DRM_GSGPU_VM			0x13
+#define DRM_GSGPU_FENCE_TO_HANDLE	0x14
+#define DRM_GSGPU_SCHED			0x15
+#define DRM_GSGPU_HWSEMA_OP		0x16
+
+#define DRM_IOCTL_GSGPU_GEM_CREATE	DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_GEM_CREATE, union drm_gsgpu_gem_create)
+#define DRM_IOCTL_GSGPU_GEM_MMAP	DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_GEM_MMAP, union drm_gsgpu_gem_mmap)
+#define DRM_IOCTL_GSGPU_CTX		DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_CTX, union drm_gsgpu_ctx)
+#define DRM_IOCTL_GSGPU_BO_LIST		DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_BO_LIST, union drm_gsgpu_bo_list)
+#define DRM_IOCTL_GSGPU_CS		DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_CS, union drm_gsgpu_cs)
+#define DRM_IOCTL_GSGPU_INFO		DRM_IOW(DRM_COMMAND_BASE + DRM_GSGPU_INFO, struct drm_gsgpu_info)
+#define DRM_IOCTL_GSGPU_GEM_METADATA	DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_GEM_METADATA, struct drm_gsgpu_gem_metadata)
+#define DRM_IOCTL_GSGPU_GEM_WAIT_IDLE	DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_GEM_WAIT_IDLE, union drm_gsgpu_gem_wait_idle)
+#define DRM_IOCTL_GSGPU_GEM_VA		DRM_IOW(DRM_COMMAND_BASE + DRM_GSGPU_GEM_VA, struct drm_gsgpu_gem_va)
+#define DRM_IOCTL_GSGPU_WAIT_CS		DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_WAIT_CS, union drm_gsgpu_wait_cs)
+#define DRM_IOCTL_GSGPU_GEM_OP		DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_GEM_OP, struct drm_gsgpu_gem_op)
+#define DRM_IOCTL_GSGPU_GEM_USERPTR	DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_GEM_USERPTR, struct drm_gsgpu_gem_userptr)
+#define DRM_IOCTL_GSGPU_WAIT_FENCES	DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_WAIT_FENCES, union drm_gsgpu_wait_fences)
+#define DRM_IOCTL_GSGPU_VM		DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_VM, union drm_gsgpu_vm)
+#define DRM_IOCTL_GSGPU_FENCE_TO_HANDLE DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_FENCE_TO_HANDLE, union drm_gsgpu_fence_to_handle)
+#define DRM_IOCTL_GSGPU_SCHED		DRM_IOW(DRM_COMMAND_BASE + DRM_GSGPU_SCHED, union drm_gsgpu_sched)
+#define DRM_IOCTL_GSGPU_HWSEMA_OP       DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_HWSEMA_OP, struct drm_gsgpu_hw_sema)
+
+#define GSGPU_GEM_DOMAIN_CPU		0x1
+#define GSGPU_GEM_DOMAIN_GTT		0x2
+#define GSGPU_GEM_DOMAIN_VRAM		0x4
+#define GSGPU_GEM_DOMAIN_GDS		0x8
+#define GSGPU_GEM_DOMAIN_GWS		0x10
+#define GSGPU_GEM_DOMAIN_OA		0x20
+
+/* Flag that CPU access will be required for the case of VRAM domain */
+#define GSGPU_GEM_CREATE_CPU_ACCESS_REQUIRED	(1 << 0)
+/* Flag that CPU access will not work, this VRAM domain is invisible */
+#define GSGPU_GEM_CREATE_NO_CPU_ACCESS		(1 << 1)
+/* Flag that USWC attributes should be used for GTT */
+#define GSGPU_GEM_CREATE_CPU_GTT_USWC		(1 << 2)
+/* Flag that the memory should be in VRAM and cleared */
+#define GSGPU_GEM_CREATE_VRAM_CLEARED		(1 << 3)
+/* Flag that create shadow bo(GTT) while allocating vram bo */
+#define GSGPU_GEM_CREATE_SHADOW			(1 << 4)
+/* Flag that allocating the BO should use linear VRAM */
+#define GSGPU_GEM_CREATE_VRAM_CONTIGUOUS	(1 << 5)
+/* Flag that BO is always valid in this VM */
+#define GSGPU_GEM_CREATE_VM_ALWAYS_VALID	(1 << 6)
+/* Flag that BO sharing will be explicitly synchronized */
+#define GSGPU_GEM_CREATE_EXPLICIT_SYNC		(1 << 7)
+
+struct drm_gsgpu_gem_create_in  {
+	/** the requested memory size */
+	__u64 bo_size;
+	/** physical start_addr alignment in bytes for some HW requirements */
+	__u64 alignment;
+	/** the requested memory domains */
+	__u64 domains;
+	/** allocation flags */
+	__u64 domain_flags;
+};
+
+struct drm_gsgpu_gem_create_out  {
+	/** returned GEM object handle */
+	__u32 handle;
+	__u32 _pad;
+};
+
+union drm_gsgpu_gem_create {
+	struct drm_gsgpu_gem_create_in		in;
+	struct drm_gsgpu_gem_create_out	out;
+};
+
+/** Opcode to create new residency list.  */
+#define GSGPU_BO_LIST_OP_CREATE	0
+/** Opcode to destroy previously created residency list */
+#define GSGPU_BO_LIST_OP_DESTROY	1
+/** Opcode to update resource information in the list */
+#define GSGPU_BO_LIST_OP_UPDATE	2
+
+struct drm_gsgpu_bo_list_in {
+	/** Type of operation */
+	__u32 operation;
+	/** Handle of list or 0 if we want to create one */
+	__u32 list_handle;
+	/** Number of BOs in list  */
+	__u32 bo_number;
+	/** Size of each element describing BO */
+	__u32 bo_info_size;
+	/** Pointer to array describing BOs */
+	__u64 bo_info_ptr;
+};
+
+struct drm_gsgpu_bo_list_entry {
+	/** Handle of BO */
+	__u32 bo_handle;
+	/** New (if specified) BO priority to be used during migration */
+	__u32 bo_priority;
+};
+
+struct drm_gsgpu_bo_list_out {
+	/** Handle of resource list  */
+	__u32 list_handle;
+	__u32 _pad;
+};
+
+union drm_gsgpu_bo_list {
+	struct drm_gsgpu_bo_list_in in;
+	struct drm_gsgpu_bo_list_out out;
+};
+
+/* context related */
+#define GSGPU_CTX_OP_ALLOC_CTX		1
+#define GSGPU_CTX_OP_FREE_CTX		2
+#define GSGPU_CTX_OP_QUERY_STATE	3
+
+/* GPU reset status */
+#define GSGPU_CTX_NO_RESET		0
+/* this the context caused it */
+#define GSGPU_CTX_GUILTY_RESET		1
+/* some other context caused it */
+#define GSGPU_CTX_INNOCENT_RESET	2
+/* unknown cause */
+#define GSGPU_CTX_UNKNOWN_RESET		3
+
+/* Context priority level */
+#define GSGPU_CTX_PRIORITY_UNSET       -2048
+#define GSGPU_CTX_PRIORITY_VERY_LOW    -1023
+#define GSGPU_CTX_PRIORITY_LOW         -512
+#define GSGPU_CTX_PRIORITY_NORMAL      0
+/* Selecting a priority above NORMAL requires CAP_SYS_NICE or DRM_MASTER */
+#define GSGPU_CTX_PRIORITY_HIGH        512
+#define GSGPU_CTX_PRIORITY_VERY_HIGH   1023
+
+struct drm_gsgpu_ctx_in {
+	/** GSGPU_CTX_OP_* */
+	__u32	op;
+	/** For future use, no flags defined so far */
+	__u32	flags;
+	__u32	ctx_id;
+	__s32	priority;
+};
+
+union drm_gsgpu_ctx_out {
+		struct {
+			__u32	ctx_id;
+			__u32	_pad;
+		} alloc;
+
+		struct {
+			/** For future use, no flags defined so far */
+			__u64	flags;
+			/** Number of resets caused by this context so far. */
+			__u32	hangs;
+			/** Reset status since the last call of the ioctl. */
+			__u32	reset_status;
+		} state;
+};
+
+union drm_gsgpu_ctx {
+	struct drm_gsgpu_ctx_in in;
+	union drm_gsgpu_ctx_out out;
+};
+
+/* vm ioctl */
+#define GSGPU_VM_OP_RESERVE_VMID	1
+#define GSGPU_VM_OP_UNRESERVE_VMID	2
+
+struct drm_gsgpu_vm_in {
+	/** GSGPU_VM_OP_* */
+	__u32	op;
+	__u32	flags;
+};
+
+struct drm_gsgpu_vm_out {
+	/** For future use, no flags defined so far */
+	__u64	flags;
+};
+
+union drm_gsgpu_vm {
+	struct drm_gsgpu_vm_in in;
+	struct drm_gsgpu_vm_out out;
+};
+
+/* sched ioctl */
+#define GSGPU_SCHED_OP_PROCESS_PRIORITY_OVERRIDE	1
+
+struct drm_gsgpu_sched_in {
+	/* GSGPU_SCHED_OP_* */
+	__u32	op;
+	__u32	fd;
+	__s32	priority;
+	__u32	flags;
+};
+
+union drm_gsgpu_sched {
+	struct drm_gsgpu_sched_in in;
+};
+
+/*
+ * This is not a reliable API and you should expect it to fail for any
+ * number of reasons and have fallback path that do not use userptr to
+ * perform any operation.
+ */
+#define GSGPU_GEM_USERPTR_READONLY	(1 << 0)
+#define GSGPU_GEM_USERPTR_ANONONLY	(1 << 1)
+#define GSGPU_GEM_USERPTR_VALIDATE	(1 << 2)
+#define GSGPU_GEM_USERPTR_REGISTER	(1 << 3)
+
+struct drm_gsgpu_gem_userptr {
+	__u64		addr;
+	__u64		size;
+	/* GSGPU_GEM_USERPTR_* */
+	__u32		flags;
+	/* Resulting GEM handle */
+	__u32		handle;
+};
+
+/* SI-CI-VI: */
+/* same meaning as the GB_TILE_MODE and GL_MACRO_TILE_MODE fields */
+#define GSGPU_TILING_ARRAY_MODE_SHIFT			0
+#define GSGPU_TILING_ARRAY_MODE_MASK			0xf
+#define GSGPU_TILING_PIPE_CONFIG_SHIFT			4
+#define GSGPU_TILING_PIPE_CONFIG_MASK			0x1f
+#define GSGPU_TILING_TILE_SPLIT_SHIFT			9
+#define GSGPU_TILING_TILE_SPLIT_MASK			0x7
+#define GSGPU_TILING_MICRO_TILE_MODE_SHIFT		12
+#define GSGPU_TILING_MICRO_TILE_MODE_MASK		0x7
+#define GSGPU_TILING_BANK_WIDTH_SHIFT			15
+#define GSGPU_TILING_BANK_WIDTH_MASK			0x3
+#define GSGPU_TILING_BANK_HEIGHT_SHIFT			17
+#define GSGPU_TILING_BANK_HEIGHT_MASK			0x3
+#define GSGPU_TILING_MACRO_TILE_ASPECT_SHIFT		19
+#define GSGPU_TILING_MACRO_TILE_ASPECT_MASK		0x3
+#define GSGPU_TILING_NUM_BANKS_SHIFT			21
+#define GSGPU_TILING_NUM_BANKS_MASK			0x3
+
+/* GFX9 and later: */
+#define GSGPU_TILING_SWIZZLE_MODE_SHIFT			0
+#define GSGPU_TILING_SWIZZLE_MODE_MASK			0x1f
+
+/* Set/Get helpers for tiling flags. */
+#define GSGPU_TILING_SET(field, value) \
+	(((__u64)(value) & GSGPU_TILING_##field##_MASK) << GSGPU_TILING_##field##_SHIFT)
+#define GSGPU_TILING_GET(value, field) \
+	(((__u64)(value) >> GSGPU_TILING_##field##_SHIFT) & GSGPU_TILING_##field##_MASK)
+
+#define GSGPU_GEM_METADATA_OP_SET_METADATA                  1
+#define GSGPU_GEM_METADATA_OP_GET_METADATA                  2
+
+/** The same structure is shared for input/output */
+struct drm_gsgpu_gem_metadata {
+	/** GEM Object handle */
+	__u32	handle;
+	/** Do we want get or set metadata */
+	__u32	op;
+	struct {
+		/** For future use, no flags defined so far */
+		__u64	flags;
+		/** family specific tiling info */
+		__u64	tiling_info;
+		__u32	data_size_bytes;
+		__u32	data[64];
+	} data;
+};
+
+struct drm_gsgpu_gem_mmap_in {
+	/** the GEM object handle */
+	__u32 handle;
+	__u32 _pad;
+};
+
+struct drm_gsgpu_gem_mmap_out {
+	/** mmap offset from the vma offset manager */
+	__u64 addr_ptr;
+};
+
+union drm_gsgpu_gem_mmap {
+	struct drm_gsgpu_gem_mmap_in   in;
+	struct drm_gsgpu_gem_mmap_out out;
+};
+
+struct drm_gsgpu_gem_wait_idle_in {
+	/** GEM object handle */
+	__u32 handle;
+	/** For future use, no flags defined so far */
+	__u32 flags;
+	/** Absolute timeout to wait */
+	__u64 timeout;
+};
+
+struct drm_gsgpu_gem_wait_idle_out {
+	/** BO status:  0 - BO is idle, 1 - BO is busy */
+	__u32 status;
+	/** Returned current memory domain */
+	__u32 domain;
+};
+
+union drm_gsgpu_gem_wait_idle {
+	struct drm_gsgpu_gem_wait_idle_in  in;
+	struct drm_gsgpu_gem_wait_idle_out out;
+};
+
+struct drm_gsgpu_wait_cs_in {
+	/* Command submission handle
+         * handle equals 0 means none to wait for
+         * handle equals ~0ull means wait for the latest sequence number
+         */
+	__u64 handle;
+	/** Absolute timeout to wait */
+	__u64 timeout;
+	__u32 ip_type;
+	__u32 ip_instance;
+	__u32 ring;
+	__u32 ctx_id;
+};
+
+struct drm_gsgpu_wait_cs_out {
+	/** CS status:  0 - CS completed, 1 - CS still busy */
+	__u64 status;
+};
+
+union drm_gsgpu_wait_cs {
+	struct drm_gsgpu_wait_cs_in in;
+	struct drm_gsgpu_wait_cs_out out;
+};
+
+struct drm_gsgpu_fence {
+	__u32 ctx_id;
+	__u32 ip_type;
+	__u32 ip_instance;
+	__u32 ring;
+	__u64 seq_no;
+};
+
+struct drm_gsgpu_wait_fences_in {
+	/** This points to uint64_t * which points to fences */
+	__u64 fences;
+	__u32 fence_count;
+	__u32 wait_all;
+	__u64 timeout_ns;
+};
+
+struct drm_gsgpu_wait_fences_out {
+	__u32 status;
+	__u32 first_signaled;
+};
+
+union drm_gsgpu_wait_fences {
+	struct drm_gsgpu_wait_fences_in in;
+	struct drm_gsgpu_wait_fences_out out;
+};
+
+#define GSGPU_GEM_OP_GET_GEM_CREATE_INFO	0
+#define GSGPU_GEM_OP_SET_PLACEMENT		1
+
+/* Sets or returns a value associated with a buffer. */
+struct drm_gsgpu_gem_op {
+	/** GEM object handle */
+	__u32	handle;
+	/** GSGPU_GEM_OP_* */
+	__u32	op;
+	/** Input or return value */
+	__u64	value;
+};
+
+#define GSGPU_VA_OP_MAP			1
+#define GSGPU_VA_OP_UNMAP			2
+#define GSGPU_VA_OP_CLEAR			3
+#define GSGPU_VA_OP_REPLACE			4
+
+/* Delay the page table update till the next CS */
+#define GSGPU_VM_DELAY_UPDATE		(1 << 0)
+
+/* Mapping flags */
+/* readable mapping */
+#define GSGPU_VM_PAGE_READABLE		(1 << 1)
+/* writable mapping */
+#define GSGPU_VM_PAGE_WRITEABLE	(1 << 2)
+/* executable mapping, new for VI */
+#define GSGPU_VM_PAGE_EXECUTABLE	(1 << 3)
+/* partially resident texture */
+#define GSGPU_VM_PAGE_PRT		(1 << 4)
+/* MTYPE flags use bit 5 to 8 */
+#define GSGPU_VM_MTYPE_MASK		(0xf << 5)
+/* Default MTYPE. Pre-AI must use this.  Recommended for newer ASICs. */
+#define GSGPU_VM_MTYPE_DEFAULT		(0 << 5)
+/* Use NC MTYPE instead of default MTYPE */
+#define GSGPU_VM_MTYPE_NC		(1 << 5)
+/* Use WC MTYPE instead of default MTYPE */
+#define GSGPU_VM_MTYPE_WC		(2 << 5)
+/* Use CC MTYPE instead of default MTYPE */
+#define GSGPU_VM_MTYPE_CC		(3 << 5)
+/* Use UC MTYPE instead of default MTYPE */
+#define GSGPU_VM_MTYPE_UC		(4 << 5)
+
+struct drm_gsgpu_gem_va {
+	/** GEM object handle */
+	__u32 handle;
+	__u32 _pad;
+	/** GSGPU_VA_OP_* */
+	__u32 operation;
+	/** GSGPU_VM_PAGE_* */
+	__u32 flags;
+	/** va address to assign . Must be correctly aligned.*/
+	__u64 va_address;
+	/** Specify offset inside of BO to assign. Must be correctly aligned.*/
+	__u64 offset_in_bo;
+	/** Specify mapping size. Must be correctly aligned. */
+	__u64 map_size;
+};
+
+#define GSGPU_HW_IP_GFX          0
+#define GSGPU_HW_IP_COMPUTE      1
+#define GSGPU_HW_IP_DMA          2
+#define GSGPU_HW_IP_UVD          3
+#define GSGPU_HW_IP_VCE          4
+#define GSGPU_HW_IP_UVD_ENC      5
+#define GSGPU_HW_IP_VCN_DEC      6
+#define GSGPU_HW_IP_VCN_ENC      7
+#define GSGPU_HW_IP_NUM          8
+
+#define GSGPU_HW_IP_INSTANCE_MAX_COUNT 1
+
+#define GSGPU_CHUNK_ID_IB		0x01
+#define GSGPU_CHUNK_ID_FENCE		0x02
+#define GSGPU_CHUNK_ID_DEPENDENCIES	0x03
+#define GSGPU_CHUNK_ID_SYNCOBJ_IN      	0x04
+#define GSGPU_CHUNK_ID_SYNCOBJ_OUT     	0x05
+
+struct drm_gsgpu_cs_chunk {
+	__u32		chunk_id;
+	__u32		length_dw;
+	__u64		chunk_data;
+};
+
+struct drm_gsgpu_cs_in {
+	/** Rendering context id */
+	__u32		ctx_id;
+	/**  Handle of resource list associated with CS */
+	__u32		bo_list_handle;
+	__u32		num_chunks;
+	__u32		_pad;
+	/** this points to __u64 * which point to cs chunks */
+	__u64		chunks;
+};
+
+struct drm_gsgpu_cs_out {
+	__u64 handle;
+};
+
+union drm_gsgpu_cs {
+	struct drm_gsgpu_cs_in in;
+	struct drm_gsgpu_cs_out out;
+};
+
+/* Specify flags to be used for IB */
+
+/* This IB should be submitted to CE */
+#define GSGPU_IB_FLAG_CE	(1<<0)
+
+/* Preamble flag, which means the IB could be dropped if no context switch */
+#define GSGPU_IB_FLAG_PREAMBLE (1<<1)
+
+/* Preempt flag, IB should set Pre_enb bit if PREEMPT flag detected */
+#define GSGPU_IB_FLAG_PREEMPT (1<<2)
+
+struct drm_gsgpu_cs_chunk_ib {
+	__u32 _pad;
+	/** GSGPU_IB_FLAG_* */
+	__u32 flags;
+	/** Virtual address to begin IB execution */
+	__u64 va_start;
+	/** Size of submission */
+	__u32 ib_bytes;
+	/** HW IP to submit to */
+	__u32 ip_type;
+	/** HW IP index of the same type to submit to  */
+	__u32 ip_instance;
+	/** Ring index to submit to */
+	__u32 ring;
+};
+
+struct drm_gsgpu_cs_chunk_dep {
+	__u32 ip_type;
+	__u32 ip_instance;
+	__u32 ring;
+	__u32 ctx_id;
+	__u64 handle;
+};
+
+struct drm_gsgpu_cs_chunk_fence {
+	__u32 handle;
+	__u32 offset;
+};
+
+struct drm_gsgpu_cs_chunk_sem {
+	__u32 handle;
+};
+
+#define GSGPU_FENCE_TO_HANDLE_GET_SYNCOBJ	0
+#define GSGPU_FENCE_TO_HANDLE_GET_SYNCOBJ_FD	1
+#define GSGPU_FENCE_TO_HANDLE_GET_SYNC_FILE_FD	2
+
+union drm_gsgpu_fence_to_handle {
+	struct {
+		struct drm_gsgpu_fence fence;
+		__u32 what;
+		__u32 pad;
+	} in;
+	struct {
+		__u32 handle;
+	} out;
+};
+
+struct drm_gsgpu_cs_chunk_data {
+	union {
+		struct drm_gsgpu_cs_chunk_ib		ib_data;
+		struct drm_gsgpu_cs_chunk_fence	fence_data;
+	};
+};
+
+/**
+ *  Query h/w info: Flag that this is integrated (a.h.a. fusion) GPU
+ *
+ */
+#define GSGPU_IDS_FLAGS_FUSION         0x1
+#define GSGPU_IDS_FLAGS_PREEMPTION     0x2
+
+/* indicate if acceleration can be working */
+#define GSGPU_INFO_ACCEL_WORKING		0x00
+/* get the crtc_id from the mode object id? */
+#define GSGPU_INFO_CRTC_FROM_ID		0x01
+/* query hw IP info */
+#define GSGPU_INFO_HW_IP_INFO			0x02
+/* query hw IP instance count for the specified type */
+#define GSGPU_INFO_HW_IP_COUNT			0x03
+/* timestamp for GL_ARB_timer_query */
+#define GSGPU_INFO_TIMESTAMP			0x05
+/* Query the firmware version */
+#define GSGPU_INFO_FW_VERSION			0x0e
+	/* Subquery id: Query VCE firmware version */
+	#define GSGPU_INFO_FW_VCE		0x1
+	/* Subquery id: Query UVD firmware version */
+	#define GSGPU_INFO_FW_UVD		0x2
+	/* Subquery id: Query GMC firmware version */
+	#define GSGPU_INFO_FW_GMC		0x03
+	/* Subquery id: Query GFX ME firmware version */
+	#define GSGPU_INFO_FW_GFX_ME		0x04
+	/* Subquery id: Query GFX PFP firmware version */
+	#define GSGPU_INFO_FW_GFX_PFP		0x05
+	/* Subquery id: Query GFX CE firmware version */
+	#define GSGPU_INFO_FW_GFX_CE		0x06
+	/* Subquery id: Query GFX RLC firmware version */
+	#define GSGPU_INFO_FW_GFX_RLC		0x07
+	/* Subquery id: Query GFX MEC firmware version */
+	#define GSGPU_INFO_FW_GFX_MEC		0x08
+	/* Subquery id: Query SMC firmware version */
+	#define GSGPU_INFO_FW_SMC		0x0a
+	/* Subquery id: Query SDMA firmware version */
+	#define GSGPU_INFO_FW_SDMA		0x0b
+	/* Subquery id: Query PSP SOS firmware version */
+	#define GSGPU_INFO_FW_SOS		0x0c
+	/* Subquery id: Query PSP ASD firmware version */
+	#define GSGPU_INFO_FW_ASD		0x0d
+/* number of bytes moved for TTM migration */
+#define GSGPU_INFO_NUM_BYTES_MOVED		0x0f
+/* the used VRAM size */
+#define GSGPU_INFO_VRAM_USAGE			0x10
+/* the used GTT size */
+#define GSGPU_INFO_GTT_USAGE			0x11
+/* Information about GDS, etc. resource configuration */
+#define GSGPU_INFO_GDS_CONFIG			0x13
+/* Query information about VRAM and GTT domains */
+#define GSGPU_INFO_VRAM_GTT			0x14
+/* Query information about register in MMR address space*/
+#define GSGPU_INFO_READ_MMR_REG		0x15
+/* Query information about device: rev id, family, etc. */
+#define GSGPU_INFO_DEV_INFO			0x16
+/* visible vram usage */
+#define GSGPU_INFO_VIS_VRAM_USAGE		0x17
+/* number of TTM buffer evictions */
+#define GSGPU_INFO_NUM_EVICTIONS		0x18
+/* Query memory about VRAM and GTT domains */
+#define GSGPU_INFO_MEMORY			0x19
+/* Query vce clock table */
+#define GSGPU_INFO_VCE_CLOCK_TABLE		0x1A
+/* Query vbios related information */
+#define GSGPU_INFO_VBIOS			0x1B
+	/* Subquery id: Query vbios size */
+	#define GSGPU_INFO_VBIOS_SIZE		0x1
+	/* Subquery id: Query vbios image */
+	#define GSGPU_INFO_VBIOS_IMAGE		0x2
+/* Query UVD handles */
+#define GSGPU_INFO_NUM_HANDLES			0x1C
+/* Query sensor related information */
+#define GSGPU_INFO_SENSOR			0x1D
+	/* Subquery id: Query GPU shader clock */
+	#define GSGPU_INFO_SENSOR_GFX_SCLK		0x1
+	/* Subquery id: Query GPU memory clock */
+	#define GSGPU_INFO_SENSOR_GFX_MCLK		0x2
+	/* Subquery id: Query GPU temperature */
+	#define GSGPU_INFO_SENSOR_GPU_TEMP		0x3
+	/* Subquery id: Query GPU load */
+	#define GSGPU_INFO_SENSOR_GPU_LOAD		0x4
+	/* Subquery id: Query average GPU power	*/
+	#define GSGPU_INFO_SENSOR_GPU_AVG_POWER	0x5
+	/* Subquery id: Query northbridge voltage */
+	#define GSGPU_INFO_SENSOR_VDDNB		0x6
+	/* Subquery id: Query graphics voltage */
+	#define GSGPU_INFO_SENSOR_VDDGFX		0x7
+/* Number of VRAM page faults on CPU access. */
+#define GSGPU_INFO_NUM_VRAM_CPU_PAGE_FAULTS	0x1E
+#define GSGPU_INFO_VRAM_LOST_COUNTER		0x1F
+
+#define GSGPU_INFO_MMR_SE_INDEX_SHIFT	0
+#define GSGPU_INFO_MMR_SE_INDEX_MASK	0xff
+#define GSGPU_INFO_MMR_SH_INDEX_SHIFT	8
+#define GSGPU_INFO_MMR_SH_INDEX_MASK	0xff
+
+struct drm_gsgpu_query_fw {
+	/** GSGPU_INFO_FW_* */
+	__u32 fw_type;
+	/**
+	 * Index of the IP if there are more IPs of
+	 * the same type.
+	 */
+	__u32 ip_instance;
+	/**
+	 * Index of the engine. Whether this is used depends
+	 * on the firmware type. (e.g. MEC, SDMA)
+	 */
+	__u32 index;
+	__u32 _pad;
+};
+
+/* Input structure for the INFO ioctl */
+struct drm_gsgpu_info {
+	/* Where the return value will be stored */
+	__u64 return_pointer;
+	/* The size of the return value. Just like "size" in "snprintf",
+	 * it limits how many bytes the kernel can write. */
+	__u32 return_size;
+	/* The query request id. */
+	__u32 query;
+
+	union {
+		struct {
+			__u32 id;
+			__u32 _pad;
+		} mode_crtc;
+
+		struct {
+			/** GSGPU_HW_IP_* */
+			__u32 type;
+			/**
+			 * Index of the IP if there are more IPs of the same
+			 * type. Ignored by GSGPU_INFO_HW_IP_COUNT.
+			 */
+			__u32 ip_instance;
+		} query_hw_ip;
+
+		struct {
+			__u32 dword_offset;
+			/** number of registers to read */
+			__u32 count;
+			__u32 instance;
+			/** For future use, no flags defined so far */
+			__u32 flags;
+		} read_mmr_reg;
+
+		struct drm_gsgpu_query_fw query_fw;
+
+		struct {
+			__u32 type;
+			__u32 offset;
+		} vbios_info;
+
+		struct {
+			__u32 type;
+		} sensor_info;
+	};
+};
+
+struct drm_gsgpu_info_gds {
+	/** GDS GFX partition size */
+	__u32 gds_gfx_partition_size;
+	/** GDS compute partition size */
+	__u32 compute_partition_size;
+	/** total GDS memory size */
+	__u32 gds_total_size;
+	/** GWS size per GFX partition */
+	__u32 gws_per_gfx_partition;
+	/** GSW size per compute partition */
+	__u32 gws_per_compute_partition;
+	/** OA size per GFX partition */
+	__u32 oa_per_gfx_partition;
+	/** OA size per compute partition */
+	__u32 oa_per_compute_partition;
+	__u32 _pad;
+};
+
+struct drm_gsgpu_info_vram_gtt {
+	__u64 vram_size;
+	__u64 vram_cpu_accessible_size;
+	__u64 gtt_size;
+};
+
+struct drm_gsgpu_heap_info {
+	/** max. physical memory */
+	__u64 total_heap_size;
+
+	/** Theoretical max. available memory in the given heap */
+	__u64 usable_heap_size;
+
+	/**
+	 * Number of bytes allocated in the heap. This includes all processes
+	 * and private allocations in the kernel. It changes when new buffers
+	 * are allocated, freed, and moved. It cannot be larger than
+	 * heap_size.
+	 */
+	__u64 heap_usage;
+
+	/**
+	 * Theoretical possible max. size of buffer which
+	 * could be allocated in the given heap
+	 */
+	__u64 max_allocation;
+};
+
+struct drm_gsgpu_memory_info {
+	struct drm_gsgpu_heap_info vram;
+	struct drm_gsgpu_heap_info cpu_accessible_vram;
+	struct drm_gsgpu_heap_info gtt;
+};
+
+struct drm_gsgpu_info_firmware {
+	__u32 ver;
+	__u32 feature;
+};
+
+#define GSGPU_VRAM_TYPE_UNKNOWN 0
+#define GSGPU_VRAM_TYPE_GDDR1 1
+#define GSGPU_VRAM_TYPE_DDR2  2
+#define GSGPU_VRAM_TYPE_GDDR3 3
+#define GSGPU_VRAM_TYPE_GDDR4 4
+#define GSGPU_VRAM_TYPE_GDDR5 5
+#define GSGPU_VRAM_TYPE_HBM   6
+#define GSGPU_VRAM_TYPE_DDR3  7
+
+struct drm_gsgpu_info_device {
+	/** PCI Device ID */
+	__u32 device_id;
+	/** Internal chip revision: A0, A1, etc.) */
+	__u32 chip_rev;
+	__u32 external_rev;
+	/** Revision id in PCI Config space */
+	__u32 pci_rev;
+	__u32 family;
+	__u32 num_shader_engines;
+	__u32 num_shader_arrays_per_engine;
+	/* in KHz */
+	__u32 gpu_counter_freq;
+	__u64 max_engine_clock;
+	__u64 max_memory_clock;
+	/* cu information */
+	__u32 cu_active_number;
+	/* NOTE: cu_ao_mask is INVALID, DON'T use it */
+	__u32 cu_ao_mask;
+	__u32 cu_bitmap[4][4];
+	/** Render backend pipe mask. One render backend is CB+DB. */
+	__u32 enabled_rb_pipes_mask;
+	__u32 num_rb_pipes;
+	__u32 num_hw_gfx_contexts;
+	__u32 _pad;
+	__u64 ids_flags;
+	/** Starting virtual address for UMDs. */
+	__u64 virtual_address_offset;
+	/** The maximum virtual address */
+	__u64 virtual_address_max;
+	/** Required alignment of virtual addresses. */
+	__u32 virtual_address_alignment;
+	/** Page table entry - fragment size */
+	__u32 pte_fragment_size;
+	__u32 gart_page_size;
+	/** constant engine ram size*/
+	__u32 ce_ram_size;
+	/** video memory type info*/
+	__u32 vram_type;
+	/** video memory bit width*/
+	__u32 vram_bit_width;
+	/* vce harvesting instance */
+	__u32 vce_harvest_config;
+	/* gfx double offchip LDS buffers */
+	__u32 gc_double_offchip_lds_buf;
+	/* NGG Primitive Buffer */
+	__u64 prim_buf_gpu_addr;
+	/* NGG Position Buffer */
+	__u64 pos_buf_gpu_addr;
+	/* NGG Control Sideband */
+	__u64 cntl_sb_buf_gpu_addr;
+	/* NGG Parameter Cache */
+	__u64 param_buf_gpu_addr;
+	__u32 prim_buf_size;
+	__u32 pos_buf_size;
+	__u32 cntl_sb_buf_size;
+	__u32 param_buf_size;
+	/* wavefront size*/
+	__u32 wave_front_size;
+	/* shader visible vgprs*/
+	__u32 num_shader_visible_vgprs;
+	/* CU per shader array*/
+	__u32 num_cu_per_sh;
+	/* number of tcc blocks*/
+	__u32 num_tcc_blocks;
+	/* gs vgt table depth*/
+	__u32 gs_vgt_table_depth;
+	/* gs primitive buffer depth*/
+	__u32 gs_prim_buffer_depth;
+	/* max gs wavefront per vgt*/
+	__u32 max_gs_waves_per_vgt;
+	__u32 _pad1;
+	/* always on cu bitmap */
+	__u32 cu_ao_bitmap[4][4];
+	/** Starting high virtual address for UMDs. */
+	__u64 high_va_offset;
+	/** The maximum high virtual address */
+	__u64 high_va_max;
+};
+
+struct drm_gsgpu_info_hw_ip {
+	/** Version of h/w IP */
+	__u32  hw_ip_version_major;
+	__u32  hw_ip_version_minor;
+	/** Capabilities */
+	__u64  capabilities_flags;
+	/** command buffer address start alignment*/
+	__u32  ib_start_alignment;
+	/** command buffer size alignment*/
+	__u32  ib_size_alignment;
+	/** Bitmask of available rings. Bit 0 means ring 0, etc. */
+	__u32  available_rings;
+	__u32  _pad;
+};
+
+struct drm_gsgpu_info_num_handles {
+	/** Max handles as supported by firmware for UVD */
+	__u32  uvd_max_handles;
+	/** Handles currently in use for UVD */
+	__u32  uvd_used_handles;
+};
+
+#define GSGPU_VCE_CLOCK_TABLE_ENTRIES		6
+
+struct drm_gsgpu_info_vce_clock_table_entry {
+	/** System clock */
+	__u32 sclk;
+	/** Memory clock */
+	__u32 mclk;
+	/** VCE clock */
+	__u32 eclk;
+	__u32 pad;
+};
+
+struct drm_gsgpu_info_vce_clock_table {
+	struct drm_gsgpu_info_vce_clock_table_entry entries[GSGPU_VCE_CLOCK_TABLE_ENTRIES];
+	__u32 num_valid_entries;
+	__u32 pad;
+};
+
+#define       GSGPU_HW_SEMA_GET 	1
+#define       GSGPU_HW_SEMA_PUT 	2
+
+struct drm_gsgpu_hw_sema {
+    /*get or set sema*/
+    __u64 id;
+    /*resv for next feature*/
+    __u32 ctx_id;
+    /*ops*/
+    __u32 ops;
+};
+
+/*
+ * Supported GPU families
+ */
+#define GSGPU_FAMILY_UNKNOWN			0
+#define GSGPU_FAMILY_GS				110 /* Godson GPU first version*/
+
+#if defined(__cplusplus)
+}
+#endif
+
+#endif
diff --git a/meson.build b/meson.build
index 27e40e75..e83e5845 100644
--- a/meson.build
+++ b/meson.build
@@ -119,6 +119,12 @@ with_nouveau = get_option('nouveau') \
   .allowed()
 summary('Nouveau', with_nouveau)
 
+with_gsgpu = get_option('gsgpu') \
+  .require(with_atomics, error_message : 'libdrm_gsgpu requires atomics') \
+  .disable_auto_if(not ['loongarch64', 'mips64'].contains(host_machine.cpu_family())) \
+  .allowed()
+summary('GSGPU', with_gsgpu)
+
 with_vmwgfx = get_option('vmwgfx').allowed()
 summary('vmwgfx', with_vmwgfx)
 
@@ -311,6 +317,7 @@ install_headers(
   'include/drm/savage_drm.h', 'include/drm/sis_drm.h',
   'include/drm/tegra_drm.h', 'include/drm/vc4_drm.h',
   'include/drm/via_drm.h', 'include/drm/virtgpu_drm.h',
+  'include/drm/gsgpu_drm.h',
   subdir : 'libdrm',
 )
 if with_vmwgfx
@@ -354,6 +361,9 @@ endif
 if with_etnaviv
   subdir('etnaviv')
 endif
+if with_gsgpu
+  subdir('gsgpu')
+endif
 if with_man_pages
   subdir('man')
 endif
diff --git a/meson_options.txt b/meson_options.txt
index e80d79e2..64686084 100644
--- a/meson_options.txt
+++ b/meson_options.txt
@@ -33,6 +33,11 @@ option(
   type : 'feature',
   description : '''Enable support for amdgpu's KMS API.''',
 )
+option(
+  'gsgpu',
+  type : 'feature',
+  description : '''Enable support for gsgpu's KMS API.''',
+)
 option(
   'nouveau',
   type : 'feature',
diff --git a/tests/gsgpu/Makefile.am b/tests/gsgpu/Makefile.am
new file mode 100644
index 00000000..d719eb7c
--- /dev/null
+++ b/tests/gsgpu/Makefile.am
@@ -0,0 +1,28 @@
+AM_CFLAGS = \
+	-I $(top_srcdir)/include/drm \
+	-I $(top_srcdir)/gsgpu \
+	-I $(top_srcdir) \
+	-pthread
+
+LDADD = $(top_builddir)/libdrm.la \
+	$(top_builddir)/gsgpu/libdrm_gsgpu.la \
+	$(CUNIT_LIBS)
+
+if HAVE_INSTALL_TESTS
+bin_PROGRAMS = \
+	gsgpu_test
+else
+noinst_PROGRAMS = \
+	gsgpu_test
+endif
+
+gsgpu_test_CPPFLAGS = $(CUNIT_CFLAGS)
+
+gsgpu_test_SOURCES = \
+	gsgpu_test.c \
+	gsgpu_test.h \
+	basic_tests.c \
+	bo_tests.c \
+	dma_tests.c \
+	deadlock_tests.c \
+	vm_tests.c
diff --git a/tests/gsgpu/basic_tests.c b/tests/gsgpu/basic_tests.c
new file mode 100755
index 00000000..ccb595b4
--- /dev/null
+++ b/tests/gsgpu/basic_tests.c
@@ -0,0 +1,1179 @@
+/*
+ * Copyright (C) 2020 Loongson Technology Co., Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+*/
+
+#ifdef HAVE_CONFIG_H
+#include "config.h"
+#endif
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <unistd.h>
+#ifdef HAVE_ALLOCA_H
+# include <alloca.h>
+#endif
+#include <sys/wait.h>
+
+#include "CUnit/Basic.h"
+
+#include "gsgpu_test.h"
+#include "gsgpu_drm.h"
+
+gsgpu_device_handle device_handle;
+uint32_t  major_version;
+uint32_t  minor_version;
+uint32_t  family_id;
+
+static void gsgpu_query_info_test(void);
+static void gsgpu_command_submission_gfx(void);
+static void gsgpu_command_submission_multi_fence(void);
+static void gsgpu_userptr_test(void);
+static void gsgpu_semaphore_test(void);
+static void gsgpu_sync_dependency_test(void);
+static void gsgpu_bo_eviction_test(void);
+
+void gsgpu_command_submission_write_linear_helper(unsigned ip_type);
+static void gsgpu_command_submission_const_fill_helper(unsigned ip_type);
+static void gsgpu_command_submission_copy_linear_helper(unsigned ip_type);
+static void gsgpu_command_submission_msaa_resolve_helper(unsigned ip_type);
+void gsgpu_test_exec_cs_helper(gsgpu_context_handle context_handle,
+				       unsigned ip_type,
+				       int instance, int pm4_dw, uint32_t *pm4_src,
+				       int res_cnt, gsgpu_bo_handle *resources,
+				       struct gsgpu_cs_ib_info *ib_info,
+				       struct gsgpu_cs_request *ibs_request);
+
+CU_TestInfo basic_tests[] = {
+	{ "Query Info Test",  gsgpu_query_info_test },
+	{ "Userptr Test",  gsgpu_userptr_test },
+	{ "bo eviction Test",  gsgpu_bo_eviction_test },
+	{ "Command submission Test (GFX)",  gsgpu_command_submission_gfx },
+	{ "Command submission Test (Multi-Fence)", gsgpu_command_submission_multi_fence },
+	{ "SW semaphore Test",  gsgpu_semaphore_test },
+	{ "Sync dependency Test",  gsgpu_sync_dependency_test },
+	CU_TEST_INFO_NULL,
+};
+
+#define BUFFER_SIZE (16 * 1024)
+#define BUFFER_ALIGN (16 * 1024)
+
+#define SWAP_32(num) (((num & 0xff000000) >> 24) | \
+		      ((num & 0x0000ff00) << 8) | \
+		      ((num & 0x00ff0000) >> 8) | \
+		      ((num & 0x000000ff) << 24))
+
+#define CODE_OFFSET 512
+#define DATA_OFFSET 1024
+
+int suite_basic_tests_init(void)
+{
+	struct gsgpu_gpu_info gpu_info = {0};
+	int r;
+
+	r = gsgpu_device_initialize(drm_gsgpu[0], &major_version,
+				   &minor_version, &device_handle);
+
+	if (r) {
+		if ((r == -EACCES) && (errno == EACCES))
+			printf("\n\nError:%s. "
+				"Hint:Try to run this test program as root.",
+				strerror(errno));
+		return CUE_SINIT_FAILED;
+	}
+
+	r = gsgpu_query_gpu_info(device_handle, &gpu_info);
+	if (r)
+		return CUE_SINIT_FAILED;
+
+	family_id = gpu_info.family_id;
+
+	return CUE_SUCCESS;
+}
+
+int suite_basic_tests_clean(void)
+{
+	int r = gsgpu_device_deinitialize(device_handle);
+
+	if (r == 0)
+		return CUE_SUCCESS;
+	else
+		return CUE_SCLEAN_FAILED;
+}
+
+static void gsgpu_query_info_test(void)
+{
+	struct gsgpu_gpu_info gpu_info = {0};
+	uint32_t version, feature;
+	int r;
+
+	r = gsgpu_query_gpu_info(device_handle, &gpu_info);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_query_firmware_version(device_handle, GSGPU_INFO_FW_VCE, 0,
+					  0, &version, &feature);
+	CU_ASSERT_EQUAL(r, 0);
+}
+
+static void gsgpu_command_submission_gfx_separate_ibs(void)
+{
+	gsgpu_context_handle context_handle;
+	gsgpu_bo_handle ib_result_handle, ib_result_ce_handle;
+	void *ib_result_cpu, *ib_result_ce_cpu;
+	uint64_t ib_result_mc_address, ib_result_ce_mc_address;
+	struct gsgpu_cs_request ibs_request = {0};
+	struct gsgpu_cs_ib_info ib_info[2];
+	struct gsgpu_cs_fence fence_status = {0};
+	uint32_t *ptr;
+	uint32_t expired;
+	gsgpu_bo_list_handle bo_list;
+	gsgpu_va_handle va_handle, va_handle_ce;
+	int r;
+
+	r = gsgpu_cs_ctx_create(device_handle, &context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_bo_alloc_and_map(device_handle, 4096, 4096,
+				    GSGPU_GEM_DOMAIN_GTT, 0,
+				    &ib_result_handle, &ib_result_cpu,
+				    &ib_result_mc_address, &va_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_bo_alloc_and_map(device_handle, 4096, 4096,
+				    GSGPU_GEM_DOMAIN_GTT, 0,
+				    &ib_result_ce_handle, &ib_result_ce_cpu,
+				    &ib_result_ce_mc_address, &va_handle_ce);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_get_bo_list(device_handle, ib_result_handle,
+			       ib_result_ce_handle, &bo_list);
+	CU_ASSERT_EQUAL(r, 0);
+
+	memset(ib_info, 0, 2 * sizeof(struct gsgpu_cs_ib_info));
+
+	/* IT_SET_CE_DE_COUNTERS */
+	ptr = ib_result_ce_cpu;
+	ptr[0] = GSGPU_CMD_NOP;
+	ib_info[0].ib_mc_address = ib_result_ce_mc_address;
+	ib_info[0].size = 1;
+	ib_info[0].flags = GSGPU_IB_FLAG_CE;
+
+	/* IT_WAIT_ON_CE_COUNTER */
+	ptr = ib_result_cpu;
+	ptr[0] = GSGPU_CMD_NOP;
+	ib_info[1].ib_mc_address = ib_result_mc_address;
+	ib_info[1].size = 1;
+
+	ibs_request.ip_type = GSGPU_HW_IP_GFX;
+	ibs_request.number_of_ibs = 2;
+	ibs_request.ibs = ib_info;
+	ibs_request.resources = bo_list;
+	ibs_request.fence_info.handle = NULL;
+
+	r = gsgpu_cs_submit(context_handle, 0,&ibs_request, 1);
+
+	CU_ASSERT_EQUAL(r, 0);
+
+	fence_status.context = context_handle;
+	fence_status.ip_type = GSGPU_HW_IP_GFX;
+	fence_status.ip_instance = 0;
+	fence_status.fence = ibs_request.seq_no;
+
+	r = gsgpu_cs_query_fence_status(&fence_status,
+					 GSGPU_TIMEOUT_INFINITE,
+					 0, &expired);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_bo_unmap_and_free(ib_result_handle, va_handle,
+				     ib_result_mc_address, 4096);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_bo_unmap_and_free(ib_result_ce_handle, va_handle_ce,
+				     ib_result_ce_mc_address, 4096);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_bo_list_destroy(bo_list);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_cs_ctx_free(context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+}
+
+static void gsgpu_command_submission_gfx_shared_ib(void)
+{
+	gsgpu_context_handle context_handle;
+	gsgpu_bo_handle ib_result_handle;
+	void *ib_result_cpu;
+	uint64_t ib_result_mc_address;
+	struct gsgpu_cs_request ibs_request = {0};
+	struct gsgpu_cs_ib_info ib_info[2];
+	struct gsgpu_cs_fence fence_status = {0};
+	uint32_t *ptr;
+	uint32_t expired;
+	gsgpu_bo_list_handle bo_list;
+	gsgpu_va_handle va_handle;
+	int r;
+
+	r = gsgpu_cs_ctx_create(device_handle, &context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_bo_alloc_and_map(device_handle, 4096, 4096,
+				    GSGPU_GEM_DOMAIN_GTT, 0,
+				    &ib_result_handle, &ib_result_cpu,
+				    &ib_result_mc_address, &va_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_get_bo_list(device_handle, ib_result_handle, NULL,
+			       &bo_list);
+	CU_ASSERT_EQUAL(r, 0);
+
+	memset(ib_info, 0, 2 * sizeof(struct gsgpu_cs_ib_info));
+
+	/* IT_SET_CE_DE_COUNTERS */
+	ptr = ib_result_cpu;
+	ptr[0] = GSGPU_CMD_NOP;
+
+	ib_info[0].ib_mc_address = ib_result_mc_address;
+	ib_info[0].size = 1;
+	ib_info[0].flags = GSGPU_IB_FLAG_CE;
+
+	ptr = (uint32_t *)ib_result_cpu + 4;
+	ptr[0] = GSGPU_CMD_NOP;
+	ib_info[1].ib_mc_address = ib_result_mc_address + 16;
+	ib_info[1].size = 1;
+
+	ibs_request.ip_type = GSGPU_HW_IP_GFX;
+	ibs_request.number_of_ibs = 2;
+	ibs_request.ibs = ib_info;
+	ibs_request.resources = bo_list;
+	ibs_request.fence_info.handle = NULL;
+
+	r = gsgpu_cs_submit(context_handle, 0, &ibs_request, 1);
+
+	CU_ASSERT_EQUAL(r, 0);
+
+	fence_status.context = context_handle;
+	fence_status.ip_type = GSGPU_HW_IP_GFX;
+	fence_status.ip_instance = 0;
+	fence_status.fence = ibs_request.seq_no;
+
+	r = gsgpu_cs_query_fence_status(&fence_status,
+					 GSGPU_TIMEOUT_INFINITE,
+					 0, &expired);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_bo_unmap_and_free(ib_result_handle, va_handle,
+				     ib_result_mc_address, 4096);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_bo_list_destroy(bo_list);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_cs_ctx_free(context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+}
+
+static void gsgpu_command_submission_gfx_cp_write_data(void)
+{
+	gsgpu_command_submission_write_linear_helper(GSGPU_HW_IP_GFX);
+}
+
+static void gsgpu_command_submission_gfx_cp_const_fill(void)
+{
+	gsgpu_command_submission_const_fill_helper(GSGPU_HW_IP_GFX);
+}
+
+static void gsgpu_command_submission_gfx_cp_copy_data(void)
+{
+	gsgpu_command_submission_copy_linear_helper(GSGPU_HW_IP_GFX);
+}
+
+static void gsgpu_bo_eviction_test(void)
+{
+	const int sdma_write_length = 1024;
+	const int pm4_dw = 256;
+	gsgpu_context_handle context_handle;
+	gsgpu_bo_handle bo1, bo2, vram_max[2], gtt_max[2];
+	gsgpu_bo_handle *resources;
+	uint32_t *pm4;
+	struct gsgpu_cs_ib_info *ib_info;
+	struct gsgpu_cs_request *ibs_request;
+	uint64_t bo1_mc, bo2_mc;
+	volatile unsigned char *bo1_cpu, *bo2_cpu;
+	int i, j, r, loop1, loop2, align_dw;
+	uint64_t gtt_flags[2] = {0, 0};
+	gsgpu_va_handle bo1_va_handle, bo2_va_handle;
+	struct gsgpu_heap_info vram_info, gtt_info;
+	struct drm_gsgpu_info_hw_ip hw_ip_info;
+
+	r = gsgpu_query_hw_ip_info(device_handle, GSGPU_HW_IP_DMA, 0, &hw_ip_info);
+	CU_ASSERT_EQUAL(r, 0);
+
+	pm4 = calloc(pm4_dw, sizeof(*pm4));
+	CU_ASSERT_NOT_EQUAL(pm4, NULL);
+
+	ib_info = calloc(1, sizeof(*ib_info));
+	CU_ASSERT_NOT_EQUAL(ib_info, NULL);
+
+	ibs_request = calloc(1, sizeof(*ibs_request));
+	CU_ASSERT_NOT_EQUAL(ibs_request, NULL);
+
+	r = gsgpu_cs_ctx_create(device_handle, &context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+	/* prepare resource */
+	resources = calloc(4, sizeof(gsgpu_bo_handle));
+	CU_ASSERT_NOT_EQUAL(resources, NULL);
+
+	r = gsgpu_query_heap_info(device_handle, GSGPU_GEM_DOMAIN_VRAM,
+				   0, &vram_info);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_bo_alloc_wrap(device_handle, vram_info.max_allocation, 4096,
+				 GSGPU_GEM_DOMAIN_VRAM, 0, &vram_max[0]);
+	CU_ASSERT_EQUAL(r, 0);
+	r = gsgpu_bo_alloc_wrap(device_handle, vram_info.max_allocation, 4096,
+				 GSGPU_GEM_DOMAIN_VRAM, 0, &vram_max[1]);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_query_heap_info(device_handle, GSGPU_GEM_DOMAIN_GTT,
+				   0, &gtt_info);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_bo_alloc_wrap(device_handle, gtt_info.max_allocation, 4096,
+				 GSGPU_GEM_DOMAIN_GTT, 0, &gtt_max[0]);
+	CU_ASSERT_EQUAL(r, 0);
+	r = gsgpu_bo_alloc_wrap(device_handle, gtt_info.max_allocation, 4096,
+				 GSGPU_GEM_DOMAIN_GTT, 0, &gtt_max[1]);
+	CU_ASSERT_EQUAL(r, 0);
+
+
+
+	loop1 = loop2 = 0;
+	/* run 9 circle to test all mapping combination */
+	while(loop1 < 2) {
+		while(loop2 < 2) {
+			/* allocate UC bo1for sDMA use */
+			r = gsgpu_bo_alloc_and_map(device_handle,
+						    sdma_write_length, 4096,
+						    GSGPU_GEM_DOMAIN_GTT,
+						    gtt_flags[loop1], &bo1,
+						    (void**)&bo1_cpu, &bo1_mc,
+						    &bo1_va_handle);
+			CU_ASSERT_EQUAL(r, 0);
+
+			/* set bo1 */
+			memset((void*)bo1_cpu, 0xaa, sdma_write_length);
+
+			/* allocate UC bo2 for sDMA use */
+			r = gsgpu_bo_alloc_and_map(device_handle,
+						    sdma_write_length, 4096,
+						    GSGPU_GEM_DOMAIN_GTT,
+						    gtt_flags[loop2], &bo2,
+						    (void**)&bo2_cpu, &bo2_mc,
+						    &bo2_va_handle);
+			CU_ASSERT_EQUAL(r, 0);
+
+			/* clear bo2 */
+			memset((void*)bo2_cpu, 0, sdma_write_length);
+
+			resources[0] = bo1;
+			resources[1] = bo2;
+			resources[2] = vram_max[loop2];
+			resources[3] = gtt_max[loop2];
+
+                        /* fulfill PM4: test DMA copy linear */
+                        i = j = 0;
+                        uint32_t res_format = GSGPU_CMD_XDMA_FORMAT_RGBA16;
+                        uint32_t op_mode = 1;
+                        pm4[i++] = GSPKT(GSGPU_CMD_XDMA_COPY, 8) | (res_format << 8) | (op_mode << 24);
+                        pm4[i++] = (1 << 16) | sdma_write_length/8;
+                        pm4[i++] = 0xffffffff & bo1_mc;
+                        pm4[i++] = (0xffffffff00000000 & bo1_mc) >> 32;
+                        pm4[i++] = 0xffffffff & bo2_mc;
+                        pm4[i++] = (0xffffffff00000000 & bo2_mc) >> 32;
+                        pm4[i++] = sdma_write_length;
+                        pm4[i++] = sdma_write_length;
+                        pm4[i++] = 0;
+
+			/* ib cmd packet align */
+			align_dw = hw_ip_info.ib_size_alignment - (i & (hw_ip_info.ib_size_alignment - 1));
+			for (j = 0; j < align_dw; j++) {
+				pm4[i++] = GSGPU_CMD_NOP;
+			}
+
+			gsgpu_test_exec_cs_helper(context_handle,
+						   GSGPU_HW_IP_DMA, 0,
+						   i, pm4,
+						   4, resources,
+						   ib_info, ibs_request);
+
+			/* verify if SDMA test result meets with expected */
+			i = 0;
+			while(i < sdma_write_length) {
+				CU_ASSERT_EQUAL(bo2_cpu[i++], 0xaa);
+			}
+			r = gsgpu_bo_unmap_and_free(bo1, bo1_va_handle, bo1_mc,
+						     sdma_write_length);
+			CU_ASSERT_EQUAL(r, 0);
+			r = gsgpu_bo_unmap_and_free(bo2, bo2_va_handle, bo2_mc,
+						     sdma_write_length);
+			CU_ASSERT_EQUAL(r, 0);
+			loop2++;
+		}
+		loop2 = 0;
+		loop1++;
+	}
+	gsgpu_bo_free(vram_max[0]);
+	gsgpu_bo_free(vram_max[1]);
+	gsgpu_bo_free(gtt_max[0]);
+	gsgpu_bo_free(gtt_max[1]);
+	/* clean resources */
+	free(resources);
+	free(ibs_request);
+	free(ib_info);
+	free(pm4);
+
+	/* end of test */
+	r = gsgpu_cs_ctx_free(context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+}
+
+
+static void gsgpu_command_submission_gfx(void)
+{
+	/* write data using the CP */
+	gsgpu_command_submission_gfx_cp_write_data();
+	/* separate IB buffers for multi-IB submission */
+	gsgpu_command_submission_gfx_separate_ibs();
+	/* shared IB buffer for multi-IB submission */
+	gsgpu_command_submission_gfx_shared_ib();
+}
+
+static void gsgpu_semaphore_test(void)
+{
+	gsgpu_context_handle context_handle[2];
+	gsgpu_semaphore_handle sem;
+	gsgpu_bo_handle ib_result_handle[2];
+	void *ib_result_cpu[2];
+	uint64_t ib_result_mc_address[2];
+	struct gsgpu_cs_request ibs_request[2] = {0};
+	struct gsgpu_cs_ib_info ib_info[2] = {0};
+	struct gsgpu_cs_fence fence_status = {0};
+	uint32_t *ptr;
+	uint32_t expired;
+	uint32_t sdma_nop, gfx_nop;
+	gsgpu_bo_list_handle bo_list[2];
+	gsgpu_va_handle va_handle[2];
+	int r, i;
+
+	sdma_nop = gfx_nop = GSGPU_CMD_NOP;
+
+	r = gsgpu_cs_create_semaphore(&sem);
+	CU_ASSERT_EQUAL(r, 0);
+	for (i = 0; i < 2; i++) {
+		r = gsgpu_cs_ctx_create(device_handle, &context_handle[i]);
+		CU_ASSERT_EQUAL(r, 0);
+
+		r = gsgpu_bo_alloc_and_map(device_handle, 4096, 4096,
+					    GSGPU_GEM_DOMAIN_GTT, 0,
+					    &ib_result_handle[i], &ib_result_cpu[i],
+					    &ib_result_mc_address[i], &va_handle[i]);
+		CU_ASSERT_EQUAL(r, 0);
+
+		r = gsgpu_get_bo_list(device_handle, ib_result_handle[i],
+				       NULL, &bo_list[i]);
+		CU_ASSERT_EQUAL(r, 0);
+	}
+
+	/* 1. same context different engine */
+	ptr = ib_result_cpu[0];
+	ptr[0] = sdma_nop;
+	ib_info[0].ib_mc_address = ib_result_mc_address[0];
+	ib_info[0].size = 1;
+
+	ibs_request[0].ip_type = GSGPU_HW_IP_DMA;
+	ibs_request[0].number_of_ibs = 1;
+	ibs_request[0].ibs = &ib_info[0];
+	ibs_request[0].resources = bo_list[0];
+	ibs_request[0].fence_info.handle = NULL;
+	r = gsgpu_cs_submit(context_handle[0], 0,&ibs_request[0], 1);
+	CU_ASSERT_EQUAL(r, 0);
+	r = gsgpu_cs_signal_semaphore(context_handle[0], GSGPU_HW_IP_DMA, 0, 0, sem);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_cs_wait_semaphore(context_handle[0], GSGPU_HW_IP_GFX, 0, 0, sem);
+	CU_ASSERT_EQUAL(r, 0);
+	ptr = ib_result_cpu[1];
+	ptr[0] = gfx_nop;
+	ib_info[1].ib_mc_address = ib_result_mc_address[1];
+	ib_info[1].size = 1;
+
+	ibs_request[1].ip_type = GSGPU_HW_IP_GFX;
+	ibs_request[1].number_of_ibs = 1;
+	ibs_request[1].ibs = &ib_info[1];
+	ibs_request[1].resources = bo_list[1];
+	ibs_request[1].fence_info.handle = NULL;
+
+	r = gsgpu_cs_submit(context_handle[0], 0,&ibs_request[1], 1);
+	CU_ASSERT_EQUAL(r, 0);
+
+	fence_status.context = context_handle[0];
+	fence_status.ip_type = GSGPU_HW_IP_GFX;
+	fence_status.ip_instance = 0;
+	fence_status.fence = ibs_request[1].seq_no;
+	r = gsgpu_cs_query_fence_status(&fence_status,
+					 5000000000000, 0, &expired);
+	CU_ASSERT_EQUAL(r, 0);
+	CU_ASSERT_EQUAL(expired, true);
+
+	/* 2. same engine different context */
+	ptr = ib_result_cpu[0];
+	ptr[0] = gfx_nop;
+	ib_info[0].ib_mc_address = ib_result_mc_address[0];
+	ib_info[0].size = 1;
+
+	ibs_request[0].ip_type = GSGPU_HW_IP_GFX;
+	ibs_request[0].number_of_ibs = 1;
+	ibs_request[0].ibs = &ib_info[0];
+	ibs_request[0].resources = bo_list[0];
+	ibs_request[0].fence_info.handle = NULL;
+	r = gsgpu_cs_submit(context_handle[0], 0,&ibs_request[0], 1);
+	CU_ASSERT_EQUAL(r, 0);
+	r = gsgpu_cs_signal_semaphore(context_handle[0], GSGPU_HW_IP_GFX, 0, 0, sem);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_cs_wait_semaphore(context_handle[1], GSGPU_HW_IP_GFX, 0, 0, sem);
+	CU_ASSERT_EQUAL(r, 0);
+	ptr = ib_result_cpu[1];
+	ptr[0] = gfx_nop;
+	ib_info[1].ib_mc_address = ib_result_mc_address[1];
+	ib_info[1].size = 1;
+
+	ibs_request[1].ip_type = GSGPU_HW_IP_GFX;
+	ibs_request[1].number_of_ibs = 1;
+	ibs_request[1].ibs = &ib_info[1];
+	ibs_request[1].resources = bo_list[1];
+	ibs_request[1].fence_info.handle = NULL;
+	r = gsgpu_cs_submit(context_handle[1], 0,&ibs_request[1], 1);
+
+	CU_ASSERT_EQUAL(r, 0);
+
+	fence_status.context = context_handle[1];
+	fence_status.ip_type = GSGPU_HW_IP_GFX;
+	fence_status.ip_instance = 0;
+	fence_status.fence = ibs_request[1].seq_no;
+	r = gsgpu_cs_query_fence_status(&fence_status,
+					 50000000000000, 0, &expired);
+	CU_ASSERT_EQUAL(r, 0);
+	CU_ASSERT_EQUAL(expired, true);
+
+	for (i = 0; i < 2; i++) {
+		r = gsgpu_bo_unmap_and_free(ib_result_handle[i], va_handle[i],
+					     ib_result_mc_address[i], 4096);
+		CU_ASSERT_EQUAL(r, 0);
+
+		r = gsgpu_bo_list_destroy(bo_list[i]);
+		CU_ASSERT_EQUAL(r, 0);
+
+		r = gsgpu_cs_ctx_free(context_handle[i]);
+		CU_ASSERT_EQUAL(r, 0);
+	}
+
+	r = gsgpu_cs_destroy_semaphore(sem);
+	CU_ASSERT_EQUAL(r, 0);
+}
+
+/*
+ * caller need create/release:
+ * pm4_src, resources, ib_info, and ibs_request
+ * submit command stream described in ibs_request and wait for this IB accomplished
+ */
+void gsgpu_test_exec_cs_helper(gsgpu_context_handle context_handle,
+				       unsigned ip_type,
+				       int instance, int pm4_dw, uint32_t *pm4_src,
+				       int res_cnt, gsgpu_bo_handle *resources,
+				       struct gsgpu_cs_ib_info *ib_info,
+				       struct gsgpu_cs_request *ibs_request)
+{
+	int r;
+	uint32_t expired;
+	uint32_t *ring_ptr;
+	gsgpu_bo_handle ib_result_handle = NULL;
+	void *ib_result_cpu;
+	uint64_t ib_result_mc_address = 0;
+	struct gsgpu_cs_fence fence_status = {0};
+	gsgpu_bo_handle *all_res = alloca(sizeof(resources[0]) * (res_cnt + 1));
+	gsgpu_va_handle va_handle = NULL;
+
+	/* prepare CS */
+	CU_ASSERT_NOT_EQUAL(pm4_src, NULL);
+	CU_ASSERT_NOT_EQUAL(resources, NULL);
+	CU_ASSERT_NOT_EQUAL(ib_info, NULL);
+	CU_ASSERT_NOT_EQUAL(ibs_request, NULL);
+	CU_ASSERT_TRUE(pm4_dw <= BUFFER_SIZE / 4);
+
+	/* allocate IB */
+	r = gsgpu_bo_alloc_and_map(device_handle, BUFFER_SIZE, BUFFER_ALIGN,
+				    GSGPU_GEM_DOMAIN_GTT, 0,
+				    &ib_result_handle, &ib_result_cpu,
+				    &ib_result_mc_address, &va_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+	/* copy PM4 packet to ring from caller */
+	ring_ptr = ib_result_cpu;
+	memcpy(ring_ptr, pm4_src, pm4_dw * sizeof(*pm4_src));
+
+	ib_info->ib_mc_address = ib_result_mc_address;
+	ib_info->size = pm4_dw;
+
+	ibs_request->ip_type = ip_type;
+	ibs_request->ring = instance;
+	ibs_request->number_of_ibs = 1;
+	ibs_request->ibs = ib_info;
+	ibs_request->fence_info.handle = NULL;
+
+	memcpy(all_res, resources, sizeof(resources[0]) * res_cnt);
+	all_res[res_cnt] = ib_result_handle;
+
+	r = gsgpu_bo_list_create(device_handle, res_cnt+1, all_res,
+				  NULL, &ibs_request->resources);
+	CU_ASSERT_EQUAL(r, 0);
+
+	CU_ASSERT_NOT_EQUAL(ibs_request, NULL);
+
+	/* submit CS */
+	r = gsgpu_cs_submit(context_handle, 0, ibs_request, 1);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_bo_list_destroy(ibs_request->resources);
+	CU_ASSERT_EQUAL(r, 0);
+
+	fence_status.ip_type = ip_type;
+	fence_status.ip_instance = 0;
+	fence_status.ring = ibs_request->ring;
+	fence_status.context = context_handle;
+	fence_status.fence = ibs_request->seq_no;
+
+	/* wait for IB accomplished */
+	r = gsgpu_cs_query_fence_status(&fence_status,
+					 GSGPU_TIMEOUT_INFINITE,
+					 0, &expired);
+	CU_ASSERT_EQUAL(r, 0);
+	CU_ASSERT_EQUAL(expired, true);
+
+	r = gsgpu_bo_unmap_and_free(ib_result_handle, va_handle,
+				     ib_result_mc_address, BUFFER_SIZE);
+	CU_ASSERT_EQUAL(r, 0);
+}
+
+void gsgpu_command_submission_write_linear_helper(unsigned ip_type)
+{
+	const int sdma_write_length = 1;
+	const int pm4_dw = 256;
+	gsgpu_context_handle context_handle;
+	gsgpu_bo_handle bo;
+	gsgpu_bo_handle *resources;
+	uint32_t *pm4;
+	struct gsgpu_cs_ib_info *ib_info;
+	struct gsgpu_cs_request *ibs_request;
+	uint64_t bo_mc = 0;
+	volatile uint32_t *bo_cpu;
+	int i, j, r, loop, ring_id;
+	uint64_t gtt_flags[2] = {0};
+	gsgpu_va_handle va_handle = NULL;
+	struct drm_gsgpu_info_hw_ip hw_ip_info;
+
+	pm4 = calloc(pm4_dw, sizeof(*pm4));
+	CU_ASSERT_NOT_EQUAL(pm4, NULL);
+
+	ib_info = calloc(1, sizeof(*ib_info));
+	CU_ASSERT_NOT_EQUAL(ib_info, NULL);
+
+	ibs_request = calloc(1, sizeof(*ibs_request));
+	CU_ASSERT_NOT_EQUAL(ibs_request, NULL);
+
+	r = gsgpu_query_hw_ip_info(device_handle, ip_type, 0, &hw_ip_info);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_cs_ctx_create(device_handle, &context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+	/* prepare resource */
+	resources = calloc(1, sizeof(gsgpu_bo_handle));
+	CU_ASSERT_NOT_EQUAL(resources, NULL);
+
+	for (ring_id = 0; (1 << ring_id) & hw_ip_info.available_rings; ring_id++) {
+		loop = 0;
+		while(loop < 1) {
+			/* allocate UC bo for sDMA use */
+			r = gsgpu_bo_alloc_and_map(device_handle,
+						    sdma_write_length * sizeof(uint32_t),
+						    4096, GSGPU_GEM_DOMAIN_GTT,
+						    gtt_flags[loop], &bo, (void**)&bo_cpu,
+						    &bo_mc, &va_handle);
+			CU_ASSERT_EQUAL(r, 0);
+
+			/* clear bo */
+			memset((void*)bo_cpu, 0, sdma_write_length * sizeof(uint32_t));
+
+			resources[0] = bo;
+
+			/* fulfill PM4: test DMA write-linear */
+			i = j = 0;
+
+			pm4[i++] = GSPKT(GSGPU_CMD_WRITE, 3) | WRITE_DST_SEL(1);
+			pm4[i++] = 0xffffffff & bo_mc;
+			pm4[i++] = (0xffffffff00000000 & bo_mc) >> 32;
+			pm4[i++] = 0xdeadbeaf;
+
+			gsgpu_test_exec_cs_helper(context_handle,
+						   ip_type, ring_id,
+						   i, pm4,
+						   1, resources,
+						   ib_info, ibs_request);
+
+			/* verify if SDMA test result meets with expected */
+			i = 0;
+			while(i < sdma_write_length) {
+				CU_ASSERT_EQUAL(bo_cpu[i++], 0xdeadbeaf);
+			}
+
+			r = gsgpu_bo_unmap_and_free(bo, va_handle, bo_mc,
+						     sdma_write_length * sizeof(uint32_t));
+			CU_ASSERT_EQUAL(r, 0);
+			loop++;
+		}
+	}
+	/* clean resources */
+	free(resources);
+	free(ibs_request);
+	free(ib_info);
+	free(pm4);
+
+	/* end of test */
+	r = gsgpu_cs_ctx_free(context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+}
+
+static void gsgpu_command_submission_const_fill_helper(unsigned ip_type)
+{
+	const int sdma_write_length = 16 * 1024;
+	const int pm4_dw = 256;
+	gsgpu_context_handle context_handle;
+	gsgpu_bo_handle bo;
+	gsgpu_bo_handle *resources;
+	uint32_t *pm4;
+	struct gsgpu_cs_ib_info *ib_info;
+	struct gsgpu_cs_request *ibs_request;
+	uint64_t bo_mc;
+	volatile uint32_t *bo_cpu;
+	int i, j, r, loop, loop_cnt, ring_id, align_dw;
+	uint64_t gtt_flags[1] = {0};
+	gsgpu_va_handle va_handle;
+	struct drm_gsgpu_info_hw_ip hw_ip_info;
+
+	pm4 = calloc(pm4_dw, sizeof(*pm4));
+	CU_ASSERT_NOT_EQUAL(pm4, NULL);
+
+	ib_info = calloc(1, sizeof(*ib_info));
+	CU_ASSERT_NOT_EQUAL(ib_info, NULL);
+
+	ibs_request = calloc(1, sizeof(*ibs_request));
+	CU_ASSERT_NOT_EQUAL(ibs_request, NULL);
+
+	r = gsgpu_query_hw_ip_info(device_handle, ip_type, 0, &hw_ip_info);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_cs_ctx_create(device_handle, &context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+	/* prepare resource */
+	resources = calloc(1, sizeof(gsgpu_bo_handle));
+	CU_ASSERT_NOT_EQUAL(resources, NULL);
+
+	for (ring_id = 0; (1 << ring_id) & hw_ip_info.available_rings; ring_id++) {
+		loop = 0;
+		loop_cnt = sizeof(gtt_flags) / sizeof(gtt_flags[0]);
+
+		while(loop < loop_cnt) {
+			/* allocate bo for DMA use */
+			r = gsgpu_bo_alloc_and_map(device_handle,
+						    sdma_write_length, BUFFER_ALIGN,
+						    GSGPU_GEM_DOMAIN_GTT,
+						    gtt_flags[loop], &bo, (void**)&bo_cpu,
+						    &bo_mc, &va_handle);
+			CU_ASSERT_EQUAL(r, 0);
+
+			/* clear bo */
+			memset((void*)bo_cpu, 0, sdma_write_length);
+
+			resources[0] = bo;
+
+			/* fulfill cmd packet: test SDMA const fill */
+			i = 0;
+			pm4[i++] = GSPKT(GSGPU_CMD_XDMA_COPY, 4);
+			pm4[i++] = (uint32_t)(0xffffffff & bo_mc);
+			pm4[i++] = (uint32_t)((0xffffffff00000000 & bo_mc) >> 32);
+			pm4[i++] = sdma_write_length;
+			pm4[i++] = 0xdeadbeaf;
+
+			/* ib cmd packet align */
+			align_dw = hw_ip_info.ib_size_alignment - (i & (hw_ip_info.ib_size_alignment - 1));
+			for (j = 0; j < align_dw; j++) {
+				pm4[i++] = GSGPU_CMD_NOP;
+			}
+
+			gsgpu_test_exec_cs_helper(context_handle,
+						   ip_type, ring_id,
+						   i, pm4,
+						   1, resources,
+						   ib_info, ibs_request);
+
+			/* verify if SDMA test result meets with expected */
+			i = 0;
+			while (i < (sdma_write_length / 4)) {
+				CU_ASSERT_EQUAL(bo_cpu[i++], 0xdeadbeaf);
+			}
+
+			r = gsgpu_bo_unmap_and_free(bo, va_handle, bo_mc,
+						     sdma_write_length);
+			CU_ASSERT_EQUAL(r, 0);
+			loop++;
+		}
+	}
+	/* clean resources */
+	free(resources);
+	free(ibs_request);
+	free(ib_info);
+	free(pm4);
+
+	/* end of test */
+	r = gsgpu_cs_ctx_free(context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+}
+
+static void gsgpu_command_submission_copy_linear_helper(unsigned ip_type)
+{
+	const int sdma_write_length = 1024;
+	const int pm4_dw = 256;
+	gsgpu_context_handle context_handle = NULL;
+	gsgpu_bo_handle bo1 = NULL, bo2 = NULL;
+	gsgpu_bo_handle *resources;
+	uint32_t *pm4;
+	struct gsgpu_cs_ib_info *ib_info;
+	struct gsgpu_cs_request *ibs_request;
+	uint64_t bo1_mc, bo2_mc;
+	volatile unsigned char *bo1_cpu, *bo2_cpu;
+	int i, j, r, loop1, loop2, ring_id, align_dw;
+	uint64_t gtt_flags[2] = {0};
+	gsgpu_va_handle bo1_va_handle = NULL, bo2_va_handle = NULL;
+	struct drm_gsgpu_info_hw_ip hw_ip_info;
+
+	pm4 = calloc(pm4_dw, sizeof(*pm4));
+	CU_ASSERT_NOT_EQUAL(pm4, NULL);
+
+	ib_info = calloc(1, sizeof(*ib_info));
+	CU_ASSERT_NOT_EQUAL(ib_info, NULL);
+
+	ibs_request = calloc(1, sizeof(*ibs_request));
+	CU_ASSERT_NOT_EQUAL(ibs_request, NULL);
+
+	r = gsgpu_query_hw_ip_info(device_handle, ip_type, 0, &hw_ip_info);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_cs_ctx_create(device_handle, &context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+	/* prepare resource */
+	resources = calloc(2, sizeof(gsgpu_bo_handle));
+	CU_ASSERT_NOT_EQUAL(resources, NULL);
+
+	for (ring_id = 0; (1 << ring_id) & hw_ip_info.available_rings; ring_id++) {
+		loop1 = loop2 = 0;
+		/* run 9 circle to test all mapping combination */
+		while(loop1 < 2) {
+			while(loop2 < 2) {
+				/* allocate UC bo1for sDMA use */
+				r = gsgpu_bo_alloc_and_map(device_handle,
+							    sdma_write_length, 4096,
+							    GSGPU_GEM_DOMAIN_GTT,
+							    gtt_flags[loop1], &bo1,
+							    (void**)&bo1_cpu, &bo1_mc,
+							    &bo1_va_handle);
+				CU_ASSERT_EQUAL(r, 0);
+
+				/* set bo1 */
+				memset((void*)bo1_cpu, 0xaa, sdma_write_length);
+
+				/* allocate UC bo2 for sDMA use */
+				r = gsgpu_bo_alloc_and_map(device_handle,
+							    sdma_write_length, 4096,
+							    GSGPU_GEM_DOMAIN_GTT,
+							    gtt_flags[loop2], &bo2,
+							    (void**)&bo2_cpu, &bo2_mc,
+							    &bo2_va_handle);
+				CU_ASSERT_EQUAL(r, 0);
+
+				/* clear bo2 */
+				memset((void*)bo2_cpu, 0, sdma_write_length);
+
+				resources[0] = bo1;
+				resources[1] = bo2;
+
+                                /* fulfill PM4: test DMA copy linear */
+                                i = j = 0;
+                                uint32_t res_format = GSGPU_CMD_XDMA_FORMAT_RGBA16;
+                                uint32_t op_mode = 1;
+                                pm4[i++] = GSPKT(GSGPU_CMD_XDMA_COPY, 8) | (res_format << 8) | (op_mode << 24);
+                                pm4[i++] = (1 << 16) | sdma_write_length/8;
+                                pm4[i++] = 0xffffffff & bo1_mc;
+                                pm4[i++] = (0xffffffff00000000 & bo1_mc) >> 32;
+                                pm4[i++] = 0xffffffff & bo2_mc;
+                                pm4[i++] = (0xffffffff00000000 & bo2_mc) >> 32;
+                                pm4[i++] = sdma_write_length;
+                                pm4[i++] = sdma_write_length;
+                                pm4[i++] = 0;
+				/* ib cmd packet align */
+				align_dw = hw_ip_info.ib_size_alignment - (i & (hw_ip_info.ib_size_alignment - 1));
+				for (j = 0; j < align_dw; j++) {
+					pm4[i++] = GSGPU_CMD_NOP;
+				}
+
+				gsgpu_test_exec_cs_helper(context_handle,
+							   ip_type, ring_id,
+							   i, pm4,
+							   2, resources,
+							   ib_info, ibs_request);
+
+				/* verify if SDMA test result meets with expected */
+				i = 0;
+				while(i < sdma_write_length) {
+					CU_ASSERT_EQUAL(bo2_cpu[i++], 0xaa);
+				}
+				r = gsgpu_bo_unmap_and_free(bo1, bo1_va_handle, bo1_mc,
+							     sdma_write_length);
+				CU_ASSERT_EQUAL(r, 0);
+				r = gsgpu_bo_unmap_and_free(bo2, bo2_va_handle, bo2_mc,
+							     sdma_write_length);
+				CU_ASSERT_EQUAL(r, 0);
+				loop2++;
+			}
+			loop1++;
+		}
+	}
+	/* clean resources */
+	free(resources);
+	free(ibs_request);
+	free(ib_info);
+	free(pm4);
+
+	/* end of test */
+	r = gsgpu_cs_ctx_free(context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+}
+
+static void gsgpu_command_submission_multi_fence_wait_all(bool wait_all)
+{
+	gsgpu_context_handle context_handle;
+	gsgpu_bo_handle ib_result_handle, ib_result_ce_handle;
+	void *ib_result_cpu, *ib_result_ce_cpu;
+	uint64_t ib_result_mc_address, ib_result_ce_mc_address;
+	struct gsgpu_cs_request ibs_request[2] = {0};
+	struct gsgpu_cs_ib_info ib_info[2];
+	struct gsgpu_cs_fence fence_status[2] = {0};
+	uint32_t *ptr;
+	uint32_t expired;
+	gsgpu_bo_list_handle bo_list;
+	gsgpu_va_handle va_handle, va_handle_ce;
+	int r;
+	int i = 0, ib_cs_num = 2;
+
+	r = gsgpu_cs_ctx_create(device_handle, &context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_bo_alloc_and_map(device_handle, 4096, 4096,
+				    GSGPU_GEM_DOMAIN_GTT, 0,
+				    &ib_result_handle, &ib_result_cpu,
+				    &ib_result_mc_address, &va_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_bo_alloc_and_map(device_handle, 4096, 4096,
+				    GSGPU_GEM_DOMAIN_GTT, 0,
+				    &ib_result_ce_handle, &ib_result_ce_cpu,
+				    &ib_result_ce_mc_address, &va_handle_ce);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_get_bo_list(device_handle, ib_result_handle,
+			       ib_result_ce_handle, &bo_list);
+	CU_ASSERT_EQUAL(r, 0);
+
+	memset(ib_info, 0, 2 * sizeof(struct gsgpu_cs_ib_info));
+
+	/* IT_SET_CE_DE_COUNTERS */
+	ptr = ib_result_ce_cpu;
+	ptr[0] = GSGPU_CMD_NOP;
+	ib_info[0].ib_mc_address = ib_result_ce_mc_address;
+	ib_info[0].size = 1;
+	ib_info[0].flags = GSGPU_IB_FLAG_CE;
+
+	/* IT_WAIT_ON_CE_COUNTER */
+	ptr = ib_result_cpu;
+	ptr[0] = GSGPU_CMD_NOP;
+	ib_info[1].ib_mc_address = ib_result_mc_address;
+	ib_info[1].size = 1;
+
+	for (i = 0; i < ib_cs_num; i++) {
+		ibs_request[i].ip_type = GSGPU_HW_IP_GFX;
+		ibs_request[i].number_of_ibs = 2;
+		ibs_request[i].ibs = ib_info;
+		ibs_request[i].resources = bo_list;
+		ibs_request[i].fence_info.handle = NULL;
+	}
+
+	r = gsgpu_cs_submit(context_handle, 0,ibs_request, ib_cs_num);
+
+	CU_ASSERT_EQUAL(r, 0);
+
+	for (i = 0; i < ib_cs_num; i++) {
+		fence_status[i].context = context_handle;
+		fence_status[i].ip_type = GSGPU_HW_IP_GFX;
+		fence_status[i].fence = ibs_request[i].seq_no;
+	}
+
+	r = gsgpu_cs_wait_fences(fence_status, ib_cs_num, wait_all,
+				GSGPU_TIMEOUT_INFINITE,
+				&expired, NULL);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_bo_unmap_and_free(ib_result_handle, va_handle,
+				     ib_result_mc_address, 4096);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_bo_unmap_and_free(ib_result_ce_handle, va_handle_ce,
+				     ib_result_ce_mc_address, 4096);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_bo_list_destroy(bo_list);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_cs_ctx_free(context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+}
+
+static void gsgpu_command_submission_multi_fence(void)
+{
+	gsgpu_command_submission_multi_fence_wait_all(true);
+	gsgpu_command_submission_multi_fence_wait_all(false);
+}
+
+static void gsgpu_userptr_test(void)
+{
+	int i, r;
+	uint32_t *pm4 = NULL;
+	uint64_t bo_mc;
+	void *ptr = NULL;
+	int pm4_dw = 256;
+	gsgpu_bo_handle handle;
+	gsgpu_context_handle context_handle;
+	struct gsgpu_cs_ib_info *ib_info;
+	struct gsgpu_cs_request *ibs_request;
+	gsgpu_bo_handle buf_handle;
+	gsgpu_va_handle va_handle;
+
+	pm4 = calloc(pm4_dw, sizeof(*pm4));
+	CU_ASSERT_NOT_EQUAL(pm4, NULL);
+
+	ib_info = calloc(1, sizeof(*ib_info));
+	CU_ASSERT_NOT_EQUAL(ib_info, NULL);
+
+	ibs_request = calloc(1, sizeof(*ibs_request));
+	CU_ASSERT_NOT_EQUAL(ibs_request, NULL);
+
+	r = gsgpu_cs_ctx_create(device_handle, &context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = posix_memalign(&ptr, BUFFER_ALIGN, BUFFER_SIZE);
+	if (r) {
+		fprintf(stderr, "gsgpu_userptr_test : posix_memalign alloc failed\r\n");
+		return;
+	}
+
+	CU_ASSERT_NOT_EQUAL(ptr, NULL);
+	memset(ptr, 0, BUFFER_SIZE);
+
+	r = gsgpu_create_bo_from_user_mem(device_handle,
+								ptr, BUFFER_SIZE, &buf_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_va_range_alloc(device_handle,
+						gsgpu_gpu_va_range_general,
+						BUFFER_SIZE, 1, 0, &bo_mc,
+						&va_handle, 0);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_bo_va_op(buf_handle, 0, BUFFER_SIZE, bo_mc, 0, GSGPU_VA_OP_MAP);
+	CU_ASSERT_EQUAL(r, 0);
+
+	handle = buf_handle;
+
+	i = 0;
+
+	pm4[i++] = GSPKT(GSGPU_CMD_WRITE, 3) | WRITE_DST_SEL(1) | WRITE_WAIT;
+	pm4[i++] = (uint32_t)(0xffffffff & bo_mc);
+	pm4[i++] = (uint32_t)((0xffffffff00000000 & bo_mc) >> 32);
+	pm4[i++] = 0xdeadbeaf;
+
+	if (!fork()) {
+		pm4[0] = 0x0;
+		exit(0);
+	}
+
+	gsgpu_test_exec_cs_helper(context_handle,
+						GSGPU_HW_IP_GFX, 0,
+						i, pm4,
+						1, &handle,
+						ib_info, ibs_request);
+
+	CU_ASSERT_EQUAL(((int*)ptr)[0], 0xdeadbeaf);
+
+	free(ibs_request);
+	free(ib_info);
+	free(pm4);
+
+	r = gsgpu_bo_va_op(buf_handle, 0, BUFFER_SIZE, bo_mc, 0, GSGPU_VA_OP_UNMAP);
+	CU_ASSERT_EQUAL(r, 0);
+	r = gsgpu_va_range_free(va_handle);
+	CU_ASSERT_EQUAL(r, 0);
+	r = gsgpu_bo_free(buf_handle);
+	CU_ASSERT_EQUAL(r, 0);
+	free(ptr);
+
+	r = gsgpu_cs_ctx_free(context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+	wait(NULL);
+}
+
+static void gsgpu_sync_dependency_test(void)
+{
+}
diff --git a/tests/gsgpu/bo_tests.c b/tests/gsgpu/bo_tests.c
new file mode 100644
index 00000000..72f9cb68
--- /dev/null
+++ b/tests/gsgpu/bo_tests.c
@@ -0,0 +1,269 @@
+/*
+ * Copyright (C) 2020 Loongson Technology Co., Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+*/
+
+#ifdef HAVE_CONFIG_H
+#include "config.h"
+#endif
+
+#include <stdio.h>
+
+#include "CUnit/Basic.h"
+
+#include "gsgpu_test.h"
+#include "gsgpu_drm.h"
+
+#define BUFFER_SIZE (16*1024)
+#define BUFFER_ALIGN (16*1024)
+
+static gsgpu_device_handle device_handle;
+static uint32_t major_version;
+static uint32_t minor_version;
+
+static gsgpu_bo_handle buffer_handle;
+static uint64_t virtual_mc_base_address;
+static gsgpu_va_handle va_handle;
+
+static void gsgpu_bo_export_import(void);
+static void gsgpu_bo_metadata(void);
+static void gsgpu_bo_map_unmap(void);
+static void gsgpu_memory_alloc(void);
+static void gsgpu_mem_fail_alloc(void);
+
+CU_TestInfo bo_tests[] = {
+	{ "Export/Import",  gsgpu_bo_export_import },
+	{ "Metadata",  gsgpu_bo_metadata },
+	{ "CPU map/unmap",  gsgpu_bo_map_unmap },
+	{ "Memory alloc Test",  gsgpu_memory_alloc },
+	{ "Memory fail alloc Test",  gsgpu_mem_fail_alloc },
+	CU_TEST_INFO_NULL,
+};
+
+int suite_bo_tests_init(void)
+{
+	struct gsgpu_bo_alloc_request req = {0};
+	gsgpu_bo_handle buf_handle;
+	uint64_t va;
+	int r;
+
+	r = gsgpu_device_initialize(drm_gsgpu[0], &major_version,
+				  &minor_version, &device_handle);
+	if (r) {
+		if ((r == -EACCES) && (errno == EACCES))
+			printf("\n\nError:%s. "
+				"Hint:Try to run this test program as root.",
+				strerror(errno));
+
+		return CUE_SINIT_FAILED;
+	}
+
+	req.alloc_size = BUFFER_SIZE;
+	req.phys_alignment = BUFFER_ALIGN;
+	req.preferred_heap = GSGPU_GEM_DOMAIN_GTT;
+
+	r = gsgpu_bo_alloc(device_handle, &req, &buf_handle);
+	if (r)
+		return CUE_SINIT_FAILED;
+
+	r = gsgpu_va_range_alloc(device_handle,
+				  gsgpu_gpu_va_range_general,
+				  BUFFER_SIZE, BUFFER_ALIGN, 0,
+				  &va, &va_handle, 0);
+	if (r)
+		goto error_va_alloc;
+
+	r = gsgpu_bo_va_op(buf_handle, 0, BUFFER_SIZE, va, 0, GSGPU_VA_OP_MAP);
+	if (r)
+		goto error_va_map;
+
+	buffer_handle = buf_handle;
+	virtual_mc_base_address = va;
+
+	return CUE_SUCCESS;
+
+error_va_map:
+	gsgpu_va_range_free(va_handle);
+
+error_va_alloc:
+	gsgpu_bo_free(buf_handle);
+	return CUE_SINIT_FAILED;
+}
+
+int suite_bo_tests_clean(void)
+{
+	int r;
+
+	r = gsgpu_bo_va_op(buffer_handle, 0, BUFFER_SIZE,
+			    virtual_mc_base_address, 0,
+			    GSGPU_VA_OP_UNMAP);
+	if (r)
+		return CUE_SCLEAN_FAILED;
+
+	r = gsgpu_va_range_free(va_handle);
+	if (r)
+		return CUE_SCLEAN_FAILED;
+
+	r = gsgpu_bo_free(buffer_handle);
+	if (r)
+		return CUE_SCLEAN_FAILED;
+
+	r = gsgpu_device_deinitialize(device_handle);
+	if (r)
+		return CUE_SCLEAN_FAILED;
+
+	return CUE_SUCCESS;
+}
+
+static void gsgpu_bo_export_import_do_type(enum gsgpu_bo_handle_type type)
+{
+	struct gsgpu_bo_import_result res = {0};
+	uint32_t shared_handle;
+	int r;
+
+	r = gsgpu_bo_export(buffer_handle, type, &shared_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_bo_import(device_handle, type, shared_handle, &res);
+	CU_ASSERT_EQUAL(r, 0);
+
+	CU_ASSERT_EQUAL(res.buf_handle, buffer_handle);
+	CU_ASSERT_EQUAL(res.alloc_size, BUFFER_SIZE);
+
+	r = gsgpu_bo_free(res.buf_handle);
+	CU_ASSERT_EQUAL(r, 0);
+}
+
+static void gsgpu_bo_export_import(void)
+{
+	if (open_render_node) {
+		printf("(DRM render node is used. Skip export/Import test) ");
+		return;
+	}
+
+	gsgpu_bo_export_import_do_type(gsgpu_bo_handle_type_gem_flink_name);
+	gsgpu_bo_export_import_do_type(gsgpu_bo_handle_type_dma_buf_fd);
+}
+
+static void gsgpu_bo_metadata(void)
+{
+	struct gsgpu_bo_metadata meta = {0};
+	struct gsgpu_bo_info info = {0};
+	int r;
+
+	meta.size_metadata = 4;
+	meta.umd_metadata[0] = 0xdeadbeef;
+	meta.flags= 3 << 9;
+
+	r = gsgpu_bo_set_metadata(buffer_handle, &meta);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_bo_query_info(buffer_handle, &info);
+	CU_ASSERT_EQUAL(r, 0);
+
+	CU_ASSERT_EQUAL(info.metadata.size_metadata, 4);
+	CU_ASSERT_EQUAL(info.metadata.umd_metadata[0], 0xdeadbeef);
+}
+
+static void gsgpu_bo_map_unmap(void)
+{
+	uint32_t *ptr;
+	int i, r;
+
+	r = gsgpu_bo_cpu_map(buffer_handle, (void **)&ptr);
+	CU_ASSERT_EQUAL(r, 0);
+	CU_ASSERT_NOT_EQUAL(ptr, NULL);
+
+	for (i = 0; i < (BUFFER_SIZE / 4); ++i)
+		ptr[i] = 0xdeadbeef;
+
+	r = gsgpu_bo_cpu_unmap(buffer_handle);
+	CU_ASSERT_EQUAL(r, 0);
+}
+
+static void gsgpu_memory_alloc(void)
+{
+	gsgpu_bo_handle bo;
+	gsgpu_va_handle va_handle;
+	uint64_t bo_mc;
+	int r;
+
+	/* Test visible VRAM */
+	bo = gpu_mem_alloc(device_handle,
+			BUFFER_SIZE, BUFFER_ALIGN,
+			GSGPU_GEM_DOMAIN_VRAM,
+			GSGPU_GEM_CREATE_CPU_ACCESS_REQUIRED,
+			&bo_mc, &va_handle);
+
+	r = gpu_mem_free(bo, va_handle, bo_mc, BUFFER_SIZE);
+	CU_ASSERT_EQUAL(r, 0);
+
+	/* Test invisible VRAM */
+	bo = gpu_mem_alloc(device_handle,
+			BUFFER_SIZE, BUFFER_ALIGN,
+			GSGPU_GEM_DOMAIN_VRAM,
+			GSGPU_GEM_CREATE_NO_CPU_ACCESS,
+			&bo_mc, &va_handle);
+
+	r = gpu_mem_free(bo, va_handle, bo_mc, BUFFER_SIZE);
+	CU_ASSERT_EQUAL(r, 0);
+
+	/* Test GART Cacheable */
+	bo = gpu_mem_alloc(device_handle,
+			BUFFER_SIZE, BUFFER_ALIGN,
+			GSGPU_GEM_DOMAIN_GTT,
+			0, &bo_mc, &va_handle);
+
+	r = gpu_mem_free(bo, va_handle, bo_mc, BUFFER_SIZE);
+	CU_ASSERT_EQUAL(r, 0);
+
+	/* Test GART USWC */
+	bo = gpu_mem_alloc(device_handle,
+			BUFFER_SIZE, BUFFER_ALIGN,
+			GSGPU_GEM_DOMAIN_GTT,
+			GSGPU_GEM_CREATE_CPU_GTT_USWC,
+			&bo_mc, &va_handle);
+
+	r = gpu_mem_free(bo, va_handle, bo_mc, BUFFER_SIZE);
+	CU_ASSERT_EQUAL(r, 0);
+}
+
+static void gsgpu_mem_fail_alloc(void)
+{
+	gsgpu_bo_handle bo = NULL;
+	int r;
+	struct gsgpu_bo_alloc_request req = {0};
+	gsgpu_bo_handle buf_handle;
+
+	/* Test impossible mem allocation, 1TB */
+	req.alloc_size = 0xE8D4A51000;
+	req.phys_alignment = BUFFER_ALIGN;
+	req.preferred_heap = GSGPU_GEM_DOMAIN_VRAM;
+	req.flags = GSGPU_GEM_CREATE_NO_CPU_ACCESS;
+
+	r = gsgpu_bo_alloc(device_handle, &req, &buf_handle);
+	CU_ASSERT_EQUAL(r, -ENOMEM);
+
+	if (!r) {
+		r = gsgpu_bo_free(bo);
+		CU_ASSERT_EQUAL(r, 0);
+	}
+}
diff --git a/tests/gsgpu/deadlock_tests.c b/tests/gsgpu/deadlock_tests.c
new file mode 100755
index 00000000..b916ab2d
--- /dev/null
+++ b/tests/gsgpu/deadlock_tests.c
@@ -0,0 +1,219 @@
+/*
+ * Copyright (C) 2020 Loongson Technology Co., Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+*/
+
+#ifdef HAVE_CONFIG_H
+#include "config.h"
+#endif
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <unistd.h>
+#ifdef HAVE_ALLOCA_H
+# include <alloca.h>
+#endif
+
+#include "CUnit/Basic.h"
+
+#include "gsgpu_test.h"
+#include "gsgpu_drm.h"
+#include "gsgpu_internal.h"
+
+#include <pthread.h>
+
+
+/*
+ * This defines the delay in MS after which memory location designated for
+ * compression against reference value is written to, unblocking command
+ * processor
+ */
+#define WRITE_MEM_ADDRESS_DELAY_MS 100
+
+static  gsgpu_device_handle device_handle;
+static  uint32_t  major_version;
+static  uint32_t  minor_version;
+
+static pthread_t stress_thread;
+static uint32_t *ptr = NULL;
+
+static void gsgpu_deadlock_helper(unsigned ip_type);
+static void gsgpu_deadlock_gfx(void);
+static void gsgpu_deadlock_dma(void);
+
+CU_BOOL suite_deadlock_tests_enable(void)
+{
+	CU_BOOL enable = CU_TRUE;
+
+	if (gsgpu_device_initialize(drm_gsgpu[0], &major_version,
+					     &minor_version, &device_handle))
+		return CU_FALSE;
+
+	if (gsgpu_device_deinitialize(device_handle))
+		return CU_FALSE;
+
+	return enable;
+}
+
+int suite_deadlock_tests_init(void)
+{
+	int r;
+
+	r = gsgpu_device_initialize(drm_gsgpu[0], &major_version,
+				   &minor_version, &device_handle);
+
+	if (r) {
+		if ((r == -EACCES) && (errno == EACCES))
+			printf("\n\nError:%s. "
+				"Hint:Try to run this test program as root.",
+				strerror(errno));
+		return CUE_SINIT_FAILED;
+	}
+
+	return CUE_SUCCESS;
+}
+
+int suite_deadlock_tests_clean(void)
+{
+	int r = gsgpu_device_deinitialize(device_handle);
+
+	if (r == 0)
+		return CUE_SUCCESS;
+	else
+		return CUE_SCLEAN_FAILED;
+}
+
+
+CU_TestInfo deadlock_tests[] = {
+	{ "gfx ring block test",  gsgpu_deadlock_gfx },
+	{ "dma ring block test",  gsgpu_deadlock_dma },
+	CU_TEST_INFO_NULL,
+};
+
+static void *write_mem_address(void *data)
+{
+	int i;
+
+	/* useconds_t range is [0, 1,000,000] so use loop for waits > 1s */
+	for (i = 0; i < WRITE_MEM_ADDRESS_DELAY_MS; i++)
+		usleep(1000);
+
+        while(NULL == ptr) {
+                usleep(1000);
+        }
+
+	ptr[256] = 0x1;
+
+	return 0;
+}
+
+static void gsgpu_deadlock_dma(void)
+{
+	gsgpu_deadlock_helper(GSGPU_HW_IP_DMA);
+}
+static void gsgpu_deadlock_gfx(void)
+{
+	gsgpu_deadlock_helper(GSGPU_HW_IP_GFX);
+}
+
+static void gsgpu_deadlock_helper(unsigned ip_type)
+{
+	gsgpu_context_handle context_handle;
+	gsgpu_bo_handle ib_result_handle = NULL;
+	void *ib_result_cpu;
+	uint64_t ib_result_mc_address = 0;
+	struct gsgpu_cs_request ibs_request;
+	struct gsgpu_cs_ib_info ib_info;
+	struct gsgpu_cs_fence fence_status;
+	uint32_t expired;
+	int i, r;
+	gsgpu_bo_list_handle bo_list;
+	gsgpu_va_handle va_handle = NULL;
+
+	r = pthread_create(&stress_thread, NULL, write_mem_address, NULL);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_cs_ctx_create(device_handle, &context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_bo_alloc_and_map(device_handle, 4096, 4096,
+			GSGPU_GEM_DOMAIN_GTT, 0,
+						    &ib_result_handle, &ib_result_cpu,
+						    &ib_result_mc_address, &va_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_get_bo_list(device_handle, ib_result_handle, NULL,
+			       &bo_list);
+	CU_ASSERT_EQUAL(r, 0);
+
+	ptr = ib_result_cpu;
+
+	ptr[0] = GSPKT(GSGPU_CMD_POLL, 6) | 4 << 8 /* not equal */ | 1 << 12 /* reg/mem */;
+	ptr[1] = (ib_result_mc_address + 256*4) & 0xffffffff;
+	ptr[2] = ((ib_result_mc_address + 256*4) >> 32) & 0xffffffff;
+	ptr[3] = 0x00000000; /* reference value */
+	ptr[4] = 0xffffffff; /* and mask */
+	ptr[5] = (0xfff) << 16 | 2; /* retry count, poll interval */
+	ptr[6] = 0x00;
+
+	for (i = 7; i < 16; ++i)
+		ptr[i] = 0x80; //NOP
+
+	ptr[256] = 0x0; /* the memory we wait on to change */
+
+	memset(&ib_info, 0, sizeof(struct gsgpu_cs_ib_info));
+	ib_info.ib_mc_address = ib_result_mc_address;
+	ib_info.size = 16;
+
+	memset(&ibs_request, 0, sizeof(struct gsgpu_cs_request));
+	ibs_request.ip_type = ip_type;
+	ibs_request.ring = 0;
+	ibs_request.number_of_ibs = 1;
+	ibs_request.ibs = &ib_info;
+	ibs_request.resources = bo_list;
+	ibs_request.fence_info.handle = NULL;
+
+	r = gsgpu_cs_submit(context_handle, 0,&ibs_request, 1);
+	CU_ASSERT_EQUAL((r == 0 || r == -ECANCELED), 1);
+
+	memset(&fence_status, 0, sizeof(struct gsgpu_cs_fence));
+	fence_status.context = context_handle;
+	fence_status.ip_type = ip_type;
+	fence_status.ip_instance = 0;
+	fence_status.ring = 0;
+	fence_status.fence = ibs_request.seq_no;
+
+	r = gsgpu_cs_query_fence_status(&fence_status,
+			GSGPU_TIMEOUT_INFINITE,0, &expired);
+	CU_ASSERT_EQUAL((r == 0 || r == -ECANCELED), 1);
+
+	pthread_join(stress_thread, NULL);
+
+	r = gsgpu_bo_list_destroy(bo_list);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_bo_unmap_and_free(ib_result_handle, va_handle,
+				     ib_result_mc_address, 4096);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_cs_ctx_free(context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+}
diff --git a/tests/gsgpu/dma_tests.c b/tests/gsgpu/dma_tests.c
new file mode 100644
index 00000000..1c882a17
--- /dev/null
+++ b/tests/gsgpu/dma_tests.c
@@ -0,0 +1,1231 @@
+/*
+ * Copyright (C) 2020 Loongson Technology Co., Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+*/
+
+#ifdef HAVE_CONFIG_H
+#include "config.h"
+#endif
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <unistd.h>
+
+#ifdef HAVE_ALLOCA_H
+# include <alloca.h>
+#endif
+
+#include <sys/wait.h>
+
+#include <stdio.h>
+
+#include "CUnit/Basic.h"
+
+#include "gsgpu_test.h"
+#include "gsgpu_drm.h"
+
+extern  gsgpu_device_handle device_handle;
+extern  uint32_t  major_version;
+extern  uint32_t  minor_version;
+extern  uint32_t  family_id;
+
+#define TABLE_SIZE(x)   (sizeof(x)/sizeof(x[0]))
+#define BUFFER_SIZE (16 * 1024)
+#define BUFFER_ALIGN (16 * 1024)
+#define MSAA_PIXEL (2*2)
+#define MINIMUM 0
+#define EQUAL_LENGTH 1
+#define UNEQUAL_LENGTH 2
+
+union pixel_rgba8 {
+        uint32_t dw;
+        struct {
+                uint8_t a;
+                uint8_t b;
+                uint8_t g;
+                uint8_t r;
+        }channel;
+};
+
+struct gsgpu_xdma_cmd_desc {
+	union {
+		uint32_t val;
+		struct {
+			uint32_t opcode : 8;
+			uint32_t format : 8;
+			uint32_t length : 8;
+			uint32_t opmode : 4;
+			uint32_t sub_mode : 4;
+		}sec;
+	}header;
+	struct {
+		union {
+			uint32_t data_size;
+			struct {
+				uint16_t width;
+				uint16_t height;
+			}size;
+		};
+		uint32_t src_lo;
+		uint32_t src_hi;
+		uint32_t dst_lo;
+		uint32_t dst_hi;
+		uint32_t src_stride;
+		uint32_t dst_stride;
+		union {
+			uint32_t val;
+			struct {
+				uint32_t    : 12;
+				uint32_t rd : 5;
+				uint32_t rd_en : 1;
+				uint32_t    : 2;
+				uint32_t wr : 5;
+				uint32_t wr_en : 1;
+				uint32_t    : 0;
+			};
+		}sema;
+
+	}body;
+};
+
+static void generate_once_mipmap(union pixel_rgba8 * const src, union pixel_rgba8 * const dst, const uint16_t width, const uint16_t height);
+static int verify_msaa_resolve(union pixel_rgba8 *src, union pixel_rgba8 *dst);
+static void verify_mipmaps(union pixel_rgba8 * const src, union pixel_rgba8 * const dst, const uint16_t width, const uint16_t height);
+static void *acquire_semaphore_thread_entry(void *number);
+static void *release_semaphore_thread_entry(void *number);
+static uint32_t get_pixel_depth(uint32_t format);
+static union pixel_rgba8 *get_tile_pixel(union pixel_rgba8 * const base, const int x, const int y, const int pitch);
+
+extern void gsgpu_command_submission_write_linear_helper(unsigned ip_type);
+static void gsgpu_command_submission_const_fill_helper(unsigned ip_type);
+static void gsgpu_command_submission_copy_linear_helper(unsigned ip_type);
+static void gsgpu_command_submission_copy_tiled_helper(unsigned ip_type);
+static void gsgpu_command_submission_msaa_resolve_helper(unsigned ip_type);
+static void gsgpu_command_submission_mipmap_generate_helper(unsigned ip_type, uint16_t width, uint16_t height);
+static void gsgpu_command_submission_sdma_semaphore_helper(unsigned ip_type);
+extern void gsgpu_test_exec_cs_helper(gsgpu_context_handle context_handle,
+                                       unsigned ip_type,
+                                       int instance, int pm4_dw, uint32_t *pm4_src,
+                                       int res_cnt, gsgpu_bo_handle *resources,
+                                       struct gsgpu_cs_ib_info *ib_info,
+                                       struct gsgpu_cs_request *ibs_request);
+
+static void gsgpu_command_submission_sdma_write_linear(void);
+static void gsgpu_command_submission_sdma_const_fill(void);
+static void gsgpu_command_submission_sdma_copy_linear(void);
+static void gsgpu_command_submission_sdma_copy_tiled(void);
+static void gsgpu_command_submission_sdma_msaa_resolve(void);
+static void gsgpu_command_submission_sdma_mipmap_generate(void);
+static void gsgpu_command_submission_sdma_semaphore(void);
+
+CU_TestInfo dma_tests[] = {
+	{ "Command submission Test (DMA write)", gsgpu_command_submission_sdma_write_linear },
+	{ "Command submission Test (DMA fill)", gsgpu_command_submission_sdma_const_fill },
+	{ "Command submission Test (DMA copy linear)", gsgpu_command_submission_sdma_copy_linear },
+	{ "Command submission Test (DMA copy tiled)", gsgpu_command_submission_sdma_copy_tiled },
+	{ "Command submission Test (DMA copy msaa)", gsgpu_command_submission_sdma_msaa_resolve },
+	{ "Command submission Test (DMA copy mipmap)", gsgpu_command_submission_sdma_mipmap_generate },
+	{ "Command submission Test (semaphore)", gsgpu_command_submission_sdma_semaphore },
+	CU_TEST_INFO_NULL,
+};
+
+int suite_dma_tests_init(void)
+{
+	struct gsgpu_gpu_info gpu_info = {0};
+	int r;
+
+	r = gsgpu_device_initialize(drm_gsgpu[0], &major_version,
+				   &minor_version, &device_handle);
+
+	if (r) {
+		if ((r == -EACCES) && (errno == EACCES))
+			printf("\n\nError:%s. "
+				"Hint:Try to run this test program as root.",
+				strerror(errno));
+		return CUE_SINIT_FAILED;
+	}
+
+	r = gsgpu_query_gpu_info(device_handle, &gpu_info);
+	if (r)
+		return CUE_SINIT_FAILED;
+
+	family_id = gpu_info.family_id;
+
+	return CUE_SUCCESS;
+}
+
+int suite_dma_tests_clean(void)
+{
+	int r = gsgpu_device_deinitialize(device_handle);
+
+	if (r == 0)
+		return CUE_SUCCESS;
+	else
+		return CUE_SCLEAN_FAILED;
+}
+
+static void gsgpu_command_submission_sdma_write_linear(void)
+{
+	gsgpu_command_submission_write_linear_helper(GSGPU_HW_IP_DMA);
+}
+
+static void gsgpu_command_submission_sdma_const_fill(void)
+{
+	gsgpu_command_submission_const_fill_helper(GSGPU_HW_IP_DMA);
+}
+
+static void gsgpu_command_submission_const_fill_helper(unsigned ip_type)
+{
+	const int pm4_dw = 256;
+	gsgpu_context_handle context_handle;
+	gsgpu_bo_handle bo;
+	gsgpu_bo_handle *resources;
+	uint32_t *pm4;
+	struct gsgpu_cs_ib_info *ib_info;
+	struct gsgpu_cs_request *ibs_request;
+	uint64_t bo_mc;
+	volatile uint32_t *bo_cpu;
+	int i, j, r, align_dw;
+	uint64_t gtt_flags[1] = {0};
+	gsgpu_va_handle va_handle;
+	struct drm_gsgpu_info_hw_ip hw_ip_info;
+
+	/* init object of command package */
+	struct gsgpu_xdma_cmd_desc cmd_buffer[] = {
+		{
+			.header.sec = {
+				GSGPU_CMD_XDMA_COPY,
+				GSGPU_CMD_XDMA_FORMAT_RGBA8,
+				GSGPU_CMD_XDMA_BODY_NR,
+				GSGPU_CMD_XDMA_MODE_MEMSET,
+				GSGPU_CMD_XDMA_SUB_MODE_DEFAULT,
+			},
+			.body = {
+				.size.width = 16 * 1024,
+				.size.height = 1,
+				.src_lo = 0xdeadbeaf,
+				.src_hi = 0,
+				.dst_lo = 0,
+				.dst_hi = 0,
+				.src_stride = 0,
+				.dst_stride = 16 * 1024,
+				.sema.val = 0,
+			},
+		},
+
+	};
+
+	pm4 = calloc(pm4_dw, sizeof(*pm4));
+	CU_ASSERT_NOT_EQUAL(pm4, NULL);
+
+	ib_info = calloc(1, sizeof(*ib_info));
+	CU_ASSERT_NOT_EQUAL(ib_info, NULL);
+
+	ibs_request = calloc(1, sizeof(*ibs_request));
+	CU_ASSERT_NOT_EQUAL(ibs_request, NULL);
+
+	r = gsgpu_query_hw_ip_info(device_handle, ip_type, 0, &hw_ip_info);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_cs_ctx_create(device_handle, &context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+	/* prepare resource */
+	resources = calloc(1, sizeof(gsgpu_bo_handle));
+	CU_ASSERT_NOT_EQUAL(resources, NULL);
+
+	for (int index = 0; index < TABLE_SIZE(cmd_buffer); index++) {
+		int sdma_write_length = get_pixel_depth(cmd_buffer[index].header.sec.format) * cmd_buffer[index].body.size.width * cmd_buffer[index].body.size.height;
+
+		/* allocate bo for DMA use */
+		r = gsgpu_bo_alloc_and_map(device_handle,
+					    sdma_write_length, BUFFER_ALIGN,
+					    GSGPU_GEM_DOMAIN_GTT,
+					    0, &bo,
+					    (void**)&bo_cpu, &bo_mc,
+					    &va_handle);
+		CU_ASSERT_EQUAL(r, 0);
+
+		/* clear bo */
+		memset((void*)bo_cpu, 0, sdma_write_length);
+
+		resources[0] = bo;
+
+		/* fulfill cmd packet: test SDMA const fill */
+		cmd_buffer[index].body.dst_lo = (uint32_t)(0xffffffff & bo_mc);
+		cmd_buffer[index].body.dst_hi = (uint32_t)((0xffffffff00000000 & bo_mc) >> 32);
+
+		i = sizeof(cmd_buffer[index]);
+		memcpy(pm4, &cmd_buffer[index], i);
+
+		i = (cmd_buffer[index].header.sec.length + 1);
+
+		/* ib cmd packet align */
+		align_dw = hw_ip_info.ib_size_alignment - (i & (hw_ip_info.ib_size_alignment - 1));
+		for (j = 0; j < align_dw; j++) {
+			pm4[i++] = GSGPU_CMD_NOP;
+		}
+
+		gsgpu_test_exec_cs_helper(context_handle,
+					   ip_type, 0,
+					   i, pm4,
+					   1, resources,
+					   ib_info, ibs_request);
+
+		/* verify if SDMA test result meets with expected */
+		i = 0;
+		while (i < sdma_write_length / get_pixel_depth(cmd_buffer[index].header.sec.format)) {
+			CU_ASSERT_EQUAL(bo_cpu[i++], 0xdeadbeaf);
+		}
+
+		r = gsgpu_bo_unmap_and_free(bo, va_handle, bo_mc,
+					     sdma_write_length);
+		CU_ASSERT_EQUAL(r, 0);
+	}
+
+	/* clean resources */
+	free(resources);
+	free(ibs_request);
+	free(ib_info);
+	free(pm4);
+
+	/* end of test */
+	r = gsgpu_cs_ctx_free(context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+}
+
+
+static void gsgpu_command_submission_sdma_copy_linear(void)
+{
+	gsgpu_command_submission_copy_linear_helper(GSGPU_HW_IP_DMA);
+}
+
+static void gsgpu_command_submission_copy_linear_helper(unsigned ip_type)
+{
+	const int pm4_dw = 256;
+	gsgpu_context_handle context_handle = NULL;
+	gsgpu_bo_handle bo1 = NULL, bo2 = NULL;
+	gsgpu_bo_handle *resources;
+	uint32_t *pm4;
+	struct gsgpu_cs_ib_info *ib_info;
+	struct gsgpu_cs_request *ibs_request;
+	uint64_t bo1_mc, bo2_mc;
+	volatile unsigned char *bo1_cpu, *bo2_cpu;
+	int i, j, r, ring_id, align_dw;
+	uint64_t gtt_flags[1] = {0};
+	gsgpu_va_handle bo1_va_handle = NULL, bo2_va_handle = NULL;
+	struct drm_gsgpu_info_hw_ip hw_ip_info;
+
+	/* init object of command package */
+	struct gsgpu_xdma_cmd_desc cmd_buffer[] = {
+		{
+			.header.sec = {
+				GSGPU_CMD_XDMA_COPY,
+				GSGPU_CMD_XDMA_FORMAT_RGBA16,
+				GSGPU_CMD_XDMA_BODY_NR,
+				GSGPU_CMD_XDMA_MODE_L2L,
+				GSGPU_CMD_XDMA_SUB_MODE_DEFAULT,
+			},
+			.body = {
+				.size.width = 1024 / 8,
+				.size.height = 1,
+				.src_lo = 0,
+				.src_hi = 0,
+				.dst_lo = 0,
+				.dst_hi = 0,
+				.src_stride = 1024,
+				.dst_stride = 1024,
+				.sema.rd = 0,
+				.sema.rd_en = 0,
+				.sema.wr = 0,
+				.sema.wr_en = 0,
+			},
+		},
+		{
+			.header.sec = {
+				GSGPU_CMD_XDMA_COPY,
+				GSGPU_CMD_XDMA_FORMAT_RGBA8,
+				GSGPU_CMD_XDMA_BODY_NR,
+				GSGPU_CMD_XDMA_MODE_L2L,
+				GSGPU_CMD_XDMA_SUB_MODE_DEFAULT,
+			},
+			.body = {
+				.size.width = 128,
+				.size.height = 1,
+				.src_lo = 0,
+				.src_hi = 0,
+				.dst_lo = 0,
+				.dst_hi = 0,
+				.src_stride = 128 * 4,
+				.dst_stride = 128 * 4,
+				.sema.rd = 0,
+				.sema.rd_en = 0,
+				.sema.wr = 0,
+				.sema.wr_en = 0,
+			},
+		},
+	};
+
+	pm4 = calloc(pm4_dw, sizeof(*pm4));
+	CU_ASSERT_NOT_EQUAL(pm4, NULL);
+
+	ib_info = calloc(1, sizeof(*ib_info));
+	CU_ASSERT_NOT_EQUAL(ib_info, NULL);
+
+	ibs_request = calloc(1, sizeof(*ibs_request));
+	CU_ASSERT_NOT_EQUAL(ibs_request, NULL);
+
+	r = gsgpu_query_hw_ip_info(device_handle, ip_type, 0, &hw_ip_info);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_cs_ctx_create(device_handle, &context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+	/* prepare resource */
+	resources = calloc(2, sizeof(gsgpu_bo_handle));
+	CU_ASSERT_NOT_EQUAL(resources, NULL);
+
+	for (int index = 0; index < TABLE_SIZE(cmd_buffer); index++) {
+		int sdma_write_length = get_pixel_depth(cmd_buffer[index].header.sec.format) * cmd_buffer[index].body.size.width * cmd_buffer[index].body.size.height;
+		/* allocate UC bo1for sDMA use */
+		r = gsgpu_bo_alloc_and_map(device_handle,
+					    sdma_write_length, 4096,
+					    GSGPU_GEM_DOMAIN_GTT,
+					    gtt_flags[0], &bo1,
+					    (void**)&bo1_cpu, &bo1_mc,
+					    &bo1_va_handle);
+		CU_ASSERT_EQUAL(r, 0);
+
+		/* set bo1 */
+		memset((void*)bo1_cpu, 0xaa, sdma_write_length);
+
+		/* allocate UC bo2 for sDMA use */
+		r = gsgpu_bo_alloc_and_map(device_handle,
+					    sdma_write_length, 4096,
+					    GSGPU_GEM_DOMAIN_GTT,
+					    gtt_flags[0], &bo2,
+					    (void**)&bo2_cpu, &bo2_mc,
+					    &bo2_va_handle);
+		CU_ASSERT_EQUAL(r, 0);
+
+		/* clear bo2 */
+		memset((void*)bo2_cpu, 0, sdma_write_length);
+
+		resources[0] = bo1;
+		resources[1] = bo2;
+
+		/* fulfill PM4: test DMA copy linear */
+		cmd_buffer[index].body.src_lo = (uint32_t)(0xffffffff & bo1_mc);
+		cmd_buffer[index].body.src_hi = (uint32_t)((0xffffffff00000000 & bo1_mc) >> 32);
+		cmd_buffer[index].body.dst_lo = (uint32_t)(0xffffffff & bo2_mc);
+		cmd_buffer[index].body.dst_hi = (uint32_t)((0xffffffff00000000 & bo2_mc) >> 32);
+
+		i = sizeof(cmd_buffer[index]);
+		memcpy(pm4, &cmd_buffer[index], i);
+
+		i = (cmd_buffer[index].header.sec.length + 1);
+
+		/* ib cmd packet align */
+		align_dw = hw_ip_info.ib_size_alignment - (i & (hw_ip_info.ib_size_alignment - 1));
+		for (j = 0; j < align_dw; j++) {
+			pm4[i++] = GSGPU_CMD_NOP;
+		}
+
+		gsgpu_test_exec_cs_helper(context_handle,
+					   ip_type, 0,
+					   i, pm4,
+					   2, resources,
+					   ib_info, ibs_request);
+
+		/* verify if SDMA test result meets with expected */
+		i = 0;
+		while(i < sdma_write_length) {
+			CU_ASSERT_EQUAL(bo2_cpu[i++], 0xaa);
+		}
+		r = gsgpu_bo_unmap_and_free(bo1, bo1_va_handle, bo1_mc,
+					     sdma_write_length);
+		CU_ASSERT_EQUAL(r, 0);
+		r = gsgpu_bo_unmap_and_free(bo2, bo2_va_handle, bo2_mc,
+					     sdma_write_length);
+		CU_ASSERT_EQUAL(r, 0);
+	}
+	/* clean resources */
+	free(resources);
+	free(ibs_request);
+	free(ib_info);
+	free(pm4);
+
+	/* end of test */
+	r = gsgpu_cs_ctx_free(context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+}
+
+static int verify_msaa_resolve(union pixel_rgba8 *src, union pixel_rgba8 *dst)
+{
+        union pixel_rgba8 ret = {1};
+        uint32_t value_rgba[4] = {0, 0, 0, 0};
+        for (int index = 0; index < MSAA_PIXEL; index++) {
+                value_rgba[0] += (uint32_t)(*(src + index)).channel.r;
+                value_rgba[1] += (uint32_t)(*(src + index)).channel.g;
+                value_rgba[2] += (uint32_t)(*(src + index)).channel.b;
+                value_rgba[3] += (uint32_t)(*(src + index)).channel.a;
+        }
+        ret.channel.r = (uint8_t)(value_rgba[0]/(MSAA_PIXEL));
+        ret.channel.g = (uint8_t)(value_rgba[1]/(MSAA_PIXEL));
+        ret.channel.b = (uint8_t)(value_rgba[2]/(MSAA_PIXEL));
+        ret.channel.a = (uint8_t)(value_rgba[3]/(MSAA_PIXEL));
+
+        if (ret.dw != (*dst).dw) return 1;
+        return 0;
+}
+
+static void gsgpu_command_submission_msaa_resolve_helper(unsigned ip_type)
+{
+        const int pm4_dw = 256;
+        gsgpu_context_handle context_handle;
+        gsgpu_bo_handle bo1, bo2;
+        gsgpu_bo_handle *resources;
+        uint32_t *pm4;
+        struct gsgpu_cs_ib_info *ib_info;
+        struct gsgpu_cs_request *ibs_request;
+        uint64_t bo1_mc, bo2_mc;
+        volatile union pixel_rgba8 *bo1_cpu, *bo2_cpu;
+        int i, j, r, loop1, loop2, ring_id, align_dw;
+        uint64_t gtt_flags[2] = {0};
+        gsgpu_va_handle bo1_va_handle, bo2_va_handle;
+        struct drm_gsgpu_info_hw_ip hw_ip_info;
+
+	/* init object of command package */
+	struct gsgpu_xdma_cmd_desc cmd_buffer[] = {
+		{
+			.header.sec = {
+				GSGPU_CMD_XDMA_COPY,
+				GSGPU_CMD_XDMA_FORMAT_RGBA8,
+				GSGPU_CMD_XDMA_BODY_NR,
+				GSGPU_CMD_XDMA_MODE_MSAA,
+				GSGPU_CMD_XDMA_SUB_MODE_TILED_4X4,
+			},
+			.body = {
+				.size.width = 8,
+				.size.height = 8,
+				.src_lo = 0,
+				.src_hi = 0,
+				.dst_lo = 0,
+				.dst_hi = 0,
+				.src_stride = 8 * 4 * 4 * 4,
+				.dst_stride = 8 * 4 * 4,
+				.sema.rd = 0,
+				.sema.rd_en = 0,
+				.sema.wr = 0,
+				.sema.wr_en = 0,
+			},
+		},
+	};
+
+        pm4 = calloc(pm4_dw, sizeof(*pm4));
+        CU_ASSERT_NOT_EQUAL(pm4, NULL);
+
+        ib_info = calloc(1, sizeof(*ib_info));
+        CU_ASSERT_NOT_EQUAL(ib_info, NULL);
+
+        ibs_request = calloc(1, sizeof(*ibs_request));
+        CU_ASSERT_NOT_EQUAL(ibs_request, NULL);
+
+        r = gsgpu_query_hw_ip_info(device_handle, ip_type, 0, &hw_ip_info);
+        CU_ASSERT_EQUAL(r, 0);
+
+        r = gsgpu_cs_ctx_create(device_handle, &context_handle);
+        CU_ASSERT_EQUAL(r, 0);
+
+        /* prepare resource */
+        resources = calloc(2, sizeof(gsgpu_bo_handle));
+        CU_ASSERT_NOT_EQUAL(resources, NULL);
+	for (int index = 0; index < TABLE_SIZE(cmd_buffer); index++) {
+		int sdma_write_length = get_pixel_depth(cmd_buffer[index].header.sec.format) * cmd_buffer[index].body.size.width * cmd_buffer[index].body.size.height;
+		/* allocate UC bo1for sDMA use */
+		r = gsgpu_bo_alloc_and_map(device_handle,
+					    sdma_write_length, 4096,
+					    GSGPU_GEM_DOMAIN_GTT,
+					    gtt_flags[loop1], &bo1,
+					    (void**)&bo1_cpu, &bo1_mc,
+					    &bo1_va_handle);
+		CU_ASSERT_EQUAL(r, 0);
+
+		/* set bo1 */
+		for (int i = 0; i < sdma_write_length/4; i++) {
+			(*(bo1_cpu + i)).dw = rand() & 0xffffffff;
+		}
+
+		/* allocate UC bo2 for sDMA use */
+		r = gsgpu_bo_alloc_and_map(device_handle,
+					    sdma_write_length, 4096,
+					    GSGPU_GEM_DOMAIN_GTT,
+					    gtt_flags[loop2], &bo2,
+					    (void**)&bo2_cpu, &bo2_mc,
+					    &bo2_va_handle);
+		CU_ASSERT_EQUAL(r, 0);
+
+		/* clear bo2 */
+		memset((void*)bo2_cpu, 0, sdma_write_length);
+
+		resources[0] = bo1;
+		resources[1] = bo2;
+
+		/* fulfill PM4: test DMA msaa reslove */
+		cmd_buffer[index].body.src_lo = (uint32_t)(0xffffffff & bo1_mc);
+		cmd_buffer[index].body.src_hi = (uint32_t)((0xffffffff00000000 & bo1_mc) >> 32);
+		cmd_buffer[index].body.dst_lo = (uint32_t)(0xffffffff & bo2_mc);
+		cmd_buffer[index].body.dst_hi = (uint32_t)((0xffffffff00000000 & bo2_mc) >> 32);
+
+		i = sizeof(cmd_buffer[index]);
+		memcpy(pm4, &cmd_buffer[index], i);
+
+		i = (cmd_buffer[index].header.sec.length + 1);
+
+		/* ib cmd packet align */
+		align_dw = hw_ip_info.ib_size_alignment - (i & (hw_ip_info.ib_size_alignment - 1));
+		for (j = 0; j < align_dw; j++) {
+			pm4[i++] = GSGPU_CMD_NOP;
+		}
+
+		gsgpu_test_exec_cs_helper(context_handle,
+					   ip_type, ring_id,
+					   i, pm4,
+					   2, resources,
+					   ib_info, ibs_request);
+
+		/* verify if SDMA test result meets with expected */
+		for (int i = 0; i < sdma_write_length/4; i += MSAA_PIXEL) {
+			int ret = verify_msaa_resolve((union pixel_rgba8 *)(bo1_cpu + i), (union pixel_rgba8 *)(bo2_cpu + i/MSAA_PIXEL));
+			CU_ASSERT_EQUAL(ret, 0);
+		}
+		r = gsgpu_bo_unmap_and_free(bo1, bo1_va_handle, bo1_mc,
+					     sdma_write_length);
+		CU_ASSERT_EQUAL(r, 0);
+		r = gsgpu_bo_unmap_and_free(bo2, bo2_va_handle, bo2_mc,
+					     sdma_write_length);
+		CU_ASSERT_EQUAL(r, 0);
+	}
+
+        /* clean resources */
+        free(resources);
+        free(ibs_request);
+        free(ib_info);
+        free(pm4);
+
+        /* end of test */
+        r = gsgpu_cs_ctx_free(context_handle);
+        CU_ASSERT_EQUAL(r, 0);
+}
+
+static void gsgpu_command_submission_sdma_msaa_resolve(void)
+{
+        gsgpu_command_submission_msaa_resolve_helper(GSGPU_HW_IP_DMA);
+}
+
+
+union pixel_rgba8 *get_tile_pixel(union pixel_rgba8 * const base, const int x, const int y, const int pitch)
+{
+	union pixel_rgba8 * tile_base = base;
+	//for title only
+	int tile_x = x >> 2;
+	int tile_y = y >> 2;
+
+	tile_base = base + (tile_y * pitch * 4 + tile_x * 16);
+
+	int tile_offset = ( ((y>>1)&1) ? 0x8 : 0 )
+			| ( ((x>>1)&1) ? 0x4 : 0 )
+			| ( ((y>>0)&1) ? 0x2 : 0 )
+			| ( ((x>>0)&1) ? 0x1 : 0 );
+
+	return &tile_base[tile_offset];
+}
+
+static void verify_mipmaps(union pixel_rgba8 * const src, union pixel_rgba8 * const dst, const uint16_t width, const uint16_t height)
+{
+	union pixel_rgba8 *rptr = src;
+	union pixel_rgba8 *wptr = src;
+	union pixel_rgba8 *mip_test;
+	union pixel_rgba8 *mip_dma;
+
+	generate_once_mipmap(rptr, wptr, width, height);
+
+	for (int y = 0; y < height; y++){
+		for (int x = 0; x < width; x++){
+			mip_test = get_tile_pixel(src, x, y, width);
+			mip_dma = get_tile_pixel(dst, x, y, width);
+			CU_ASSERT_EQUAL((*mip_test).dw, (*mip_dma).dw);
+
+			if ((*mip_test).dw != (*mip_dma).dw) {
+				printf("\n");
+				printf("test[%d][%d]: r=%x, g=%x, b=%x, a=%x, value=%x \n", x, y, mip_test->channel.r, mip_test->channel.g, mip_test->channel.b, mip_test->channel.a, mip_test->dw);
+				printf("xdma[%d][%d]: r=%x, g=%x, b=%x, a=%x, value=%x \n", x, y, mip_dma->channel.r, mip_dma->channel.g, mip_dma->channel.b, mip_dma->channel.a, mip_dma->dw);
+			}
+		}
+	}
+}
+
+static void gsgpu_command_submission_mipmap_generate_helper(unsigned ip_type, uint16_t width, uint16_t height)
+{
+        const int pm4_dw = 256;
+        gsgpu_context_handle context_handle;
+        gsgpu_bo_handle bo1, bo2;
+        gsgpu_bo_handle *resources;
+        uint32_t *pm4;
+        struct gsgpu_cs_ib_info *ib_info;
+        struct gsgpu_cs_request *ibs_request;
+        uint64_t bo1_mc, bo2_mc;
+        volatile union pixel_rgba8 *bo1_cpu, *bo2_cpu;
+        int i, j, r, loop1, loop2, ring_id, align_dw;
+        uint64_t gtt_flags[2] = {0};
+        gsgpu_va_handle bo1_va_handle, bo2_va_handle;
+        struct drm_gsgpu_info_hw_ip hw_ip_info;
+	uint16_t dst_width = width;
+	uint16_t dst_height = height;
+	uint16_t src_width = dst_width * 2;
+	uint16_t src_height = dst_height * 2;
+
+	/*hardware will ride 4, software need round up*/
+	if (0 != (dst_height % 4))
+		dst_height = dst_height/4 + 1;
+	else
+		dst_height = dst_height/4;
+
+	/* init object of command package */
+	struct gsgpu_xdma_cmd_desc cmd_buffer[] = {
+		{
+			.header.sec = {
+				GSGPU_CMD_XDMA_COPY,
+				GSGPU_CMD_XDMA_FORMAT_RGBA8,
+				GSGPU_CMD_XDMA_BODY_NR,
+				GSGPU_CMD_XDMA_MODE_MIPMAP,
+				GSGPU_CMD_XDMA_SUB_MODE_TILED_4X4,
+			},
+			.body = {
+				.size.width = dst_width,
+				.size.height = dst_height,
+				.src_lo = 0,
+				.src_hi = 0,
+				.dst_lo = 0,
+				.dst_hi = 0,
+				.src_stride = src_width * 4 * 4,
+				.dst_stride = src_width / 2 * 4 * 4,
+				.sema.rd = 0,
+				.sema.rd_en = 0,
+				.sema.wr = 0,
+				.sema.wr_en = 0,
+			},
+		},
+	};
+
+	pm4 = calloc(pm4_dw, sizeof(*pm4));
+	CU_ASSERT_NOT_EQUAL(pm4, NULL);
+
+	ib_info = calloc(1, sizeof(*ib_info));
+	CU_ASSERT_NOT_EQUAL(ib_info, NULL);
+
+	ibs_request = calloc(1, sizeof(*ibs_request));
+	CU_ASSERT_NOT_EQUAL(ibs_request, NULL);
+
+	r = gsgpu_query_hw_ip_info(device_handle, ip_type, 0, &hw_ip_info);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_cs_ctx_create(device_handle, &context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+	/* prepare resource */
+	resources = calloc(2, sizeof(gsgpu_bo_handle));
+	CU_ASSERT_NOT_EQUAL(resources, NULL);
+
+	for (int index = 0; index < TABLE_SIZE(cmd_buffer); index++) {
+		int sdma_write_length = get_pixel_depth(cmd_buffer[index].header.sec.format) * src_width * src_height;
+		/* allocate UC bo1for sDMA use */
+		r = gsgpu_bo_alloc_and_map(device_handle,
+					    sdma_write_length, 4096,
+					    GSGPU_GEM_DOMAIN_GTT,
+					    gtt_flags[loop1], &bo1,
+					    (void**)&bo1_cpu, &bo1_mc,
+					    &bo1_va_handle);
+		CU_ASSERT_EQUAL(r, 0);
+
+
+		memset((void*)bo1_cpu, 0, sdma_write_length);
+
+		/* set bo1 */
+		for (int i = 0; i < sdma_write_length/get_pixel_depth(cmd_buffer[index].header.sec.format); i++) {
+			(*(bo1_cpu + i)).dw = rand() & 0xffffffff;
+		}
+
+		/* allocate UC bo2 for sDMA use */
+		r = gsgpu_bo_alloc_and_map(device_handle,
+					    sdma_write_length, 4096,
+					    GSGPU_GEM_DOMAIN_GTT,
+					    gtt_flags[loop2], &bo2,
+					    (void**)&bo2_cpu, &bo2_mc,
+					    &bo2_va_handle);
+		CU_ASSERT_EQUAL(r, 0);
+
+		/* clear bo2 */
+		memset((void*)bo2_cpu, 0, sdma_write_length);
+
+		resources[0] = bo1;
+		resources[1] = bo2;
+
+		/* fulfill PM4: test DMA mipmap reslove */
+		cmd_buffer[index].body.src_lo = (uint32_t)(0xffffffff & bo1_mc);
+		cmd_buffer[index].body.src_hi = (uint32_t)((0xffffffff00000000 & bo1_mc) >> 32);
+		cmd_buffer[index].body.dst_lo = (uint32_t)(0xffffffff & bo2_mc);
+		cmd_buffer[index].body.dst_hi = (uint32_t)((0xffffffff00000000 & bo2_mc) >> 32);
+
+		i = sizeof(cmd_buffer[index]);
+		memcpy(pm4, &cmd_buffer[index], i);
+
+		i = (cmd_buffer[index].header.sec.length + 1);
+
+		/* ib cmd packet align */
+		align_dw = hw_ip_info.ib_size_alignment - (i & (hw_ip_info.ib_size_alignment - 1));
+		for (j = 0; j < align_dw; j++) {
+			pm4[i++] = GSGPU_CMD_NOP;
+		}
+
+		gsgpu_test_exec_cs_helper(context_handle,
+					   ip_type, ring_id,
+					   i, pm4,
+					   2, resources,
+					   ib_info, ibs_request);
+
+		/* verify if SDMA test result meets with expected */
+		verify_mipmaps((union pixel_rgba8 *)bo1_cpu, (union pixel_rgba8 *)bo2_cpu, dst_width, src_height/2);
+
+		r = gsgpu_bo_unmap_and_free(bo1, bo1_va_handle, bo1_mc,
+					     sdma_write_length);
+		CU_ASSERT_EQUAL(r, 0);
+		r = gsgpu_bo_unmap_and_free(bo2, bo2_va_handle, bo2_mc,
+					     sdma_write_length);
+		CU_ASSERT_EQUAL(r, 0);
+	}
+
+	/* clean resources */
+	free(resources);
+	free(ibs_request);
+	free(ib_info);
+	free(pm4);
+
+	/* end of test */
+	r = gsgpu_cs_ctx_free(context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+}
+
+static void gsgpu_command_submission_sdma_mipmap_generate(void)
+{
+	uint16_t width, height;
+
+	for (int count = 0; count < 3; count++) {
+		switch (count) {
+		case MINIMUM:
+			width = 1;
+			height = 1;
+			break;
+		case EQUAL_LENGTH:
+			width = 128;
+			height = 128;
+			break;
+		case UNEQUAL_LENGTH:
+			width = 512;
+			height = 384;
+			break;
+		default:
+			break;
+		}
+
+		gsgpu_command_submission_mipmap_generate_helper(GSGPU_HW_IP_DMA, width, height);
+	}
+}
+
+
+#define PIXEL_FORMAT_RGBA8 4
+static void generate_once_mipmap(union pixel_rgba8 * const src, union pixel_rgba8 * const dst, const uint16_t width, const uint16_t height)
+{
+	union pixel_rgba8 *test_src00, *test_src10, *test_src01, *test_src11;
+	union pixel_rgba8 *test_dst;
+
+	for (int y = 0; y < height; y++){
+		for(int x = 0; x < width; x++){
+			test_src00 = get_tile_pixel(src, 2*x+0, 2*y+0, width *2);
+			test_src10 = get_tile_pixel(src, 2*x+1, 2*y+0, width *2);
+			test_src01 = get_tile_pixel(src, 2*x+0, 2*y+1, width *2);
+			test_src11 = get_tile_pixel(src, 2*x+1, 2*y+1, width* 2);
+			test_dst = get_tile_pixel(dst, x, y , width);
+
+			(*test_dst).channel.r = ((*test_src00).channel.r + (*test_src01).channel.r + (*test_src10).channel.r + (*test_src11).channel.r)/4;
+			(*test_dst).channel.g = ((*test_src00).channel.g + (*test_src01).channel.g + (*test_src10).channel.g + (*test_src11).channel.g)/4;
+			(*test_dst).channel.b = ((*test_src00).channel.b + (*test_src01).channel.b + (*test_src10).channel.b + (*test_src11).channel.b)/4;
+			(*test_dst).channel.a = ((*test_src00).channel.a + (*test_src01).channel.a + (*test_src10).channel.a + (*test_src11).channel.a)/4;
+		}
+	}
+}
+
+static void gsgpu_command_submission_sdma_copy_tiled(void)
+{
+        gsgpu_command_submission_copy_tiled_helper(GSGPU_HW_IP_DMA);
+}
+
+static void gsgpu_command_submission_copy_tiled_helper(unsigned ip_type)
+{
+	const int pm4_dw = 256;
+	gsgpu_context_handle context_handle = NULL;
+	gsgpu_bo_handle bo1 = NULL, bo2 = NULL;
+	gsgpu_bo_handle *resources;
+	uint32_t *pm4;
+	struct gsgpu_cs_ib_info *ib_info;
+	struct gsgpu_cs_request *ibs_request;
+	uint64_t bo1_mc, bo2_mc;
+	volatile unsigned char *bo1_cpu, *bo2_cpu;
+	int i, j, r, ring_id, align_dw;
+	uint64_t gtt_flags[1] = {0};
+	gsgpu_va_handle bo1_va_handle = NULL, bo2_va_handle = NULL;
+	struct drm_gsgpu_info_hw_ip hw_ip_info;
+
+	/* init object of command package */
+	struct gsgpu_xdma_cmd_desc cmd_buffer[] = {
+		{
+			.header.sec = {
+				GSGPU_CMD_XDMA_COPY,
+				GSGPU_CMD_XDMA_FORMAT_RGBA8,
+				GSGPU_CMD_XDMA_BODY_NR,
+				GSGPU_CMD_XDMA_MODE_L2T,
+				GSGPU_CMD_XDMA_SUB_MODE_TILED_4X4,
+			},
+			.body = {
+				.size.width = 16,
+				.size.height = 16,
+				.src_lo = 0,
+				.src_hi = 0,
+				.dst_lo = 0,
+				.dst_hi = 0,
+				.src_stride = 16 * 4,
+				.dst_stride = 16 * 4 * 4,
+				.sema.rd = 0,
+				.sema.rd_en = 0,
+				.sema.wr = 0,
+				.sema.wr_en = 0,
+			},
+		},
+		{
+			.header.sec = {
+				GSGPU_CMD_XDMA_COPY,
+				GSGPU_CMD_XDMA_FORMAT_RGBA8,
+				GSGPU_CMD_XDMA_BODY_NR,
+				GSGPU_CMD_XDMA_MDOE_T2L,
+				GSGPU_CMD_XDMA_SUB_MODE_TILED_4X4,
+			},
+			.body = {
+				.size.width = 16,
+				.size.height = 16,
+				.src_lo = 0,
+				.src_hi = 0,
+				.dst_lo = 0,
+				.dst_hi = 0,
+				.src_stride = 16 * 4 * 4,
+				.dst_stride = 16 * 4,
+				.sema.rd = 0,
+				.sema.rd_en = 0,
+				.sema.wr = 0,
+				.sema.wr_en = 0,
+			},
+		},
+	};
+
+	pm4 = calloc(pm4_dw, sizeof(*pm4));
+	CU_ASSERT_NOT_EQUAL(pm4, NULL);
+
+	ib_info = calloc(1, sizeof(*ib_info));
+	CU_ASSERT_NOT_EQUAL(ib_info, NULL);
+
+	ibs_request = calloc(1, sizeof(*ibs_request));
+	CU_ASSERT_NOT_EQUAL(ibs_request, NULL);
+
+	r = gsgpu_query_hw_ip_info(device_handle, ip_type, 0, &hw_ip_info);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_cs_ctx_create(device_handle, &context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+	/* prepare resource */
+	resources = calloc(2, sizeof(gsgpu_bo_handle));
+	CU_ASSERT_NOT_EQUAL(resources, NULL);
+
+	for (int index = 0; index < TABLE_SIZE(cmd_buffer); index++) {
+		int sdma_write_length = get_pixel_depth(cmd_buffer[index].header.sec.format) * cmd_buffer[index].body.size.width * cmd_buffer[index].body.size.height;
+		/* allocate UC bo1for sDMA use */
+		r = gsgpu_bo_alloc_and_map(device_handle,
+					    sdma_write_length, 4096,
+					    GSGPU_GEM_DOMAIN_GTT,
+					    gtt_flags[0], &bo1,
+					    (void**)&bo1_cpu, &bo1_mc,
+					    &bo1_va_handle);
+		CU_ASSERT_EQUAL(r, 0);
+
+		/* set bo1 */
+		memset((void*)bo1_cpu, 0xaa, sdma_write_length);
+
+		/* allocate UC bo2 for sDMA use */
+		r = gsgpu_bo_alloc_and_map(device_handle,
+					    sdma_write_length, 4096,
+					    GSGPU_GEM_DOMAIN_GTT,
+					    gtt_flags[0], &bo2,
+					    (void**)&bo2_cpu, &bo2_mc,
+					    &bo2_va_handle);
+		CU_ASSERT_EQUAL(r, 0);
+
+		/* clear bo2 */
+		memset((void*)bo2_cpu, 0, sdma_write_length);
+
+		resources[0] = bo1;
+		resources[1] = bo2;
+
+		/* fulfill PM4: test DMA copy linear */
+		cmd_buffer[index].body.src_lo = (uint32_t)(0xffffffff & bo1_mc);
+		cmd_buffer[index].body.src_hi = (uint32_t)((0xffffffff00000000 & bo1_mc) >> 32);
+		cmd_buffer[index].body.dst_lo = (uint32_t)(0xffffffff & bo2_mc);
+		cmd_buffer[index].body.dst_hi = (uint32_t)((0xffffffff00000000 & bo2_mc) >> 32);
+
+		i = sizeof(cmd_buffer[index]);
+		memcpy(pm4, &cmd_buffer[index], i);
+
+		i = (cmd_buffer[index].header.sec.length + 1);
+
+		/* ib cmd packet align */
+		align_dw = hw_ip_info.ib_size_alignment - (i & (hw_ip_info.ib_size_alignment - 1));
+		for (j = 0; j < align_dw; j++) {
+			pm4[i++] = GSGPU_CMD_NOP;
+		}
+
+		gsgpu_test_exec_cs_helper(context_handle,
+					   ip_type, 0,
+					   i, pm4,
+					   2, resources,
+					   ib_info, ibs_request);
+
+		/* verify if SDMA test result meets with expected */
+		i = 0;
+		while(i < sdma_write_length) {
+			CU_ASSERT_EQUAL(bo2_cpu[i++], 0xaa);
+		}
+		r = gsgpu_bo_unmap_and_free(bo1, bo1_va_handle, bo1_mc,
+					     sdma_write_length);
+		CU_ASSERT_EQUAL(r, 0);
+		r = gsgpu_bo_unmap_and_free(bo2, bo2_va_handle, bo2_mc,
+					     sdma_write_length);
+		CU_ASSERT_EQUAL(r, 0);
+	}
+	/* clean resources */
+	free(resources);
+	free(ibs_request);
+	free(ib_info);
+	free(pm4);
+
+	/* end of test */
+	r = gsgpu_cs_ctx_free(context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+}
+
+static void gsgpu_command_submission_sdma_semaphore(void)
+{
+        gsgpu_command_submission_sdma_semaphore_helper(GSGPU_HW_IP_DMA);
+}
+
+static void gsgpu_command_submission_sdma_semaphore_helper(unsigned ip_type)
+{
+	const int pm4_dw = 256;
+	gsgpu_context_handle context_handle;
+	gsgpu_bo_handle bo;
+	gsgpu_bo_handle *resources;
+	uint32_t *pm4;
+	struct gsgpu_cs_ib_info *ib_info;
+	struct gsgpu_cs_request *ibs_request;
+	uint64_t bo_mc;
+	volatile uint32_t *bo_cpu;
+	int i, j, r, align_dw;
+	uint64_t gtt_flags[1] = {0};
+	gsgpu_va_handle va_handle;
+	struct drm_gsgpu_info_hw_ip hw_ip_info;
+
+	struct gsgpu_xdma_cmd_desc cmd_buffer[] = {
+		{
+			.header.sec = {
+				GSGPU_CMD_XDMA_COPY,
+				GSGPU_CMD_XDMA_FORMAT_RGBA8,
+				GSGPU_CMD_XDMA_BODY_NR,
+				GSGPU_CMD_XDMA_MODE_MEMSET,
+				GSGPU_CMD_XDMA_SUB_MODE_DEFAULT,
+			},
+			.body = {
+				.size.width = 16 * 1024,
+				.size.height = 1,
+				.src_lo = 0xdeadbeaf,
+				.src_hi = 0,
+				.dst_lo = 0,
+				.dst_hi = 0,
+				.src_stride = 0,
+				.dst_stride = 16 * 1024,
+				.sema.val = 0,
+			},
+		},
+	};
+
+	pm4 = calloc(pm4_dw, sizeof(*pm4));
+	CU_ASSERT_NOT_EQUAL(pm4, NULL);
+
+	ib_info = calloc(1, sizeof(*ib_info));
+	CU_ASSERT_NOT_EQUAL(ib_info, NULL);
+
+	ibs_request = calloc(1, sizeof(*ibs_request));
+	CU_ASSERT_NOT_EQUAL(ibs_request, NULL);
+
+	r = gsgpu_query_hw_ip_info(device_handle, ip_type, 0, &hw_ip_info);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_cs_ctx_create(device_handle, &context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+	/* prepare resource */
+	resources = calloc(1, sizeof(gsgpu_bo_handle));
+	CU_ASSERT_NOT_EQUAL(resources, NULL);
+
+	int sdma_write_length = get_pixel_depth(cmd_buffer[0].header.sec.format) * cmd_buffer[0].body.size.width * cmd_buffer[0].body.size.height;
+
+	/* allocate bo for DMA use */
+	r = gsgpu_bo_alloc_and_map(device_handle,
+				    sdma_write_length, BUFFER_ALIGN,
+				    GSGPU_GEM_DOMAIN_GTT,
+				    0, &bo,
+				    (void**)&bo_cpu, &bo_mc,
+				    &va_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+	/* clear bo */
+	memset((void*)bo_cpu, 0, sdma_write_length);
+
+	resources[0] = bo;
+
+	uint64_t sema_rd = 0;
+	gsgpu_hw_sema_get(device_handle, context_handle, &sema_rd);
+
+	/* fulfill cmd packet: test SDMA const fill */
+	i = 0;
+	cmd_buffer[0].body.dst_lo = (uint32_t)(0xffffffff & bo_mc);
+	cmd_buffer[0].body.dst_hi = (uint32_t)((0xffffffff00000000 & bo_mc) >> 32);
+	cmd_buffer[0].body.sema.rd = sema_rd;
+	cmd_buffer[0].body.sema.rd_en = 1;
+
+
+        /* FILL get semaphore */
+	pm4[i++] = cmd_buffer[0].header.val;
+	pm4[i++] = cmd_buffer[0].body.data_size;
+	pm4[i++] = cmd_buffer[0].body.src_lo;
+	pm4[i++] = cmd_buffer[0].body.src_hi;
+	pm4[i++] = cmd_buffer[0].body.dst_lo;
+	pm4[i++] = cmd_buffer[0].body.dst_hi;
+	pm4[i++] = cmd_buffer[0].body.src_stride;
+	pm4[i++] = cmd_buffer[0].body.dst_stride;
+	pm4[i++] = cmd_buffer[0].body.sema.val;
+
+	/* POLL set semaphore */
+        pm4[i++] = GSPKT(0x85, 6) | 0 << 8 /* ture */ | 1 << 12 /* reg/mem */;
+        pm4[i++] = (bo_mc + 256*4) & 0xffffffff;
+        pm4[i++] = ((bo_mc + 256*4) >> 32) & 0xffffffff;
+        pm4[i++] = 0x00000000; /* reference value */
+        pm4[i++] = 0xffffffff; /* and mask */
+        pm4[i++] = (0xfff) << 16 | 2; /* retry count, poll interval */
+        pm4[i++] = 0x10 | (sema_rd & 0x0f); /* set semaphore */
+
+	/* ib cmd packet align */
+	align_dw = hw_ip_info.ib_size_alignment - (i & (hw_ip_info.ib_size_alignment - 1));
+	for (j = 0; j < align_dw; j++) {
+		pm4[i++] = GSGPU_CMD_NOP;
+	}
+
+	gsgpu_test_exec_cs_helper(context_handle,
+				   ip_type, 0,
+				   i, pm4,
+				   1, resources,
+				   ib_info, ibs_request);
+
+	/* verify if SDMA test result meets with expected */
+	i = 0;
+	while (i < sdma_write_length / get_pixel_depth(cmd_buffer[0].header.sec.format)) {
+		CU_ASSERT_EQUAL(bo_cpu[i++], 0xdeadbeaf);
+	}
+
+	gsgpu_hw_sema_put(device_handle, context_handle, sema_rd);
+
+	r = gsgpu_bo_unmap_and_free(bo, va_handle, bo_mc,
+				     sdma_write_length);
+	CU_ASSERT_EQUAL(r, 0);
+
+	/* clean resources */
+	free(resources);
+	free(ibs_request);
+	free(ib_info);
+	free(pm4);
+
+	/* end of test */
+	r = gsgpu_cs_ctx_free(context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+}
+
+static uint32_t get_pixel_depth(uint32_t format)
+{
+	uint32_t cpp;
+
+	switch (format) {
+	case GSGPU_CMD_XDMA_FORMAT_R8:
+	case GSGPU_CMD_XDMA_FORMAT_S8:
+		cpp = 1;
+		break;
+	case GSGPU_CMD_XDMA_FORMAT_R16:
+	case GSGPU_CMD_XDMA_FORMAT_RG8:
+	case GSGPU_CMD_XDMA_FORMAT_RGB5A1:
+	case GSGPU_CMD_XDMA_FORMAT_R5G6B5:
+	case GSGPU_CMD_XDMA_FORMAT_D16:
+		cpp = 2;
+		break;
+	case GSGPU_CMD_XDMA_FORMAT_D24:
+		cpp = 3;
+		break;
+	case GSGPU_CMD_XDMA_FORMAT_RGBA8:
+	case GSGPU_CMD_XDMA_FORMAT_RG16:
+	case GSGPU_CMD_XDMA_FORMAT_RGB10A2:
+	case GSGPU_CMD_XDMA_FORMAT_D24S8:
+		cpp = 4;
+		break;
+	case GSGPU_CMD_XDMA_FORMAT_RGBA16:
+		cpp = 8;
+		break;
+	default:
+		cpp = 4;
+		break;
+	}
+
+	return cpp;
+}
diff --git a/tests/gsgpu/gsgpu_test.c b/tests/gsgpu/gsgpu_test.c
new file mode 100644
index 00000000..a17075af
--- /dev/null
+++ b/tests/gsgpu/gsgpu_test.c
@@ -0,0 +1,571 @@
+/*
+ * Copyright (C) 2020 Loongson Technology Co., Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+*/
+
+#ifdef HAVE_CONFIG_H
+#include "config.h"
+#endif
+
+#include <string.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <unistd.h>
+#include <string.h>
+#include <ctype.h>
+#include <fcntl.h>
+#include <errno.h>
+#include <signal.h>
+#include <time.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <sys/ioctl.h>
+#include <sys/time.h>
+#include <stdarg.h>
+#include <stdint.h>
+
+#include "drm.h"
+#include "xf86drmMode.h"
+#include "xf86drm.h"
+
+#include "CUnit/Basic.h"
+
+#include "gsgpu_test.h"
+#include "gsgpu_internal.h"
+
+/* Test suite names */
+#define BASIC_TESTS_STR "Basic Tests"
+#define BO_TESTS_STR "BO Tests"
+#define DMA_TESTS_STR "DMA Tests"
+#define CS_TESTS_STR "CS Tests"
+#define DEADLOCK_TESTS_STR "Deadlock Tests"
+#define VM_TESTS_STR "VM Tests"
+
+/**
+ *  Open handles for gsgpu devices
+ *
+ */
+int drm_gsgpu[MAX_CARDS_SUPPORTED];
+
+/** Open render node to test */
+int open_render_node = 0;	/* By default run most tests on primary node */
+
+/** The table of all known test suites to run */
+static CU_SuiteInfo suites[] = {
+	{
+		.pName = BASIC_TESTS_STR,
+		.pInitFunc = suite_basic_tests_init,
+		.pCleanupFunc = suite_basic_tests_clean,
+		.pTests = basic_tests,
+	},
+	{
+		.pName = BO_TESTS_STR,
+		.pInitFunc = suite_bo_tests_init,
+		.pCleanupFunc = suite_bo_tests_clean,
+		.pTests = bo_tests,
+	},
+	{
+		.pName = DMA_TESTS_STR,
+		.pInitFunc = suite_dma_tests_init,
+		.pCleanupFunc = suite_dma_tests_clean,
+		.pTests = dma_tests,
+	},
+	{
+		.pName = DEADLOCK_TESTS_STR,
+		.pInitFunc = suite_deadlock_tests_init,
+		.pCleanupFunc = suite_deadlock_tests_clean,
+		.pTests = deadlock_tests,
+	},
+	{
+		.pName = VM_TESTS_STR,
+		.pInitFunc = suite_vm_tests_init,
+		.pCleanupFunc = suite_vm_tests_clean,
+		.pTests = vm_tests,
+	},
+
+	CU_SUITE_INFO_NULL,
+};
+
+typedef CU_BOOL (*active__stat_func)(void);
+
+typedef struct Suites_Active_Status {
+	char*             pName;
+	active__stat_func pActive;
+}Suites_Active_Status;
+
+static CU_BOOL always_active()
+{
+	return CU_TRUE;
+}
+
+static Suites_Active_Status suites_active_stat[] = {
+		{
+			.pName = BASIC_TESTS_STR,
+			.pActive = always_active,
+		},
+		{
+			.pName = BO_TESTS_STR,
+			.pActive = always_active,
+		},
+		{
+			.pName = DMA_TESTS_STR,
+			.pActive = always_active,
+		},
+		{
+			.pName = DEADLOCK_TESTS_STR,
+			.pActive = suite_deadlock_tests_enable,
+		},
+		{
+			.pName = VM_TESTS_STR,
+			.pActive = suite_vm_tests_enable,
+		},
+};
+
+
+/*
+ * Display information about all  suites and their tests
+ *
+ * NOTE: Must be run after registry is initialized and suites registered.
+ */
+static void display_test_suites(void)
+{
+	int iSuite;
+	int iTest;
+	CU_pSuite pSuite = NULL;
+	CU_pTest  pTest  = NULL;
+
+	printf("Suites\n");
+
+	for (iSuite = 0; suites[iSuite].pName != NULL; iSuite++) {
+
+		pSuite = CU_get_suite_by_index((unsigned int) iSuite + 1,
+						      CU_get_registry());
+
+		if (!pSuite) {
+			fprintf(stderr, "Invalid suite id : %d\n", iSuite + 1);
+			continue;
+		}
+
+		printf("Suite id = %d: Name '%s status: %s'\n",
+				iSuite + 1, suites[iSuite].pName,
+				pSuite->fActive ? "ENABLED" : "DISABLED");
+
+
+
+		for (iTest = 0; suites[iSuite].pTests[iTest].pName != NULL;
+			iTest++) {
+
+			pTest = CU_get_test_by_index((unsigned int) iTest + 1,
+									pSuite);
+
+			if (!pTest) {
+				fprintf(stderr, "Invalid test id : %d\n", iTest + 1);
+				continue;
+			}
+
+			printf("Test id %d: Name: '%s status: %s'\n", iTest + 1,
+					suites[iSuite].pTests[iTest].pName,
+					pSuite->fActive && pTest->fActive ?
+						     "ENABLED" : "DISABLED");
+		}
+	}
+}
+
+
+/** Help string for command line parameters */
+static const char usage[] =
+	"Usage: %s [-hlpr] [<-s <suite id>> [-t <test id>] [-f]] "
+	"[-b <pci_bus_id> [-d <pci_device_id>]]\n"
+	"where:\n"
+	"       l - Display all suites and their tests\n"
+	"       r - Run the tests on render node\n"
+	"       b - Specify device's PCI bus id to run tests\n"
+	"       d - Specify device's PCI device id to run tests (optional)\n"
+	"       p - Display information of GSGPU devices in system\n"
+	"       f - Force executing inactive suite or test\n"
+	"       h - Display this help\n";
+/** Specified options strings for getopt */
+static const char options[]   = "hlrps:t:b:d:f";
+
+/* Open GPU devices.
+ * Return the number of GPU device openned.
+ */
+static int gsgpu_open_devices(int open_render_node)
+{
+	drmDevicePtr devices[MAX_CARDS_SUPPORTED];
+	int i;
+	int drm_node;
+	int gsgpu_index = 0;
+	int drm_count;
+	int fd;
+	drmVersionPtr version;
+
+	drm_count = drmGetDevices2(0, devices, MAX_CARDS_SUPPORTED);
+
+	if (drm_count < 0) {
+		fprintf(stderr,
+			"drmGetDevices2() returned an error %d\n",
+			drm_count);
+		return 0;
+	}
+
+	for (i = 0; i < drm_count; i++) {
+		/* If this is not PCI device, skip*/
+		if (devices[i]->bustype != DRM_BUS_PCI)
+			continue;
+
+		/* If this is not Loongson GPU vender ID, skip*/
+		if (devices[i]->deviceinfo.pci->vendor_id != 0x0014)
+			continue;
+
+		if (open_render_node)
+			drm_node = DRM_NODE_RENDER;
+		else
+			drm_node = DRM_NODE_PRIMARY;
+
+		fd = -1;
+		if (devices[i]->available_nodes & 1 << drm_node)
+			fd = open(
+				devices[i]->nodes[drm_node],
+				O_RDWR | O_CLOEXEC);
+
+		/* This node is not available. */
+		if (fd < 0) continue;
+
+		version = drmGetVersion(fd);
+		if (!version) {
+			fprintf(stderr,
+				"Warning: Cannot get version for %s."
+				"Error is %s\n",
+				devices[i]->nodes[drm_node],
+				strerror(errno));
+			close(fd);
+			continue;
+		}
+
+		if (strcmp(version->name, "gsgpu")) {
+			/* This is not GSGPU driver, skip.*/
+			drmFreeVersion(version);
+			close(fd);
+			continue;
+		}
+
+		drmFreeVersion(version);
+
+		drm_gsgpu[gsgpu_index] = fd;
+		gsgpu_index++;
+	}
+
+	drmFreeDevices(devices, drm_count);
+	return gsgpu_index;
+}
+
+/* Close GPU devices.
+ */
+static void gsgpu_close_devices()
+{
+	int i;
+	for (i = 0; i < MAX_CARDS_SUPPORTED; i++)
+		if (drm_gsgpu[i] >=0)
+			close(drm_gsgpu[i]);
+}
+
+/* Print GPU devices information */
+static void gsgpu_print_devices()
+{
+	int i;
+	drmDevicePtr device;
+
+	/* Open the first GPU devcie to print driver information. */
+	if (drm_gsgpu[0] >=0) {
+		/* Display GPU driver version information.*/
+		drmVersionPtr retval = drmGetVersion(drm_gsgpu[0]);
+
+		if (retval == NULL) {
+			perror("Cannot get version for GSGPU device");
+			return;
+		}
+
+		printf("Driver name: %s, Date: %s, Description: %s.\n",
+			retval->name, retval->date, retval->desc);
+		drmFreeVersion(retval);
+	}
+
+	/* Display information of GPU devices */
+	printf("Devices:\n");
+	for (i = 0; i < MAX_CARDS_SUPPORTED && drm_gsgpu[i] >=0; i++)
+		if (drmGetDevice2(drm_gsgpu[i],
+			DRM_DEVICE_GET_PCI_REVISION,
+			&device) == 0) {
+			if (device->bustype == DRM_BUS_PCI) {
+				printf("PCI ");
+				printf(" domain:%04x",
+					device->businfo.pci->domain);
+				printf(" bus:%02x",
+					device->businfo.pci->bus);
+				printf(" device:%02x",
+					device->businfo.pci->dev);
+				printf(" function:%01x",
+					device->businfo.pci->func);
+				printf(" vendor_id:%04x",
+					device->deviceinfo.pci->vendor_id);
+				printf(" device_id:%04x",
+					device->deviceinfo.pci->device_id);
+				printf(" subvendor_id:%04x",
+					device->deviceinfo.pci->subvendor_id);
+				printf(" subdevice_id:%04x",
+					device->deviceinfo.pci->subdevice_id);
+				printf(" revision_id:%02x",
+					device->deviceinfo.pci->revision_id);
+				printf("\n");
+			}
+			drmFreeDevice(&device);
+		}
+}
+
+/* Find a match GPU device in PCI bus
+ * Return the index of the device or -1 if not found
+ */
+static int gsgpu_find_device(uint8_t bus, uint16_t dev)
+{
+	int i;
+	drmDevicePtr device;
+
+	for (i = 0; i < MAX_CARDS_SUPPORTED && drm_gsgpu[i] >= 0; i++) {
+		if (drmGetDevice2(drm_gsgpu[i],
+			DRM_DEVICE_GET_PCI_REVISION,
+			&device) == 0) {
+			if (device->bustype == DRM_BUS_PCI)
+				if ((bus == 0xFF || device->businfo.pci->bus == bus) &&
+					device->deviceinfo.pci->device_id == dev) {
+					drmFreeDevice(&device);
+					return i;
+				}
+
+			drmFreeDevice(&device);
+		}
+	}
+
+	return -1;
+}
+
+static void gsgpu_disable_suites()
+{
+	gsgpu_device_handle device_handle;
+	uint32_t major_version, minor_version, family_id = GSGPU_FAMILY_UNKNOWN;
+	int i;
+	int size = sizeof(suites_active_stat) / sizeof(suites_active_stat[0]);
+
+	if (gsgpu_device_initialize(drm_gsgpu[0], &major_version,
+				   &minor_version, &device_handle))
+		return;
+
+	family_id = device_handle->info.family_id;
+
+	if (gsgpu_device_deinitialize(device_handle))
+		return;
+
+	/* Set active status for suites based on their policies */
+	for (i = 0; i < size; ++i)
+		if (gsgpu_set_suite_active(suites_active_stat[i].pName,
+				suites_active_stat[i].pActive()))
+			fprintf(stderr, "suite deactivation failed - %s\n", CU_get_error_msg());
+
+	/* Explicitly disable specific tests due to known bugs or preferences */
+	/*
+	* BUG: Compute ring stalls and never recovers when the address is
+	* written after the command already submitted
+	*/
+	if (gsgpu_set_test_active(BASIC_TESTS_STR, "Sync dependency Test", CU_FALSE))
+		fprintf(stderr, "test Sync dependency Test failed - %s\n", CU_get_error_msg());
+
+	/* if (gsgpu_set_test_active(BO_TESTS_STR, "Metadata", CU_FALSE)) */
+		/* fprintf(stderr, "test Metadata Test failed - %s\n", CU_get_error_msg()); */
+
+}
+
+/* The main() function for setting up and running the tests.
+ * Returns a CUE_SUCCESS on successful running, another
+ * CUnit error code on failure.
+ */
+int main(int argc, char **argv)
+{
+	int c;			/* Character received from getopt */
+	int i = 0;
+	int suite_id = -1;	/* By default run everything */
+	int test_id  = -1;	/* By default run all tests in the suite */
+	int pci_bus_id = -1;    /* By default PC bus ID is not specified */
+	int pci_device_id = 0;  /* By default PC device ID is zero */
+	int display_devices = 0;/* By default not to display devices' info */
+	CU_pSuite pSuite = NULL;
+	CU_pTest  pTest  = NULL;
+	int test_device_index;
+	int display_list = 0;
+	int force_run = 0;
+
+	for (i = 0; i < MAX_CARDS_SUPPORTED; i++)
+		drm_gsgpu[i] = -1;
+
+
+	/* Parse command line string */
+	opterr = 0;		/* Do not print error messages from getopt */
+	while ((c = getopt(argc, argv, options)) != -1) {
+		switch (c) {
+		case 'l':
+			display_list = 1;
+			break;
+		case 's':
+			suite_id = atoi(optarg);
+			break;
+		case 't':
+			test_id = atoi(optarg);
+			break;
+		case 'b':
+			pci_bus_id = atoi(optarg);
+			break;
+		case 'd':
+			sscanf(optarg, "%x", &pci_device_id);
+			break;
+		case 'p':
+			display_devices = 1;
+			break;
+		case 'r':
+			open_render_node = 1;
+			break;
+		case 'f':
+			force_run = 1;
+			break;
+		case '?':
+		case 'h':
+			fprintf(stderr, usage, argv[0]);
+			exit(EXIT_SUCCESS);
+		default:
+			fprintf(stderr, usage, argv[0]);
+			exit(EXIT_FAILURE);
+		}
+	}
+
+	if (gsgpu_open_devices(open_render_node) <= 0) {
+		perror("Cannot open GSGPU device");
+		exit(EXIT_FAILURE);
+	}
+
+	if (drm_gsgpu[0] < 0) {
+		perror("Cannot open GSGPU device");
+		exit(EXIT_FAILURE);
+	}
+
+	if (display_devices) {
+		gsgpu_print_devices();
+		gsgpu_close_devices();
+		exit(EXIT_SUCCESS);
+	}
+
+	if (pci_bus_id > 0 || pci_device_id) {
+		/* A device was specified to run the test */
+		test_device_index = gsgpu_find_device(pci_bus_id,
+						       pci_device_id);
+
+		if (test_device_index >= 0) {
+			/* Most tests run on device of drm_gsgpu[0].
+			 * Swap the chosen device to drm_gsgpu[0].
+			 */
+			i = drm_gsgpu[0];
+			drm_gsgpu[0] = drm_gsgpu[test_device_index];
+			drm_gsgpu[test_device_index] = i;
+		} else {
+			fprintf(stderr,
+				"The specified GPU device does not exist.\n");
+			exit(EXIT_FAILURE);
+		}
+	}
+
+	/* Initialize test suites to run */
+
+	/* initialize the CUnit test registry */
+	if (CUE_SUCCESS != CU_initialize_registry()) {
+		gsgpu_close_devices();
+		return CU_get_error();
+	}
+
+	/* Register suites. */
+	if (CU_register_suites(suites) != CUE_SUCCESS) {
+		fprintf(stderr, "suite registration failed - %s\n",
+				CU_get_error_msg());
+		CU_cleanup_registry();
+		gsgpu_close_devices();
+		exit(EXIT_FAILURE);
+	}
+
+	/* Run tests using the CUnit Basic interface */
+	CU_basic_set_mode(CU_BRM_VERBOSE);
+
+	/* Disable suites and individual tests based on misc. conditions */
+	gsgpu_disable_suites();
+
+	if (display_list) {
+		display_test_suites();
+		goto end;
+	}
+
+	if (suite_id != -1) {	/* If user specify particular suite? */
+		pSuite = CU_get_suite_by_index((unsigned int) suite_id,
+						CU_get_registry());
+
+		if (pSuite) {
+
+			if (force_run)
+				CU_set_suite_active(pSuite, CU_TRUE);
+
+			if (test_id != -1) {   /* If user specify test id */
+				pTest = CU_get_test_by_index(
+						(unsigned int) test_id,
+						pSuite);
+				if (pTest) {
+					if (force_run)
+						CU_set_test_active(pTest, CU_TRUE);
+
+					CU_basic_run_test(pSuite, pTest);
+				}
+				else {
+					fprintf(stderr, "Invalid test id: %d\n",
+								test_id);
+					CU_cleanup_registry();
+					gsgpu_close_devices();
+					exit(EXIT_FAILURE);
+				}
+			} else
+				CU_basic_run_suite(pSuite);
+		} else {
+			fprintf(stderr, "Invalid suite id : %d\n",
+					suite_id);
+			CU_cleanup_registry();
+			gsgpu_close_devices();
+			exit(EXIT_FAILURE);
+		}
+	} else
+		CU_basic_run_tests();
+
+end:
+	CU_cleanup_registry();
+	gsgpu_close_devices();
+	return CU_get_error();
+}
diff --git a/tests/gsgpu/gsgpu_test.h b/tests/gsgpu/gsgpu_test.h
new file mode 100755
index 00000000..9df19a35
--- /dev/null
+++ b/tests/gsgpu/gsgpu_test.h
@@ -0,0 +1,367 @@
+/*
+ * Copyright (C) 2020 Loongson Technology Co., Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+*/
+
+#ifndef _GSGPU_TEST_H_
+#define _GSGPU_TEST_H_
+
+#include "gsgpu.h"
+#include "gsgpu_drm.h"
+
+/**
+ * Define max. number of card in system which we are able to handle
+ */
+#define MAX_CARDS_SUPPORTED     8
+
+/* Forward reference for array to keep "drm" handles */
+extern int drm_gsgpu[MAX_CARDS_SUPPORTED];
+
+/* Global variables */
+extern int open_render_node;
+
+/*************************  Basic test suite ********************************/
+
+/*
+ * Define basic test suite to serve as the starting point for future testing
+*/
+
+/**
+ * Initialize basic test suite
+ */
+int suite_basic_tests_init();
+
+/**
+ * Deinitialize basic test suite
+ */
+int suite_basic_tests_clean();
+
+/**
+ * Tests in basic test suite
+ */
+extern CU_TestInfo basic_tests[];
+
+/**
+ * Initialize bo test suite
+ */
+int suite_bo_tests_init();
+
+/**
+ * Deinitialize bo test suite
+ */
+int suite_bo_tests_clean();
+
+/**
+ * Tests in bo test suite
+ */
+extern CU_TestInfo bo_tests[];
+
+/**
+ *  * Initialize dma test suite
+ *   */
+int suite_dma_tests_init();
+
+/**
+ *  * Deinitialize dma test suite
+ *   */
+int suite_dma_tests_clean();
+
+/**
+ *  * Tests in dma test suite
+ *   */
+extern CU_TestInfo dma_tests[];
+
+/**
+ * Initialize deadlock test suite
+ */
+int suite_deadlock_tests_init();
+
+/**
+ * Deinitialize deadlock test suite
+ */
+int suite_deadlock_tests_clean();
+
+/**
+ * Decide if the suite is enabled by default or not.
+ */
+CU_BOOL suite_deadlock_tests_enable(void);
+
+/**
+ * Tests in deadlock test suite
+ */
+extern CU_TestInfo deadlock_tests[];
+
+/**
+ * Initialize vm test suite
+ */
+int suite_vm_tests_init();
+
+/**
+ * Deinitialize deadlock test suite
+ */
+int suite_vm_tests_clean();
+
+/**
+ * Decide if the suite is enabled by default or not.
+ */
+CU_BOOL suite_vm_tests_enable(void);
+
+/**
+ * Tests in vm test suite
+ */
+extern CU_TestInfo vm_tests[];
+
+/**
+ * Helper functions
+ */
+static inline gsgpu_bo_handle gpu_mem_alloc(
+					gsgpu_device_handle device_handle,
+					uint64_t size,
+					uint64_t alignment,
+					uint32_t type,
+					uint64_t flags,
+					uint64_t *vmc_addr,
+					gsgpu_va_handle *va_handle)
+{
+	struct gsgpu_bo_alloc_request req = {0};
+	gsgpu_bo_handle buf_handle;
+	int r;
+
+	CU_ASSERT_NOT_EQUAL(vmc_addr, NULL);
+
+	req.alloc_size = size;
+	req.phys_alignment = alignment;
+	req.preferred_heap = type;
+	req.flags = flags;
+
+	r = gsgpu_bo_alloc(device_handle, &req, &buf_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_va_range_alloc(device_handle,
+				  gsgpu_gpu_va_range_general,
+				  size, alignment, 0, vmc_addr,
+				  va_handle, 0);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_bo_va_op(buf_handle, 0, size, *vmc_addr, 0, GSGPU_VA_OP_MAP);
+	CU_ASSERT_EQUAL(r, 0);
+
+	return buf_handle;
+}
+
+static inline int gpu_mem_free(gsgpu_bo_handle bo,
+			       gsgpu_va_handle va_handle,
+			       uint64_t vmc_addr,
+			       uint64_t size)
+{
+	int r;
+
+	r = gsgpu_bo_va_op(bo, 0, size, vmc_addr, 0, GSGPU_VA_OP_UNMAP);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_va_range_free(va_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_bo_free(bo);
+	CU_ASSERT_EQUAL(r, 0);
+
+	return 0;
+}
+
+static inline int
+gsgpu_bo_alloc_wrap(gsgpu_device_handle dev, unsigned size,
+		     unsigned alignment, unsigned heap, uint64_t flags,
+		     gsgpu_bo_handle *bo)
+{
+	struct gsgpu_bo_alloc_request request = {};
+	gsgpu_bo_handle buf_handle;
+	int r;
+
+	request.alloc_size = size;
+	request.phys_alignment = alignment;
+	request.preferred_heap = heap;
+	request.flags = flags;
+
+	r = gsgpu_bo_alloc(dev, &request, &buf_handle);
+	if (r)
+		return r;
+
+	*bo = buf_handle;
+
+	return 0;
+}
+
+static inline int
+gsgpu_bo_alloc_and_map(gsgpu_device_handle dev, unsigned size,
+			unsigned alignment, unsigned heap, uint64_t flags,
+			gsgpu_bo_handle *bo, void **cpu, uint64_t *mc_address,
+			gsgpu_va_handle *va_handle)
+{
+	struct gsgpu_bo_alloc_request request = {};
+	gsgpu_bo_handle buf_handle;
+	gsgpu_va_handle handle;
+	uint64_t vmc_addr;
+	int r;
+
+	request.alloc_size = size;
+	request.phys_alignment = alignment;
+	request.preferred_heap = heap;
+	request.flags = flags;
+
+	r = gsgpu_bo_alloc(dev, &request, &buf_handle);
+	if (r)
+		return r;
+
+	r = gsgpu_va_range_alloc(dev,
+				  gsgpu_gpu_va_range_general,
+				  size, alignment, 0, &vmc_addr,
+				  &handle, 0);
+	if (r)
+		goto error_va_alloc;
+
+	r = gsgpu_bo_va_op(buf_handle, 0, size, vmc_addr, 0, GSGPU_VA_OP_MAP);
+	if (r)
+		goto error_va_map;
+
+	r = gsgpu_bo_cpu_map(buf_handle, cpu);
+	if (r)
+		goto error_cpu_map;
+
+	*bo = buf_handle;
+	*mc_address = vmc_addr;
+	*va_handle = handle;
+
+	return 0;
+
+error_cpu_map:
+	gsgpu_bo_cpu_unmap(buf_handle);
+
+error_va_map:
+	gsgpu_bo_va_op(buf_handle, 0, size, vmc_addr, 0, GSGPU_VA_OP_UNMAP);
+
+error_va_alloc:
+	gsgpu_bo_free(buf_handle);
+	return r;
+}
+
+static inline int
+gsgpu_bo_unmap_and_free(gsgpu_bo_handle bo, gsgpu_va_handle va_handle,
+			 uint64_t mc_addr, uint64_t size)
+{
+	gsgpu_bo_cpu_unmap(bo);
+	gsgpu_bo_va_op(bo, 0, size, mc_addr, 0, GSGPU_VA_OP_UNMAP);
+	gsgpu_va_range_free(va_handle);
+	gsgpu_bo_free(bo);
+
+	return 0;
+
+}
+
+static inline int
+gsgpu_get_bo_list(gsgpu_device_handle dev, gsgpu_bo_handle bo1,
+		   gsgpu_bo_handle bo2, gsgpu_bo_list_handle *list)
+{
+	gsgpu_bo_handle resources[] = {bo1, bo2};
+
+	return gsgpu_bo_list_create(dev, bo2 ? 2 : 1, resources, NULL, list);
+}
+
+
+static inline CU_ErrorCode gsgpu_set_suite_active(const char *suite_name,
+							  CU_BOOL active)
+{
+	CU_ErrorCode r = CU_set_suite_active(CU_get_suite(suite_name), active);
+
+	if (r != CUE_SUCCESS)
+		fprintf(stderr, "Failed to obtain suite %s\n", suite_name);
+
+	return r;
+}
+
+static inline CU_ErrorCode gsgpu_set_test_active(const char *suite_name,
+				  const char *test_name, CU_BOOL active)
+{
+	CU_ErrorCode r;
+	CU_pSuite pSuite = CU_get_suite(suite_name);
+
+	if (!pSuite) {
+		fprintf(stderr, "Failed to obtain suite %s\n",
+				suite_name);
+		return CUE_NOSUITE;
+	}
+
+	r = CU_set_test_active(CU_get_test(pSuite, test_name), active);
+	if (r != CUE_SUCCESS)
+		fprintf(stderr, "Failed to obtain test %s\n", test_name);
+
+	return r;
+}
+
+#define GSPKT(op, n)    (((op) & 0xFF) | ((n) & 0xFFFF) << 16)
+
+#define GSGPU_CMD_NOP                   0x80
+#define GSGPU_CMD_WRITE                 0x81
+#define GSGPU_CMD_INDIRECT              0x82
+#define GSGPU_CMD_FENCE                 0x83
+#define GSGPU_CMD_TRAP                  0x84
+#define GSGPU_CMD_POLL                  0x85
+
+#define GSGPU_CMD_XDMA_COPY         0xc0
+
+#define GSGPU_CMD_XDMA_FORMAT_RGBA8        0
+#define GSGPU_CMD_XDMA_FORMAT_RGBA16       1
+#define GSGPU_CMD_XDMA_FORMAT_RG8          10
+#define GSGPU_CMD_XDMA_FORMAT_RG16         11
+#define GSGPU_CMD_XDMA_FORMAT_R8           20
+#define GSGPU_CMD_XDMA_FORMAT_R16          21
+#define GSGPU_CMD_XDMA_FORMAT_RGB10A2      30
+#define GSGPU_CMD_XDMA_FORMAT_RGB5A1       33
+#define GSGPU_CMD_XDMA_FORMAT_R5G6B5       34
+#define GSGPU_CMD_XDMA_FORMAT_D16          36
+#define GSGPU_CMD_XDMA_FORMAT_D24          37
+#define GSGPU_CMD_XDMA_FORMAT_D24S8        39
+#define GSGPU_CMD_XDMA_FORMAT_S8           41
+
+#define GSGPU_CMD_XDMA_MODE_L2L             1
+#define GSGPU_CMD_XDMA_MODE_L2T             2
+#define GSGPU_CMD_XDMA_MDOE_T2L             3
+#define GSGPU_CMD_XDMA_MODE_MSAA            4
+#define GSGPU_CMD_XDMA_MODE_MIPMAP          5
+#define GSGPU_CMD_XDMA_MODE_MEMSET          7
+
+#define GSGPU_CMD_XDMA_SUB_MODE_DEFAULT		0
+#define GSGPU_CMD_XDMA_SUB_MODE_TILED_4X4   0
+#define GSGPU_CMD_XDMA_SUB_MODE_TILED_8X8   1
+
+#define GSGPU_CMD_XDMA_SUB_MODE_PAGE_GEN_PTEPDE		0x1
+#define GSGPU_CMD_XDMA_SUB_MODE_PAGE_SIZE_4K		(0x0 << 1)
+#define GSGPU_CMD_XDMA_SUB_MODE_PAGE_SIZE_16K		(0x1 << 1)
+#define GSGPU_CMD_XDMA_SUB_MODE_PAGE_SIZE_2M   		(0x2 << 1)
+#define GSGPU_CMD_XDMA_SUB_MODE_PAGE_SIZE_32M		(0x3 << 1)
+
+#define GSGPU_CMD_XDMA_BODY_NR	8
+
+#define WRITE_DST_SEL(x)                ((x) << 8)
+                /* 0 - register
+		 * 1 - memory
+		 */
+#define WRITE_WAIT                      (1 << 15)
+#endif  /* #ifdef _GSGPU_TEST_H_ */
diff --git a/tests/gsgpu/meson.build b/tests/gsgpu/meson.build
new file mode 100644
index 00000000..1cddeb31
--- /dev/null
+++ b/tests/gsgpu/meson.build
@@ -0,0 +1,33 @@
+# Copyright © 2017-2018 Intel Corporation
+
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+
+# The above copyright notice and this permission notice shall be included in
+# all copies or substantial portions of the Software.
+
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+# SOFTWARE.
+
+if dep_cunit.found()
+  gsgpu_test = executable(
+    'gsgpu_test',
+    files(
+      'gsgpu_test.c', 'basic_tests.c', 'bo_tests.c',
+      'deadlock_tests.c', 'vm_tests.c','dma_tests.c'
+    ),
+    dependencies : [dep_cunit, dep_threads],
+    include_directories : [inc_root, inc_drm, include_directories('../../gsgpu')],
+    link_with : [libdrm, libdrm_gsgpu],
+    install : with_install_tests,
+  )
+endif
diff --git a/tests/gsgpu/vm_tests.c b/tests/gsgpu/vm_tests.c
new file mode 100644
index 00000000..c98736d5
--- /dev/null
+++ b/tests/gsgpu/vm_tests.c
@@ -0,0 +1,173 @@
+/*
+ * Copyright (C) 2020 Loongson Technology Co., Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+*/
+
+#ifdef HAVE_CONFIG_H
+#include "config.h"
+#endif
+
+#include "CUnit/Basic.h"
+
+#include "gsgpu_test.h"
+#include "gsgpu_drm.h"
+#include "gsgpu_internal.h"
+
+static  gsgpu_device_handle device_handle;
+static  uint32_t  major_version;
+static  uint32_t  minor_version;
+
+
+static void gsgpu_vmid_reserve_test(void);
+
+CU_BOOL suite_vm_tests_enable(void)
+{
+    CU_BOOL enable = CU_TRUE;
+
+	if (gsgpu_device_initialize(drm_gsgpu[0], &major_version,
+				     &minor_version, &device_handle))
+		return CU_FALSE;
+
+	if (device_handle->info.family_id == GSGPU_FAMILY_GS) {
+		printf("\n\nCurrently hangs the CP on this ASIC, VM suite disabled\n");
+		enable = CU_FALSE;
+	}
+
+	if (gsgpu_device_deinitialize(device_handle))
+		return CU_FALSE;
+
+	return enable;
+}
+
+int suite_vm_tests_init(void)
+{
+	int r;
+
+	r = gsgpu_device_initialize(drm_gsgpu[0], &major_version,
+				   &minor_version, &device_handle);
+
+	if (r) {
+		if ((r == -EACCES) && (errno == EACCES))
+			printf("\n\nError:%s. "
+				"Hint:Try to run this test program as root.",
+				strerror(errno));
+		return CUE_SINIT_FAILED;
+	}
+
+	return CUE_SUCCESS;
+}
+
+int suite_vm_tests_clean(void)
+{
+	int r = gsgpu_device_deinitialize(device_handle);
+
+	if (r == 0)
+		return CUE_SUCCESS;
+	else
+		return CUE_SCLEAN_FAILED;
+}
+
+
+CU_TestInfo vm_tests[] = {
+	{ "resere vmid test",  gsgpu_vmid_reserve_test },
+	CU_TEST_INFO_NULL,
+};
+
+static void gsgpu_vmid_reserve_test(void)
+{
+	gsgpu_context_handle context_handle = NULL;
+	gsgpu_bo_handle ib_result_handle = NULL;
+	void *ib_result_cpu;
+	uint64_t ib_result_mc_address;
+	struct gsgpu_cs_request ibs_request;
+	struct gsgpu_cs_ib_info ib_info;
+	struct gsgpu_cs_fence fence_status;
+	uint32_t expired, flags;
+	int i, r;
+	gsgpu_bo_list_handle bo_list = NULL;
+	gsgpu_va_handle va_handle = NULL;
+	static uint32_t *ptr;
+
+	r = gsgpu_cs_ctx_create(device_handle, &context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+	flags = 0;
+	r = gsgpu_vm_reserve_vmid(device_handle, flags);
+	CU_ASSERT_EQUAL(r, 0);
+
+
+	r = gsgpu_bo_alloc_and_map(device_handle, 4096, 4096,
+			GSGPU_GEM_DOMAIN_GTT, 0,
+						    &ib_result_handle, &ib_result_cpu,
+						    &ib_result_mc_address, &va_handle);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_get_bo_list(device_handle, ib_result_handle, NULL,
+			       &bo_list);
+	CU_ASSERT_EQUAL(r, 0);
+
+	ptr = ib_result_cpu;
+
+	for (i = 0; i < 16; ++i)
+		ptr[i] = 0x80; //NOP
+
+	memset(&ib_info, 0, sizeof(struct gsgpu_cs_ib_info));
+	ib_info.ib_mc_address = ib_result_mc_address;
+	ib_info.size = 16;
+
+	memset(&ibs_request, 0, sizeof(struct gsgpu_cs_request));
+	ibs_request.ip_type = GSGPU_HW_IP_GFX;
+	ibs_request.ring = 0;
+	ibs_request.number_of_ibs = 1;
+	ibs_request.ibs = &ib_info;
+	ibs_request.resources = bo_list;
+	ibs_request.fence_info.handle = NULL;
+
+	r = gsgpu_cs_submit(context_handle, 0,&ibs_request, 1);
+	CU_ASSERT_EQUAL(r, 0);
+
+
+	memset(&fence_status, 0, sizeof(struct gsgpu_cs_fence));
+	fence_status.context = context_handle;
+	fence_status.ip_type = GSGPU_HW_IP_GFX;
+	fence_status.ip_instance = 0;
+	fence_status.ring = 0;
+	fence_status.fence = ibs_request.seq_no;
+
+	r = gsgpu_cs_query_fence_status(&fence_status,
+			GSGPU_TIMEOUT_INFINITE,0, &expired);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_bo_list_destroy(bo_list);
+	CU_ASSERT_EQUAL(r, 0);
+
+	r = gsgpu_bo_unmap_and_free(ib_result_handle, va_handle,
+				     ib_result_mc_address, 4096);
+	CU_ASSERT_EQUAL(r, 0);
+
+	flags = 0;
+	r = gsgpu_vm_unreserve_vmid(device_handle, flags);
+	CU_ASSERT_EQUAL(r, 0);
+
+
+	r = gsgpu_cs_ctx_free(context_handle);
+	CU_ASSERT_EQUAL(r, 0);
+}
diff --git a/tests/meson.build b/tests/meson.build
index ac9e66b0..1bb49f94 100644
--- a/tests/meson.build
+++ b/tests/meson.build
@@ -43,6 +43,9 @@ endif
 if with_nouveau
   subdir('nouveau')
 endif
+if with_gsgpu
+  subdir('gsgpu')
+endif
 
 drmsl = executable(
   'drmsl',
-- 
2.47.0

